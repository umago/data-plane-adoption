<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
<meta name="generator" content="Asciidoctor 2.0.23"/>
<title>Adopting a Red Hat OpenStack Platform 17.1 deployment</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700"/>
<style>
/*! Asciidoctor default stylesheet | MIT License | https://asciidoctor.org */
/* Uncomment the following line when using as a custom stylesheet */
/* @import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700"; */
html{font-family:sans-serif;-webkit-text-size-adjust:100%}
a{background:none}
a:focus{outline:thin dotted}
a:active,a:hover{outline:0}
h1{font-size:2em;margin:.67em 0}
b,strong{font-weight:bold}
abbr{font-size:.9em}
abbr[title]{cursor:help;border-bottom:1px dotted #dddddf;text-decoration:none}
dfn{font-style:italic}
hr{height:0}
mark{background:#ff0;color:#000}
code,kbd,pre,samp{font-family:monospace;font-size:1em}
pre{white-space:pre-wrap}
q{quotes:"\201C" "\201D" "\2018" "\2019"}
small{font-size:80%}
sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}
sup{top:-.5em}
sub{bottom:-.25em}
img{border:0}
svg:not(:root){overflow:hidden}
figure{margin:0}
audio,video{display:inline-block}
audio:not([controls]){display:none;height:0}
fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}
legend{border:0;padding:0}
button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}
button,input{line-height:normal}
button,select{text-transform:none}
button,html input[type=button],input[type=reset],input[type=submit]{-webkit-appearance:button;cursor:pointer}
button[disabled],html input[disabled]{cursor:default}
input[type=checkbox],input[type=radio]{padding:0}
button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}
textarea{overflow:auto;vertical-align:top}
table{border-collapse:collapse;border-spacing:0}
*,::before,::after{box-sizing:border-box}
html,body{font-size:100%}
body{background:#fff;color:rgba(0,0,0,.8);padding:0;margin:0;font-family:"Noto Serif","DejaVu Serif",serif;line-height:1;position:relative;cursor:auto;-moz-tab-size:4;-o-tab-size:4;tab-size:4;word-wrap:anywhere;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}
a:hover{cursor:pointer}
img,object,embed{max-width:100%;height:auto}
object,embed{height:100%}
img{-ms-interpolation-mode:bicubic}
.left{float:left!important}
.right{float:right!important}
.text-left{text-align:left!important}
.text-right{text-align:right!important}
.text-center{text-align:center!important}
.text-justify{text-align:justify!important}
.hide{display:none}
img,object,svg{display:inline-block;vertical-align:middle}
textarea{height:auto;min-height:50px}
select{width:100%}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#7a2518;font-weight:400;margin-top:0;margin-bottom:.25em}
div,dl,dt,dd,ul,ol,li,h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6,pre,form,p,blockquote,th,td{margin:0;padding:0}
a{color:#2156a5;text-decoration:underline;line-height:inherit}
a:hover,a:focus{color:#1d4b8f}
a img{border:0}
p{line-height:1.6;margin-bottom:1.25em;text-rendering:optimizeLegibility}
p aside{font-size:.875em;line-height:1.35;font-style:italic}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{font-family:"Open Sans","DejaVu Sans",sans-serif;font-weight:300;font-style:normal;color:#ba3925;text-rendering:optimizeLegibility;margin-top:1em;margin-bottom:.5em;line-height:1.0125em}
h1 small,h2 small,h3 small,#toctitle small,.sidebarblock>.content>.title small,h4 small,h5 small,h6 small{font-size:60%;color:#e99b8f;line-height:0}
h1{font-size:2.125em}
h2{font-size:1.6875em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.375em}
h4,h5{font-size:1.125em}
h6{font-size:1em}
hr{border:solid #dddddf;border-width:1px 0 0;clear:both;margin:1.25em 0 1.1875em}
em,i{font-style:italic;line-height:inherit}
strong,b{font-weight:bold;line-height:inherit}
small{font-size:60%;line-height:inherit}
code{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;color:rgba(0,0,0,.9)}
ul,ol,dl{line-height:1.6;margin-bottom:1.25em;list-style-position:outside;font-family:inherit}
ul,ol{margin-left:1.5em}
ul li ul,ul li ol{margin-left:1.25em;margin-bottom:0}
ul.circle{list-style-type:circle}
ul.disc{list-style-type:disc}
ul.square{list-style-type:square}
ul.circle ul:not([class]),ul.disc ul:not([class]),ul.square ul:not([class]){list-style:inherit}
ol li ul,ol li ol{margin-left:1.25em;margin-bottom:0}
dl dt{margin-bottom:.3125em;font-weight:bold}
dl dd{margin-bottom:1.25em}
blockquote{margin:0 0 1.25em;padding:.5625em 1.25em 0 1.1875em;border-left:1px solid #ddd}
blockquote,blockquote p{line-height:1.6;color:rgba(0,0,0,.85)}
@media screen and (min-width:768px){h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2}
h1{font-size:2.75em}
h2{font-size:2.3125em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.6875em}
h4{font-size:1.4375em}}
table{background:#fff;margin-bottom:1.25em;border:1px solid #dedede;word-wrap:normal}
table thead,table tfoot{background:#f7f8f7}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8)}
table tr.even,table tr.alt{background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{line-height:1.6}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2;word-spacing:-.05em}
h1 strong,h2 strong,h3 strong,#toctitle strong,.sidebarblock>.content>.title strong,h4 strong,h5 strong,h6 strong{font-weight:400}
.center{margin-left:auto;margin-right:auto}
.stretch{width:100%}
.clearfix::before,.clearfix::after,.float-group::before,.float-group::after{content:" ";display:table}
.clearfix::after,.float-group::after{clear:both}
:not(pre).nobreak{word-wrap:normal}
:not(pre).nowrap{white-space:nowrap}
:not(pre).pre-wrap{white-space:pre-wrap}
:not(pre):not([class^=L])>code{font-size:.9375em;font-style:normal!important;letter-spacing:0;padding:.1em .5ex;word-spacing:-.15em;background:#f7f7f8;border-radius:4px;line-height:1.45;text-rendering:optimizeSpeed}
pre{color:rgba(0,0,0,.9);font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;line-height:1.45;text-rendering:optimizeSpeed}
pre code,pre pre{color:inherit;font-size:inherit;line-height:inherit}
pre>code{display:block}
pre.nowrap,pre.nowrap pre{white-space:pre;word-wrap:normal}
em em{font-style:normal}
strong strong{font-weight:400}
.keyseq{color:rgba(51,51,51,.8)}
kbd{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;display:inline-block;color:rgba(0,0,0,.8);font-size:.65em;line-height:1.45;background:#f7f7f7;border:1px solid #ccc;border-radius:3px;box-shadow:0 1px 0 rgba(0,0,0,.2),inset 0 0 0 .1em #fff;margin:0 .15em;padding:.2em .5em;vertical-align:middle;position:relative;top:-.1em;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}
.menuseq,.menuref{color:#000}
.menuseq b:not(.caret),.menuref{font-weight:inherit}
.menuseq{word-spacing:-.02em}
.menuseq b.caret{font-size:1.25em;line-height:.8}
.menuseq i.caret{font-weight:bold;text-align:center;width:.45em}
b.button::before,b.button::after{position:relative;top:-1px;font-weight:400}
b.button::before{content:"[";padding:0 3px 0 2px}
b.button::after{content:"]";padding:0 2px 0 3px}
p a>code:hover{color:rgba(0,0,0,.9)}
#header,#content,#footnotes,#footer{width:100%;margin:0 auto;max-width:62.5em;*zoom:1;position:relative;padding-left:.9375em;padding-right:.9375em}
#header::before,#header::after,#content::before,#content::after,#footnotes::before,#footnotes::after,#footer::before,#footer::after{content:" ";display:table}
#header::after,#content::after,#footnotes::after,#footer::after{clear:both}
#content{margin-top:1.25em}
#content::before{content:none}
#header>h1:first-child{color:rgba(0,0,0,.85);margin-top:2.25rem;margin-bottom:0}
#header>h1:first-child+#toc{margin-top:8px;border-top:1px solid #dddddf}
#header>h1:only-child{border-bottom:1px solid #dddddf;padding-bottom:8px}
#header .details{border-bottom:1px solid #dddddf;line-height:1.45;padding-top:.25em;padding-bottom:.25em;padding-left:.25em;color:rgba(0,0,0,.6);display:flex;flex-flow:row wrap}
#header .details span:first-child{margin-left:-.125em}
#header .details span.email a{color:rgba(0,0,0,.85)}
#header .details br{display:none}
#header .details br+span::before{content:"\00a0\2013\00a0"}
#header .details br+span.author::before{content:"\00a0\22c5\00a0";color:rgba(0,0,0,.85)}
#header .details br+span#revremark::before{content:"\00a0|\00a0"}
#header #revnumber{text-transform:capitalize}
#header #revnumber::after{content:"\00a0"}
#content>h1:first-child:not([class]){color:rgba(0,0,0,.85);border-bottom:1px solid #dddddf;padding-bottom:8px;margin-top:0;padding-top:1rem;margin-bottom:1.25rem}
#toc{border-bottom:1px solid #e7e7e9;padding-bottom:.5em}
#toc>ul{margin-left:.125em}
#toc ul.sectlevel0>li>a{font-style:italic}
#toc ul.sectlevel0 ul.sectlevel1{margin:.5em 0}
#toc ul{font-family:"Open Sans","DejaVu Sans",sans-serif;list-style-type:none}
#toc li{line-height:1.3334;margin-top:.3334em}
#toc a{text-decoration:none}
#toc a:active{text-decoration:underline}
#toctitle{color:#7a2518;font-size:1.2em}
@media screen and (min-width:768px){#toctitle{font-size:1.375em}
body.toc2{padding-left:15em;padding-right:0}
body.toc2 #header>h1:nth-last-child(2){border-bottom:1px solid #dddddf;padding-bottom:8px}
#toc.toc2{margin-top:0!important;background:#f8f8f7;position:fixed;width:15em;left:0;top:0;border-right:1px solid #e7e7e9;border-top-width:0!important;border-bottom-width:0!important;z-index:1000;padding:1.25em 1em;height:100%;overflow:auto}
#toc.toc2 #toctitle{margin-top:0;margin-bottom:.8rem;font-size:1.2em}
#toc.toc2>ul{font-size:.9em;margin-bottom:0}
#toc.toc2 ul ul{margin-left:0;padding-left:1em}
#toc.toc2 ul.sectlevel0 ul.sectlevel1{padding-left:0;margin-top:.5em;margin-bottom:.5em}
body.toc2.toc-right{padding-left:0;padding-right:15em}
body.toc2.toc-right #toc.toc2{border-right-width:0;border-left:1px solid #e7e7e9;left:auto;right:0}}
@media screen and (min-width:1280px){body.toc2{padding-left:20em;padding-right:0}
#toc.toc2{width:20em}
#toc.toc2 #toctitle{font-size:1.375em}
#toc.toc2>ul{font-size:.95em}
#toc.toc2 ul ul{padding-left:1.25em}
body.toc2.toc-right{padding-left:0;padding-right:20em}}
#content #toc{border:1px solid #e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;border-radius:4px}
#content #toc>:first-child{margin-top:0}
#content #toc>:last-child{margin-bottom:0}
#footer{max-width:none;background:rgba(0,0,0,.8);padding:1.25em}
#footer-text{color:hsla(0,0%,100%,.8);line-height:1.44}
#content{margin-bottom:.625em}
.sect1{padding-bottom:.625em}
@media screen and (min-width:768px){#content{margin-bottom:1.25em}
.sect1{padding-bottom:1.25em}}
.sect1:last-child{padding-bottom:0}
.sect1+.sect1{border-top:1px solid #e7e7e9}
#content h1>a.anchor,h2>a.anchor,h3>a.anchor,#toctitle>a.anchor,.sidebarblock>.content>.title>a.anchor,h4>a.anchor,h5>a.anchor,h6>a.anchor{position:absolute;z-index:1001;width:1.5ex;margin-left:-1.5ex;display:block;text-decoration:none!important;visibility:hidden;text-align:center;font-weight:400}
#content h1>a.anchor::before,h2>a.anchor::before,h3>a.anchor::before,#toctitle>a.anchor::before,.sidebarblock>.content>.title>a.anchor::before,h4>a.anchor::before,h5>a.anchor::before,h6>a.anchor::before{content:"\00A7";font-size:.85em;display:block;padding-top:.1em}
#content h1:hover>a.anchor,#content h1>a.anchor:hover,h2:hover>a.anchor,h2>a.anchor:hover,h3:hover>a.anchor,#toctitle:hover>a.anchor,.sidebarblock>.content>.title:hover>a.anchor,h3>a.anchor:hover,#toctitle>a.anchor:hover,.sidebarblock>.content>.title>a.anchor:hover,h4:hover>a.anchor,h4>a.anchor:hover,h5:hover>a.anchor,h5>a.anchor:hover,h6:hover>a.anchor,h6>a.anchor:hover{visibility:visible}
#content h1>a.link,h2>a.link,h3>a.link,#toctitle>a.link,.sidebarblock>.content>.title>a.link,h4>a.link,h5>a.link,h6>a.link{color:#ba3925;text-decoration:none}
#content h1>a.link:hover,h2>a.link:hover,h3>a.link:hover,#toctitle>a.link:hover,.sidebarblock>.content>.title>a.link:hover,h4>a.link:hover,h5>a.link:hover,h6>a.link:hover{color:#a53221}
details,.audioblock,.imageblock,.literalblock,.listingblock,.stemblock,.videoblock{margin-bottom:1.25em}
details{margin-left:1.25rem}
details>summary{cursor:pointer;display:block;position:relative;line-height:1.6;margin-bottom:.625rem;outline:none;-webkit-tap-highlight-color:transparent}
details>summary::-webkit-details-marker{display:none}
details>summary::before{content:"";border:solid transparent;border-left:solid;border-width:.3em 0 .3em .5em;position:absolute;top:.5em;left:-1.25rem;transform:translateX(15%)}
details[open]>summary::before{border:solid transparent;border-top:solid;border-width:.5em .3em 0;transform:translateY(15%)}
details>summary::after{content:"";width:1.25rem;height:1em;position:absolute;top:.3em;left:-1.25rem}
.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{text-rendering:optimizeLegibility;text-align:left;font-family:"Noto Serif","DejaVu Serif",serif;font-size:1rem;font-style:italic}
table.tableblock.fit-content>caption.title{white-space:nowrap;width:0}
.paragraph.lead>p,#preamble>.sectionbody>[class=paragraph]:first-of-type p{font-size:1.21875em;line-height:1.6;color:rgba(0,0,0,.85)}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;width:80px}
.admonitionblock>table td.icon img{max-width:none}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #dddddf;color:rgba(0,0,0,.6);word-wrap:anywhere}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.exampleblock>.content{border:1px solid #e6e6e6;margin-bottom:1.25em;padding:1.25em;background:#fff;border-radius:4px}
.sidebarblock{border:1px solid #dbdbd6;margin-bottom:1.25em;padding:1.25em;background:#f3f3f2;border-radius:4px}
.sidebarblock>.content>.title{color:#7a2518;margin-top:0;text-align:center}
.exampleblock>.content>:first-child,.sidebarblock>.content>:first-child{margin-top:0}
.exampleblock>.content>:last-child,.exampleblock>.content>:last-child>:last-child,.exampleblock>.content .olist>ol>li:last-child>:last-child,.exampleblock>.content .ulist>ul>li:last-child>:last-child,.exampleblock>.content .qlist>ol>li:last-child>:last-child,.sidebarblock>.content>:last-child,.sidebarblock>.content>:last-child>:last-child,.sidebarblock>.content .olist>ol>li:last-child>:last-child,.sidebarblock>.content .ulist>ul>li:last-child>:last-child,.sidebarblock>.content .qlist>ol>li:last-child>:last-child{margin-bottom:0}
.literalblock pre,.listingblock>.content>pre{border-radius:4px;overflow-x:auto;padding:1em;font-size:.8125em}
@media screen and (min-width:768px){.literalblock pre,.listingblock>.content>pre{font-size:.90625em}}
@media screen and (min-width:1280px){.literalblock pre,.listingblock>.content>pre{font-size:1em}}
.literalblock pre,.listingblock>.content>pre:not(.highlight),.listingblock>.content>pre[class=highlight],.listingblock>.content>pre[class^="highlight "]{background:#f7f7f8}
.literalblock.output pre{color:#f7f7f8;background:rgba(0,0,0,.9)}
.listingblock>.content{position:relative}
.listingblock code[data-lang]::before{display:none;content:attr(data-lang);position:absolute;font-size:.75em;top:.425rem;right:.5rem;line-height:1;text-transform:uppercase;color:inherit;opacity:.5}
.listingblock:hover code[data-lang]::before{display:block}
.listingblock.terminal pre .command::before{content:attr(data-prompt);padding-right:.5em;color:inherit;opacity:.5}
.listingblock.terminal pre .command:not([data-prompt])::before{content:"$"}
.listingblock pre.highlightjs{padding:0}
.listingblock pre.highlightjs>code{padding:1em;border-radius:4px}
.listingblock pre.prettyprint{border-width:0}
.prettyprint{background:#f7f7f8}
pre.prettyprint .linenums{line-height:1.45;margin-left:2em}
pre.prettyprint li{background:none;list-style-type:inherit;padding-left:0}
pre.prettyprint li code[data-lang]::before{opacity:1}
pre.prettyprint li:not(:first-child) code[data-lang]::before{display:none}
table.linenotable{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.linenotable td[class]{color:inherit;vertical-align:top;padding:0;line-height:inherit;white-space:normal}
table.linenotable td.code{padding-left:.75em}
table.linenotable td.linenos,pre.pygments .linenos{border-right:1px solid;opacity:.35;padding-right:.5em;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}
pre.pygments span.linenos{display:inline-block;margin-right:.75em}
.quoteblock{margin:0 1em 1.25em 1.5em;display:table}
.quoteblock:not(.excerpt)>.title{margin-left:-1.5em;margin-bottom:.75em}
.quoteblock blockquote,.quoteblock p{color:rgba(0,0,0,.85);font-size:1.15rem;line-height:1.75;word-spacing:.1em;letter-spacing:0;font-style:italic;text-align:justify}
.quoteblock blockquote{margin:0;padding:0;border:0}
.quoteblock blockquote::before{content:"\201c";float:left;font-size:2.75em;font-weight:bold;line-height:.6em;margin-left:-.6em;color:#7a2518;text-shadow:0 1px 2px rgba(0,0,0,.1)}
.quoteblock blockquote>.paragraph:last-child p{margin-bottom:0}
.quoteblock .attribution{margin-top:.75em;margin-right:.5ex;text-align:right}
.verseblock{margin:0 1em 1.25em}
.verseblock pre{font-family:"Open Sans","DejaVu Sans",sans-serif;font-size:1.15rem;color:rgba(0,0,0,.85);font-weight:300;text-rendering:optimizeLegibility}
.verseblock pre strong{font-weight:400}
.verseblock .attribution{margin-top:1.25rem;margin-left:.5ex}
.quoteblock .attribution,.verseblock .attribution{font-size:.9375em;line-height:1.45;font-style:italic}
.quoteblock .attribution br,.verseblock .attribution br{display:none}
.quoteblock .attribution cite,.verseblock .attribution cite{display:block;letter-spacing:-.025em;color:rgba(0,0,0,.6)}
.quoteblock.abstract blockquote::before,.quoteblock.excerpt blockquote::before,.quoteblock .quoteblock blockquote::before{display:none}
.quoteblock.abstract blockquote,.quoteblock.abstract p,.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{line-height:1.6;word-spacing:0}
.quoteblock.abstract{margin:0 1em 1.25em;display:block}
.quoteblock.abstract>.title{margin:0 0 .375em;font-size:1.15em;text-align:center}
.quoteblock.excerpt>blockquote,.quoteblock .quoteblock{padding:0 0 .25em 1em;border-left:.25em solid #dddddf}
.quoteblock.excerpt,.quoteblock .quoteblock{margin-left:0}
.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{color:inherit;font-size:1.0625rem}
.quoteblock.excerpt .attribution,.quoteblock .quoteblock .attribution{color:inherit;font-size:.85rem;text-align:left;margin-right:0}
p.tableblock:last-child{margin-bottom:0}
td.tableblock>.content{margin-bottom:1.25em;word-wrap:anywhere}
td.tableblock>.content>:last-child{margin-bottom:-1.25em}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all>*>tr>*{border-width:1px}
table.grid-cols>*>tr>*{border-width:0 1px}
table.grid-rows>*>tr>*{border-width:1px 0}
table.frame-all{border-width:1px}
table.frame-ends{border-width:1px 0}
table.frame-sides{border-width:0 1px}
table.frame-none>colgroup+*>:first-child>*,table.frame-sides>colgroup+*>:first-child>*{border-top-width:0}
table.frame-none>:last-child>:last-child>*,table.frame-sides>:last-child>:last-child>*{border-bottom-width:0}
table.frame-none>*>tr>:first-child,table.frame-ends>*>tr>:first-child{border-left-width:0}
table.frame-none>*>tr>:last-child,table.frame-ends>*>tr>:last-child{border-right-width:0}
table.stripes-all>*>tr,table.stripes-odd>*>tr:nth-of-type(odd),table.stripes-even>*>tr:nth-of-type(even),table.stripes-hover>*>tr:hover{background:#f8f8f7}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
p.tableblock{font-size:1em}
ol{margin-left:1.75em}
ul li ol{margin-left:1.5em}
dl dd{margin-left:1.125em}
dl dd:last-child,dl dd:last-child>:last-child{margin-bottom:0}
li p,ul dd,ol dd,.olist .olist,.ulist .ulist,.ulist .olist,.olist .ulist{margin-bottom:.625em}
ul.checklist,ul.none,ol.none,ul.no-bullet,ol.no-bullet,ol.unnumbered,ul.unstyled,ol.unstyled{list-style-type:none}
ul.no-bullet,ol.no-bullet,ol.unnumbered{margin-left:.625em}
ul.unstyled,ol.unstyled{margin-left:0}
li>p:empty:only-child::before{content:"";display:inline-block}
ul.checklist>li>p:first-child{margin-left:-1em}
ul.checklist>li>p:first-child>.fa-square-o:first-child,ul.checklist>li>p:first-child>.fa-check-square-o:first-child{width:1.25em;font-size:.8em;position:relative;bottom:.125em}
ul.checklist>li>p:first-child>input[type=checkbox]:first-child{margin-right:.25em}
ul.inline{display:flex;flex-flow:row wrap;list-style:none;margin:0 0 .625em -1.25em}
ul.inline>li{margin-left:1.25em}
.unstyled dl dt{font-weight:400;font-style:normal}
ol.arabic{list-style-type:decimal}
ol.decimal{list-style-type:decimal-leading-zero}
ol.loweralpha{list-style-type:lower-alpha}
ol.upperalpha{list-style-type:upper-alpha}
ol.lowerroman{list-style-type:lower-roman}
ol.upperroman{list-style-type:upper-roman}
ol.lowergreek{list-style-type:lower-greek}
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1,td.hdlist2{vertical-align:top;padding:0 .625em}
td.hdlist1{font-weight:bold;padding-bottom:1.25em}
td.hdlist2{word-wrap:anywhere}
.literalblock+.colist,.listingblock+.colist{margin-top:-.5em}
.colist td:not([class]):first-child{padding:.4em .75em 0;line-height:1;vertical-align:top}
.colist td:not([class]):first-child img{max-width:none}
.colist td:not([class]):last-child{padding:.25em 0}
.thumb,.th{line-height:0;display:inline-block;border:4px solid #fff;box-shadow:0 0 0 1px #ddd}
.imageblock.left{margin:.25em .625em 1.25em 0}
.imageblock.right{margin:.25em 0 1.25em .625em}
.imageblock>.title{margin-bottom:0}
.imageblock.thumb,.imageblock.th{border-width:6px}
.imageblock.thumb>.title,.imageblock.th>.title{padding:0 .125em}
.image.left,.image.right{margin-top:.25em;margin-bottom:.25em;display:inline-block;line-height:0}
.image.left{margin-right:.625em}
.image.right{margin-left:.625em}
a.image{text-decoration:none;display:inline-block}
a.image object{pointer-events:none}
sup.footnote,sup.footnoteref{font-size:.875em;position:static;vertical-align:super}
sup.footnote a,sup.footnoteref a{text-decoration:none}
sup.footnote a:active,sup.footnoteref a:active,#footnotes .footnote a:first-of-type:active{text-decoration:underline}
#footnotes{padding-top:.75em;padding-bottom:.75em;margin-bottom:.625em}
#footnotes hr{width:20%;min-width:6.25em;margin:-.25em 0 .75em;border-width:1px 0 0}
#footnotes .footnote{padding:0 .375em 0 .225em;line-height:1.3334;font-size:.875em;margin-left:1.2em;margin-bottom:.2em}
#footnotes .footnote a:first-of-type{font-weight:bold;text-decoration:none;margin-left:-1.05em}
#footnotes .footnote:last-of-type{margin-bottom:0}
#content #footnotes{margin-top:-.625em;margin-bottom:0;padding:.75em 0}
div.unbreakable{page-break-inside:avoid}
.big{font-size:larger}
.small{font-size:smaller}
.underline{text-decoration:underline}
.overline{text-decoration:overline}
.line-through{text-decoration:line-through}
.aqua{color:#00bfbf}
.aqua-background{background:#00fafa}
.black{color:#000}
.black-background{background:#000}
.blue{color:#0000bf}
.blue-background{background:#0000fa}
.fuchsia{color:#bf00bf}
.fuchsia-background{background:#fa00fa}
.gray{color:#606060}
.gray-background{background:#7d7d7d}
.green{color:#006000}
.green-background{background:#007d00}
.lime{color:#00bf00}
.lime-background{background:#00fa00}
.maroon{color:#600000}
.maroon-background{background:#7d0000}
.navy{color:#000060}
.navy-background{background:#00007d}
.olive{color:#606000}
.olive-background{background:#7d7d00}
.purple{color:#600060}
.purple-background{background:#7d007d}
.red{color:#bf0000}
.red-background{background:#fa0000}
.silver{color:#909090}
.silver-background{background:#bcbcbc}
.teal{color:#006060}
.teal-background{background:#007d7d}
.white{color:#bfbfbf}
.white-background{background:#fafafa}
.yellow{color:#bfbf00}
.yellow-background{background:#fafa00}
span.icon>.fa{cursor:default}
a span.icon>.fa{cursor:inherit}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note::before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip::before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning::before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution::before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important::before{content:"\f06a";color:#bf0000}
.conum[data-value]{display:inline-block;color:#fff!important;background:rgba(0,0,0,.8);border-radius:50%;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]::after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
dt,th.tableblock,td.content,div.footnote{text-rendering:optimizeLegibility}
h1,h2,p,td.content,span.alt,summary{letter-spacing:-.01em}
p strong,td.content strong,div.footnote strong{letter-spacing:-.005em}
p,blockquote,dt,td.content,td.hdlist1,span.alt,summary{font-size:1.0625rem}
p{margin-bottom:1.25rem}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock{font-size:1em}
.exampleblock>.content{background:#fffef7;border-color:#e0e0dc;box-shadow:0 1px 4px #e0e0dc}
.print-only{display:none!important}
@page{margin:1.25cm .75cm}
@media print{*{box-shadow:none!important;text-shadow:none!important}
html{font-size:80%}
a{color:inherit!important;text-decoration:underline!important}
a.bare,a[href^="#"],a[href^="mailto:"]{text-decoration:none!important}
a[href^="http:"]:not(.bare)::after,a[href^="https:"]:not(.bare)::after{content:"(" attr(href) ")";display:inline-block;font-size:.875em;padding-left:.25em}
abbr[title]{border-bottom:1px dotted}
abbr[title]::after{content:" (" attr(title) ")"}
pre,blockquote,tr,img,object,svg{page-break-inside:avoid}
thead{display:table-header-group}
svg{max-width:100%}
p,blockquote,dt,td.content{font-size:1em;orphans:3;widows:3}
h2,h3,#toctitle,.sidebarblock>.content>.title{page-break-after:avoid}
#header,#content,#footnotes,#footer{max-width:none}
#toc,.sidebarblock,.exampleblock>.content{background:none!important}
#toc{border-bottom:1px solid #dddddf!important;padding-bottom:0!important}
body.book #header{text-align:center}
body.book #header>h1:first-child{border:0!important;margin:2.5em 0 1em}
body.book #header .details{border:0!important;display:block;padding:0!important}
body.book #header .details span:first-child{margin-left:0!important}
body.book #header .details br{display:block}
body.book #header .details br+span::before{content:none!important}
body.book #toc{border:0!important;text-align:left!important;padding:0!important;margin:0!important}
body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-break-before:always}
.listingblock code[data-lang]::before{display:block}
#footer{padding:0 .9375em}
.hide-on-print{display:none!important}
.print-only{display:block!important}
.hide-for-print{display:none!important}
.show-for-print{display:inherit!important}}
@media amzn-kf8,print{#header>h1:first-child{margin-top:1.25rem}
.sect1{padding:0!important}
.sect1+.sect1{border:0}
#footer{background:none}
#footer-text{color:rgba(0,0,0,.6);font-size:.9em}}
@media amzn-kf8{#header,#content,#footnotes,#footer{padding:0}}
</style>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"/>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.3/styles/monokai.min.css"/>
</head>
<body class="book toc2 toc-left">
<div id="header">
<h1>Adopting a Red Hat OpenStack Platform 17.1 deployment</h1>
<div id="toc" class="toc2">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#planning-the-new-deployment_assembly">Planning the new deployment</a>
<ul class="sectlevel2">
<li><a href="#service-configurations_planning">Service configurations</a></li>
<li><a href="#about-node-roles_planning">About node roles</a></li>
<li><a href="#about-node-selector_planning">About node selector</a></li>
<li><a href="#about-machine-configs_planning">About machine configs</a></li>
<li><a href="#key-manager-service-support-for-crypto-plug-ins_planning">Key Manager service support for crypto plug-ins</a></li>
<li><a href="#configuring-network-for-RHOSO-deployment_planning">Configuring the network for the RHOSO deployment</a>
<ul class="sectlevel3">
<li><a href="#retrieving-the-network-configuration_configuring-network">Retrieving the network configuration from your existing deployment</a></li>
<li><a href="#planning-your-ipam-configuration_configuring-network">Planning your IPAM configuration</a></li>
<li><a href="#configuring-isolated-networks_ipam-configuration">Configuring isolated networks</a></li>
</ul>
</li>
<li><a href="#storage-requirements_isolated-networks">Storage requirements</a>
<ul class="sectlevel3">
<li><a href="#storage-driver-certification_storage-requirements">Storage driver certification</a></li>
<li><a href="#block-storage-requirements_storage-requirements">Block Storage service requirements</a></li>
</ul>
</li>
<li><a href="#comparing-configuration-files-between-deployments_storage-requirements">Comparing configuration files between deployments</a></li>
</ul>
</li>
<li><a href="#migrating-tls-everywhere_storage-requirements">Migrating TLS-e to the RHOSO deployment</a></li>
<li><a href="#migrating-databases-to-the-control-plane_storage-requirements">Migrating databases to the control plane</a>
<ul class="sectlevel2">
<li><a href="#proc_retrieving-topology-specific-service-configuration_migrating-databases">Retrieving topology-specific service configuration</a></li>
<li><a href="#deploying-backend-services_migrating-databases">Deploying back-end services</a></li>
<li><a href="#configuring-a-ceph-backend_migrating-databases">Configuring a Ceph backend</a></li>
<li><a href="#creating-a-ceph-nfs-cluster_migrating-databases">Creating a NFS Ganesha cluster</a></li>
<li><a href="#stopping-openstack-services_migrating-databases">Stopping Red&#160;Hat OpenStack Platform services</a></li>
<li><a href="#migrating-databases-to-mariadb-instances_migrating-databases">Migrating databases to MariaDB instances</a></li>
<li><a href="#migrating-ovn-data_migrating-databases">Migrating OVN data</a></li>
</ul>
</li>
<li><a href="#adopting-openstack-control-plane-services_migrating-databases">Adopting Red&#160;Hat OpenStack Platform control plane services</a>
<ul class="sectlevel2">
<li><a href="#adopting-the-identity-service_adopt-control-plane">Adopting the Identity service</a></li>
<li><a href="#adopting-the-key-manager-service_adopt-control-plane">Adopting the Key Manager service</a></li>
<li><a href="#adopting-the-object-storage-service_adopt-control-plane">Adopting the Object Storage service</a></li>
<li><a href="#adopting-the-image-service_adopt-control-plane">Adopting the Image service</a>
<ul class="sectlevel3">
<li><a href="#adopting-image-service-with-object-storage-backend_image-service">Adopting the Image service that is deployed with a Object Storage service back end</a></li>
<li><a href="#adopting-image-service-with-block-storage-backend_image-service">Adopting the Image service that is deployed with a Block Storage service back end</a></li>
<li><a href="#adopting-image-service-with-nfs-backend_image-service">Adopting the Image service that is deployed with an NFS back end</a></li>
<li><a href="#adopting-image-service-with-ceph-backend_image-service">Adopting the Image service that is deployed with a Red Hat Ceph Storage back end</a></li>
<li><a href="#verifying-the-image-service-adoption_image-service">Verifying the Image service adoption</a></li>
</ul>
</li>
<li><a href="#adopting-the-placement-service_image-service">Adopting the Placement service</a></li>
<li><a href="#adopting-the-compute-service_image-service">Adopting the Compute service</a></li>
<li><a href="#adopting-the-block-storage-service_image-service">Adopting the Block Storage service</a>
<ul class="sectlevel3">
<li><a href="#block-storage-limitations_adopting-block-storage">Limitations for adopting the Block Storage service</a></li>
<li><a href="#openshift-preparation-for-block-storage-adoption_adopting-block-storage">Red Hat OpenShift Container Platform preparation for Block Storage service adoption</a></li>
<li><a href="#preparing-the-block-storage-service_adopting-block-storage">Preparing the Block Storage service configurations for adoption</a></li>
<li><a href="#deploying-the-block-storage-services_preparing-block-storage">Deploying the Block Storage services</a></li>
</ul>
</li>
<li><a href="#adopting-the-openstack-dashboard_preparing-block-storage">Adopting the Dashboard service</a></li>
<li><a href="#adopting-the-shared-file-systems-service_preparing-block-storage">Adopting the Shared File Systems service</a>
<ul class="sectlevel3">
<li><a href="#changes-to-cephFS-through-NFS_adopting-shared-file-systems">Changes to CephFS through NFS</a></li>
<li><a href="#deploying-file-systems-service-control-plane_adopting-shared-file-systems">Deploying the Shared File Systems service control plane</a></li>
<li><a href="#decommissioning-RHOSP-standalone-Ceph-NFS-service_adopting-shared-file-systems">Decommissioning the Red&#160;Hat OpenStack Platform standalone Ceph NFS service</a></li>
</ul>
</li>
<li><a href="#adopting-the-orchestration-service_adopting-shared-file-systems">Adopting the Orchestration service</a></li>
<li><a href="#adopting-telemetry-services_adopting-shared-file-systems">Adopting Telemetry services</a></li>
<li><a href="#adopting-autoscaling_adopting-shared-file-systems">Adopting Autoscaling services</a></li>
<li><a href="#reviewing-the-openstack-control-plane-configuration_adopting-shared-file-systems">Reviewing the Red&#160;Hat OpenStack Platform control plane configuration</a>
<ul class="sectlevel3">
<li><a href="#pulling-configuration-from-tripleo-deployment_reviewing-configuration">Pulling the configuration from a director deployment</a></li>
</ul>
</li>
<li><a href="#rolling-back-control-plane-adoption_reviewing-configuration">Rolling back the control plane adoption</a></li>
</ul>
</li>
<li><a href="#adopting-data-plane_reviewing-configuration">Adopting the data plane</a>
<ul class="sectlevel2">
<li><a href="#stopping-infrastructure-management-and-compute-services_data-plane">Stopping infrastructure management and Compute services</a></li>
<li><a href="#adopting-compute-services-to-the-data-plane_data-plane">Adopting Compute services to the RHOSO data plane</a></li>
<li><a href="#performing-a-fast-forward-upgrade-on-compute-services_data-plane">Performing a fast-forward upgrade on Compute services</a></li>
<li><a href="#adopting-networker-services-to-the-data-plane_data-plane">Adopting Networker services to the RHOSO data plane</a></li>
</ul>
</li>
<li><a href="#migrating-the-object-storage-service_data-plane">Migrating the Object Storage service (swift) to Red&#160;Hat OpenStack Services on OpenShift (RHOSO) nodes</a>
<ul class="sectlevel2">
<li><a href="#migrating-object-storage-data-to-rhoso-nodes_migrate-object-storage-service">Migrating the Object Storage service (swift) data from RHOSP to Red&#160;Hat OpenStack Services on OpenShift (RHOSO) nodes</a></li>
<li><a href="#troubleshooting-object-storage-migration_migrate-object-storage-service">Troubleshooting the Object Storage service (swift) migration</a></li>
</ul>
</li>
<li><a href="#ceph-migration_migrate-object-storage-service">Migrating the Red Hat Ceph Storage Cluster</a>
<ul class="sectlevel2">
<li><a href="#ceph-daemon-cardinality_migrate-object-storage-service">Red Hat Ceph Storage daemon cardinality</a></li>
<li><a href="#migrating-ceph-monitoring_migrate-object-storage-service">Migrating the monitoring stack component to new nodes within an existing Red Hat Ceph Storage cluster</a>
<ul class="sectlevel3">
<li><a href="#completing-prerequisites-for-migrating-ceph-monitoring-stack_migrating-ceph-monitoring">Completing prerequisites for a Red Hat Ceph Storage cluster with monitoring stack components</a></li>
<li><a href="#migrating-monitoring-stack-to-target-nodes_migrating-ceph-monitoring">Migrating the monitoring stack to the target nodes</a></li>
</ul>
</li>
<li><a href="#migrating-ceph-mds_migrating-monitoring-stack">Migrating Red Hat Ceph Storage MDS to new nodes within the existing cluster</a></li>
<li><a href="#migrating-ceph-rgw_migrating-monitoring-stack">Migrating Red Hat Ceph Storage RGW to external RHEL nodes</a>
<ul class="sectlevel3">
<li><a href="#completing-prerequisites-for-migrating-ceph-rgw_migrating-ceph-rgw">Completing prerequisites for migrating Red Hat Ceph Storage RGW</a></li>
<li><a href="#migrating-the-rgw-backends_migrating-ceph-rgw">Migrating the Red Hat Ceph Storage RGW backends</a></li>
<li><a href="#deploying-a-ceph-ingress-daemon_migrating-ceph-rgw">Deploying a Red Hat Ceph Storage ingress daemon</a></li>
<li><a href="#updating-the-object-storage-endpoints_migrating-ceph-rgw">Updating the object-store endpoints</a></li>
</ul>
</li>
<li><a href="#migrating-ceph-rbd_migrating-ceph-rgw">Migrating Red Hat Ceph Storage RBD to external RHEL nodes</a>
<ul class="sectlevel3">
<li><a href="#_migrating_ceph_manager_daemons_to_red_hat_ceph_storage_nodes">Migrating Ceph Manager daemons to Red Hat Ceph Storage nodes</a></li>
<li><a href="#migrating-mon-from-controller-nodes_migrating-ceph-rbd">Migrating Ceph Monitor daemons to Red Hat Ceph Storage nodes</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="content">
<div class="sect1">
<h2 id="planning-the-new-deployment_assembly">Planning the new deployment</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Just like you did back when you installed your director-deployed Red&#160;Hat OpenStack Platform, the
upgrade/migration to the control plane requires planning various aspects
of the environment such as node roles, planning your network topology, and
storage.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
This section contains considerations for adoption planning, but it is
important to read the whole adoption guide before actually starting
the process. You should form an understanding of the procedure,
prepare the necessary configuration snippets for each service ahead of
time, and test the procedure in a representative test environment
before adopting your main environment.
</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="service-configurations_planning">Service configurations</h3>
<div class="paragraph">
<p>There is a fundamental difference between the director and operator deployments
regarding the configuration of the services.</p>
</div>
<div class="paragraph">
<p>In director deployments many of the service configurations are abstracted by
director-specific configuration options. A single director option may trigger
changes for multiple services and support for drivers, for example, the Block Storage service (cinder), that
require patches to the director code base.</p>
</div>
<div class="paragraph">
<p>In operator deployments this approach has changed: reduce the installer specific knowledge and leverage Red Hat OpenShift Container Platform (RHOCP) and
Red&#160;Hat OpenStack Platform (RHOSP) service specific knowledge whenever possible.</p>
</div>
<div class="paragraph">
<p>To this effect RHOSP services will have sensible defaults for RHOCP deployments and human operators will provide configuration snippets to provide
necessary configuration, such as the Block Storage service backend configuration, or to override
the defaults.</p>
</div>
<div class="paragraph">
<p>This shortens the distance between a service specific configuration file (such
as <code>cinder.conf</code>) and what the human operator provides in the manifests.</p>
</div>
<div class="paragraph">
<p>These configuration snippets are passed to the operators in the different
<code>customServiceConfig</code> sections available in the manifests, and then they are
layered in the services available in the following levels. To illustrate this,
if you were to set a configuration at the top Block Storage service level (<code>spec: cinder:
template:</code>) then it would be applied to all the Block Storage services; for example to
enable debug in all the Block Storage services you would do:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: core.openstack.org/v1beta1
kind: OpenStackControlPlane
metadata:
  name: openstack
spec:
  cinder:
    template:
      customServiceConfig: |
        [DEFAULT]
        debug = True
&lt; . . . &gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>If you only want to set it for one of the Block Storage services, for example the
scheduler, then you use the <code>cinderScheduler</code> section instead:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: core.openstack.org/v1beta1
kind: OpenStackControlPlane
metadata:
  name: openstack
spec:
  cinder:
    template:
      cinderScheduler:
        customServiceConfig: |
          [DEFAULT]
          debug = True
&lt; . . . &gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>In Red Hat OpenShift Container Platform it is not recommended to store sensitive information like the
credentials to the Block Storage service storage array in the CRs, so most RHOSP operators
have a mechanism to use the Red Hat OpenShift Container Platform <code>Secrets</code> for sensitive configuration
parameters of the services and then use them by reference in the
<code>customServiceConfigSecrets</code> section which is analogous to the
<code>customServiceConfig</code>.</p>
</div>
<div class="paragraph">
<p>The contents of the <code>Secret</code> references passed in the
<code>customServiceConfigSecrets</code> will have the same format as <code>customServiceConfig</code>:
a snippet with the section/s and configuration options.</p>
</div>
<div class="paragraph">
<p>When there are sensitive information in the service configuration then it
becomes a matter of personal preference whether to store all the configuration
in the <code>Secret</code> or only the sensitive parts. However, if you split the
configuration between <code>Secret</code> and <code>customServiceConfig</code> you still need the
section header (eg: <code>[DEFAULT]</code>) to be present in both places.</p>
</div>
<div class="paragraph">
<p>Attention should be paid to each service&#8217;s adoption process as they may have
some particularities regarding their configuration.</p>
</div>
</div>
<div class="sect2">
<h3 id="about-node-roles_planning">About node roles</h3>
<div class="paragraph">
<p>In director deployments you had 5 different standard roles
for the nodes: <code>Controller</code>, <code>Compute</code>, <code>Networker</code>, <code>Ceph Storage</code>, <code>Swift
Storage</code>, but in the control plane you make a distinction based on where things
are running, in Red Hat OpenShift Container Platform (RHOCP) or external to it.</p>
</div>
<div class="paragraph">
<p>When adopting a director Red&#160;Hat OpenStack Platform (RHOSP) your <code>Compute</code> nodes will directly become
external nodes, so there should not be much additional planning needed there.
Your <code>Networker</code> nodes will also become external nodes.</p>
</div>
<div class="paragraph">
<p>In many deployments being adopted the <code>Controller</code> nodes will require some
thought because you have many RHOCP nodes where the Controller services
could run, and you have to decide which ones you want to use, how you are going to use them, and make sure those nodes are ready to run the services.</p>
</div>
<div class="paragraph">
<p>In most deployments running RHOSP services on <code>master</code> nodes can have a
seriously adverse impact on the RHOCP cluster, so it is recommended that you place RHOSP services on non <code>master</code> nodes.</p>
</div>
<div class="paragraph">
<p>By default RHOSP Operators deploy RHOSP services on any worker node, but
that is not necessarily what&#8217;s best for all deployments, and there may be even
services that won&#8217;t even work deployed like that.</p>
</div>
<div class="paragraph">
<p>When planing a deployment it&#8217;s good to remember that not all the services on an
RHOSP deployments are the same as they have very different requirements.</p>
</div>
<div class="paragraph">
<p>Looking at the Block Storage service (cinder) component you can clearly see different requirements for
its services: the cinder-scheduler is a very light service with low
memory, disk, network, and CPU usage; cinder-api service has a higher network
usage due to resource listing requests; the cinder-volume service will have a
high disk and network usage since many of its operations are in the data path
(offline volume migration, create volume from image, etc.), and then you have
the cinder-backup service which has high memory, network, and CPU (to compress
data) requirements.</p>
</div>
<div class="paragraph">
<p>The Image Service (glance) and Object Storage service (swift) components are in the data path, as well as RabbitMQ and Galera services.</p>
</div>
<div class="paragraph">
<p>Given these requirements it may be preferable not to let these services wander
all over your RHOCP worker nodes with the possibility of impacting other
workloads, or maybe you don&#8217;t mind the light services wandering around but you
want to pin down the heavy ones to a set of infrastructure nodes.</p>
</div>
<div class="paragraph">
<p>There are also hardware restrictions to take into consideration, because if you
are using a Fibre Channel (FC) Block Storage service backend you need the cinder-volume,
cinder-backup, and maybe even the Image Service (glance) (if it&#8217;s using the Block Storage service as a backend)
services to run on a RHOCP host that has an HBA.</p>
</div>
<div class="paragraph">
<p>The RHOSP Operators allow a great deal of flexibility on where to run the
RHOSP services, as you can use node labels to define which RHOCP nodes
are eligible to run the different RHOSP services.  Refer to the <a href="#about-node-selector_planning">About node
selector</a> to learn more about using labels to define
placement of the RHOSP services.</p>
</div>
</div>
<div class="sect2">
<h3 id="about-node-selector_planning">About node selector</h3>
<div class="paragraph">
<p>There are a variety of reasons why you might want to restrict the nodes where
Red&#160;Hat OpenStack Platform (RHOSP) services can be placed:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Hardware requirements: System memory, Disk space, Cores, HBAs</p>
</li>
<li>
<p>Limit the impact of the RHOSP services on other Red Hat OpenShift Container Platform workloads.</p>
</li>
<li>
<p>Avoid collocating RHOSP services.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The mechanism provided by the RHOSP operators to achieve this is through the
use of labels.</p>
</div>
<div class="paragraph">
<p>You either label the RHOCP nodes or use existing labels, and then use those labels in the RHOSP manifests in the
<code>nodeSelector</code> field.</p>
</div>
<div class="paragraph">
<p>The <code>nodeSelector</code> field in the RHOSP manifests follows the standard
RHOCP <code>nodeSelector</code> field. For more information, see <a href="https://docs.openshift.com/container-platform/4.15/nodes/scheduling/nodes-scheduler-node-selectors.html">About node selectors</a> in <em>OpenShift Container Platform 4.15 Documentation</em>.</p>
</div>
<div class="paragraph">
<p>This field is present at all the different levels of the RHOSP manifests:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Deployment: The <code>OpenStackControlPlane</code> object.</p>
</li>
<li>
<p>Component: For example the <code>cinder</code> element in the <code>OpenStackControlPlane</code>.</p>
</li>
<li>
<p>Service: For example the <code>cinderVolume</code> element within the <code>cinder</code> element
in the <code>OpenStackControlPlane</code>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This allows a fine grained control of the placement of the RHOSP services
with minimal repetition.</p>
</div>
<div class="paragraph">
<p>Values of the <code>nodeSelector</code> are propagated to the next levels unless they are
overwritten. This means that a <code>nodeSelector</code> value at the deployment level will
affect all the RHOSP services.</p>
</div>
<div class="paragraph">
<p>For example, you can add label <code>type: openstack</code> to any 3 RHOCP nodes:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc label nodes worker0 type=openstack
$ oc label nodes worker1 type=openstack
$ oc label nodes worker2 type=openstack</pre>
</div>
</div>
<div class="paragraph">
<p>And then in our <code>OpenStackControlPlane</code> you can use the label to place all the
services in those 3 nodes:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: core.openstack.org/v1beta1
kind: OpenStackControlPlane
metadata:
  name: openstack
spec:
  secret: osp-secret
  storageClass: local-storage
  nodeSelector:
    type: openstack
&lt; . . . &gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>You can use the selector for specific services. For example, you might want to place your the Block Storage service (cinder) volume and backup services on certain nodes if you are using FC and only have HBAs on a subset of
nodes. The following example assumes that you have the label <code>fc_card: true</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: core.openstack.org/v1beta1
kind: OpenStackControlPlane
metadata:
  name: openstack
spec:
  secret: osp-secret
  storageClass: local-storage
  cinder:
    template:
      cinderVolumes:
          pure_fc:
            nodeSelector:
              fc_card: true
&lt; . . . &gt;
          lvm-iscsi:
            nodeSelector:
              fc_card: true
&lt; . . . &gt;
      cinderBackup:
          nodeSelector:
            fc_card: true
&lt; . . . &gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>The Block Storage service operator does not currently have the possibility of defining
the <code>nodeSelector</code> in <code>cinderVolumes</code>, so you need to specify it on each of the
backends.</p>
</div>
<div class="paragraph">
<p>It is possible to leverage labels added by the  Node Feature Discovery (NFD) Operator to place RHOSP services. For more information, see <a href="https://docs.openshift.com/container-platform/4.13/hardware_enablement/psap-node-feature-discovery-operator.html">Node Feature Discovery Operator</a> in <em>OpenShift Container Platform 4.15 Documentation</em>.</p>
</div>
</div>
<div class="sect2">
<h3 id="about-machine-configs_planning">About machine configs</h3>
<div class="paragraph">
<p>Some services require you to have services or kernel modules running on the hosts where they run, for example <code>iscsid</code> or <code>multipathd</code> daemons, or the
<code>nvme-fabrics</code> kernel module.</p>
</div>
<div class="paragraph">
<p>For those cases you use <code>MachineConfig</code> manifests, and if you are restricting
the nodes that you are placing the Red&#160;Hat OpenStack Platform services on using the <code>nodeSelector</code> then
you also want to limit where the <code>MachineConfig</code> is applied.</p>
</div>
<div class="paragraph">
<p>To define where the <code>MachineConfig</code> can be applied, you need to use a
<code>MachineConfigPool</code> that links the <code>MachineConfig</code> to the nodes.</p>
</div>
<div class="paragraph">
<p>For example to be able to limit <code>MachineConfig</code> to the 3 Red Hat OpenShift Container Platform (RHOCP) nodes that you
marked with the <code>type: openstack</code> label, you create the
<code>MachineConfigPool</code> like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: openstack
spec:
  machineConfigSelector:
    matchLabels:
      machineconfiguration.openshift.io/role: openstack
  nodeSelector:
    matchLabels:
      type: openstack</code></pre>
</div>
</div>
<div class="paragraph">
<p>And then you use it in the <code>MachineConfig</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: openstack
&lt; . . . &gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>Refer to the <a href="https://docs.openshift.com/container-platform/4.15/post_installation_configuration/machine-configuration-tasks.html">Postinstallation machine configuration tasks</a> in <em>OpenShift Container Platform 4.15 Documentation</em>.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
Applying a <code>MachineConfig</code> to an RHOCP node makes the node reboot.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="key-manager-service-support-for-crypto-plug-ins_planning">Key Manager service support for crypto plug-ins</h3>
<div class="paragraph">
<p>The Key Manager service (barbican) does not yet support all of the crypto plug-ins available in director.</p>
</div>
</div>
<div class="sect2">
<h3 id="configuring-network-for-RHOSO-deployment_planning">Configuring the network for the RHOSO deployment</h3>
<div class="paragraph">
<p>With Red Hat OpenShift Container Platform (RHOCP), the network is a very important aspect of the deployment, and
it is important to plan it carefully. The general network requirements for the
Red&#160;Hat OpenStack Platform (RHOSP) services are not much different from the ones in a director deployment, but the way you handle them is.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
For more information about the network architecture and configuration, see
<a href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/18.0-dev-preview/html/deploying_red_hat_openstack_platform_18.0_development_preview_3_on_red_hat_openshift_container_platform/assembly_preparing-rhocp-for-rhosp"><em>Deploying Red Hat OpenStack Platform 18.0 Development Preview 3 on Red Hat OpenShift Container Platform</em></a> and <a href="https://docs.openshift.com/container-platform/4.15/networking/about-networking.html">About
networking</a> in <em>OpenShift Container Platform 4.15 Documentation</em>. This document will address concerns specific to adoption.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>When adopting a new RHOSP deployment, it is important to align the network
configuration with the adopted cluster to maintain connectivity for existing
workloads.</p>
</div>
<div class="paragraph">
<p>The following logical configuration steps will incorporate the existing network
configuration:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>configure <strong>RHOCP worker nodes</strong> to align VLAN tags and IPAM
configuration with the existing deployment.</p>
</li>
<li>
<p>configure <strong>Control Plane services</strong> to use compatible IP ranges for
service and load balancing IPs.</p>
</li>
<li>
<p>configure <strong>Data Plane nodes</strong> to use corresponding compatible configuration for
VLAN tags and IPAM.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Specifically,</p>
</div>
<div class="ulist">
<ul>
<li>
<p>IPAM configuration will either be reused from the
<strong>existing</strong> deployment or, depending on IP address availability in the
existing allocation pools, <strong>new</strong> ranges will be defined to be used for the
new control plane services. If so, <strong>IP routing</strong> will be configured between
the old and new ranges. For more information, see <a href="#planning-your-ipam-configuration_configuring-network">Planning your IPAM configuration</a>.</p>
</li>
<li>
<p>VLAN tags will be reused from the existing deployment.</p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="retrieving-the-network-configuration_configuring-network">Retrieving the network configuration from your existing deployment</h4>
<div class="paragraph">
<p>Let&#8217;s first determine which isolated networks are defined in the existing
deployment. You can find the network configuration in the <code>network_data.yaml</code>
file. For example,</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">- name: InternalApi
  mtu: 1500
  vip: true
  vlan: 20
  name_lower: internal_api
  dns_domain: internal.mydomain.tld.
  service_net_map_replace: internal
  subnets:
    internal_api_subnet:
      ip_subnet: '172.17.0.0/24'
      allocation_pools: [{'start': '172.17.0.4', 'end': '172.17.0.250'}]</code></pre>
</div>
</div>
<div class="paragraph">
<p>You should make a note of the VLAN tag used (<code>vlan</code> key) and the IP range
(<code>ip_subnet</code> key) for each isolated network. The IP range will later be split
into separate pools for control plane services and load balancer IP addresses.</p>
</div>
<div class="paragraph">
<p>You should also determine the list of IP addresses already consumed in the
adopted environment. Consult <code>tripleo-ansible-inventory.yaml</code> file to find this
information. In the file, for each listed host, note IP and VIP addresses
consumed by the node.</p>
</div>
<div class="paragraph">
<p>For example,</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">Standalone:
  hosts:
    standalone:
      ...
      internal_api_ip: 172.17.0.100
    ...
  ...
standalone:
  children:
    Standalone: {}
  vars:
    ...
    internal_api_vip: 172.17.0.2
    ...</code></pre>
</div>
</div>
<div class="paragraph">
<p>In the example above, note that the <code>172.17.0.2</code> and <code>172.17.0.100</code> are
consumed and won&#8217;t be available for the new control plane services, at least
until the adoption is complete.</p>
</div>
<div class="paragraph">
<p>Repeat the process for each isolated network and each host in the
configuration.</p>
</div>
<hr/>
<div class="paragraph">
<p>At the end of this process, you should have the following information:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A list of isolated networks used in the existing deployment.</p>
</li>
<li>
<p>For each of the isolated networks, the VLAN tag and IP ranges used for
dynamic address allocation.</p>
</li>
<li>
<p>A list of existing IP address allocations used in the environment. You will
later exclude these addresses from allocation pools available for the new
control plane services.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="planning-your-ipam-configuration_configuring-network">Planning your IPAM configuration</h4>
<div class="paragraph">
<p>The new deployment model puts additional burden on the size of IP allocation
pools available for Red&#160;Hat OpenStack Platform (RHOSP) services. This is because each service deployed
on Red Hat OpenShift Container Platform (RHOCP) worker nodes will now require an IP address from the IPAM pool (in
the previous deployment model, all services hosted on a controller node shared
the same IP address.)</p>
</div>
<div class="paragraph">
<p>Since the new control plane deployment has different requirements as to the
number of IP addresses available for services, it may even be impossible to
reuse the existing IP ranges used in adopted environment, depending on its
size. Prudent planning is required to determine which options are available in
your particular case.</p>
</div>
<div class="paragraph">
<p>The total number of IP addresses required for the new control plane services,
in each isolated network, is calculated as a sum of the following:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The number of RHOCP worker nodes. (Each node will require 1 IP address in
<code>NodeNetworkConfigurationPolicy</code> custom resources (CRs).)</p>
</li>
<li>
<p>The number of IP addresses required for the data plane nodes. (Each node will require
an IP address from <code>NetConfig</code> CRs.)</p>
</li>
<li>
<p>The number of IP addresses required for control plane services. (Each service
will require an IP address from <code>NetworkAttachmentDefinition</code> CRs.) This
number depends on the number of replicas for each service.</p>
</li>
<li>
<p>The number of IP addresses required for load balancer IP addresses. (Each
service will require a VIP address from <code>IPAddressPool</code> CRs.)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>As of the time of writing, the simplest single worker node RHOCP deployment
(CRC) has the following IP ranges defined (for the <code>internalapi</code> network):</p>
</div>
<div class="ulist">
<ul>
<li>
<p>1 IP address for the single worker node;</p>
</li>
<li>
<p>1 IP address for the data plane node;</p>
</li>
<li>
<p><code>NetworkAttachmentDefinition</code> CRs for control plane services:
<code>X.X.X.30-X.X.X.70</code> (41 addresses);</p>
</li>
<li>
<p><code>IPAllocationPool</code> CRs for load balancer IPs: <code>X.X.X.80-X.X.X.90</code> (11
addresses).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Which comes to a total of 54 IP addresses allocated to the <code>internalapi</code>
allocation pools.</p>
</div>
<div class="paragraph">
<p>The exact requirements may differ depending on the list of RHOSP services
to be deployed, their replica numbers, as well as the number of RHOCP
worker nodes and data plane nodes.</p>
</div>
<div class="paragraph">
<p>Additional IP addresses may be required in future RHOSP releases, so it is
advised to plan for some extra capacity, for each of the allocation pools used
in the new environment.</p>
</div>
<div class="paragraph">
<p>Once you know the required IP pool size for the new deployment, you can choose
one of the following scenarios to handle IPAM allocation in the new
environment.</p>
</div>
<div class="paragraph">
<p>The first listed scenario is more general and implies using new IP ranges,
while the second scenario implies reusing the existing ranges. The end state of
the former scenario is using the new subnet ranges for control plane services,
but keeping the old ranges, with their node IP address allocations intact, for
data plane nodes.</p>
</div>
<div class="paragraph">
<p>Regardless of the IPAM scenario, the VLAN tags used in the existing deployment will be reused in the new deployment. Depending on the scenario, the IP address ranges to be used for control plane services will be either reused from the old deployment or defined anew. Adjust the configuration as described in <a href="#configuring-isolated-networks_ipam-configuration">Configuring isolated networks</a>.</p>
</div>
<div class="sect4">
<h5 id="using-new-subnet-ranges_ipam-configuration">Scenario 1: Using new subnet ranges</h5>
<div class="paragraph">
<p>This scenario is compatible with any existing subnet configuration, and can be
used even when the existing cluster subnet ranges don&#8217;t have enough free IP
addresses for the new control plane services.</p>
</div>
<div class="paragraph">
<p>The general idea here is to define new IP ranges for control plane services
that belong to a different subnet that was not used in the existing cluster.
Then, configure link local IP routing between the old and new subnets to allow
old and new service deployments to communicate. This involves using director
mechanism on pre-adopted cluster to configure additional link local routes
there. This will allow EDP deployment to reach out to adopted nodes using their
old subnet addresses.</p>
</div>
<div class="paragraph">
<p>The new subnet should be sized appropriately to accommodate the new control
plane services, but otherwise doesn&#8217;t have any specific requirements as to the
existing deployment allocation pools already consumed. Actually, the
requirements as to the size of the new subnet are lower than in the second
scenario, as the old subnet ranges are kept for the adopted nodes, which means
they don&#8217;t consume any IP addresses from the new range.</p>
</div>
<div class="paragraph">
<p>In this scenario, you will configure <code>NetworkAttachmentDefinition</code> custom resources (CRs) to use a
different subnet from what will be configured in <code>NetConfig</code> CR for the same
networks. The former range will be used for control plane services,
while the latter will be used to manage IPAM for data plane nodes.</p>
</div>
<div class="paragraph">
<p>During the process, you will need to make sure that adopted node IP addresses
don&#8217;t change during the adoption process. This is achieved by listing the
addresses in <code>fixedIP</code> fields in <code>OpenstackDataplaneNodeSet</code> per-node section.</p>
</div>
<hr/>
<div class="paragraph">
<p>Before proceeding, configure host routes on the adopted nodes for the
control plane subnets.</p>
</div>
<div class="paragraph">
<p>To achieve this, you will need to re-run <code>tripleo deploy</code> with additional
<code>routes</code> entries added to <code>network_config</code>. (This change should be applied
for every adopted node configuration.) For example, you may add the following
to <code>net_config.yaml</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">network_config:
  - type: ovs_bridge
    name: br-ctlplane
    routes:
    - ip_netmask: 0.0.0.0/0
      next_hop: 192.168.1.1
    - ip_netmask: 172.31.0.0/24  # &lt;- new ctlplane subnet
      next_hop: 192.168.1.100    # &lt;- adopted node ctlplane IP address</code></pre>
</div>
</div>
<div class="paragraph">
<p>Do the same for other networks that will need to use different subnets for the
new and old parts of the deployment.</p>
</div>
<div class="paragraph">
<p>Once done, run <code>tripleo deploy</code> to apply the new configuration.</p>
</div>
<div class="paragraph">
<p>Note that network configuration changes are not applied by default to avoid
risk of network disruption. You will have to enforce the changes by setting the
<code>StandaloneNetworkConfigUpdate: true</code> in the director configuration files.</p>
</div>
<div class="paragraph">
<p>Once <code>tripleo deploy</code> is complete, you should see new link local routes to the
new subnet on each node. For example,</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># ip route | grep 172
172.31.0.0/24 via 192.168.122.100 dev br-ctlplane</code></pre>
</div>
</div>
<hr/>
<div class="paragraph">
<p>The next step is to configure similar routes for the old subnet for control plane services attached to the networks. This is done by adding <code>routes</code> entries to
<code>NodeNetworkConfigurationPolicy</code> CRs for each network. For example,</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">      - destination: 192.168.122.0/24
        next-hop-interface: ospbr</code></pre>
</div>
</div>
<div class="paragraph">
<p>Once applied, you should eventually see the following route added to your Red Hat OpenShift Container Platform (RHOCP) nodes.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># ip route | grep 192
192.168.122.0/24 dev ospbr proto static scope link</code></pre>
</div>
</div>
<hr/>
<div class="paragraph">
<p>At this point, you should be able to ping the adopted nodes from RHOCP nodes
using their old subnet addresses; and vice versa.</p>
</div>
<hr/>
<div class="paragraph">
<p>Finally, during the data plane adoption, you will have to take care of several aspects:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>in network_config, add link local routes to the new subnets, for example:</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">  nodeTemplate:
    ansible:
      ansibleUser: root
      ansibleVars:
        additional_ctlplane_host_routes:
        - ip_netmask: 172.31.0.0/24
          next_hop: '{{ ctlplane_ip }}'
        edpm_network_config_template: |
          network_config:
          - type: ovs_bridge
            routes: {{ ctlplane_host_routes + additional_ctlplane_host_routes }}
            ...</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>list the old IP addresses as <code>ansibleHost</code> and <code>fixedIP</code>, for example:</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">  nodes:
    standalone:
      ansible:
        ansibleHost: 192.168.122.100
        ansibleUser: ""
      hostName: standalone
      networks:
      - defaultRoute: true
        fixedIP: 192.168.122.100
        name: ctlplane
        subnetName: subnet1</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>expand SSH range for the firewall configuration to include both subnets:</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">        edpm_sshd_allowed_ranges:
        - 192.168.122.0/24
        - 172.31.0.0/24</code></pre>
</div>
</div>
<div class="paragraph">
<p>This is to allow SSH access from the new subnet to the adopted nodes as well as
the old one.</p>
</div>
<hr/>
<div class="paragraph">
<p>Since you are applying new network configuration to the nodes, consider also
setting <code>edpm_network_config_update: true</code> to enforce the changes.</p>
</div>
<hr/>
<div class="paragraph">
<p>Note that the examples above are incomplete and should be incorporated into
your general configuration.</p>
</div>
</div>
<div class="sect4">
<h5 id="reusing-existing-subnet-ranges_ipam-configuration">Scenario 2: Reusing existing subnet ranges</h5>
<div class="paragraph">
<p>This scenario is only applicable when the existing subnet ranges have enough IP
addresses for the new control plane services. On the other hand, it allows to
avoid additional routing configuration between the old and new subnets, as in <a href="#using-new-subnet-ranges_ipam-configuration">Scenario 1: Using new subnet ranges</a>.</p>
</div>
<div class="paragraph">
<p>The general idea here is to instruct the new control plane services to use the
same subnet as in the adopted environment, but define allocation pools used by
the new services in a way that would exclude IP addresses that were already
allocated to existing cluster nodes.</p>
</div>
<div class="paragraph">
<p>This scenario implies that the remaining IP addresses in the existing subnet is
enough for the new control plane services. If not,
<a href="#using-new-subnet-ranges_ipam-configuration">Scenario 1: Using new subnet ranges</a> should be used
instead. For more information, see <a href="#planning-your-ipam-configuration_configuring-network">Planning your IPAM configuration</a>.</p>
</div>
<div class="paragraph">
<p>No special routing configuration is required in this scenario; the only thing
to pay attention to is to make sure that already consumed IP addresses don&#8217;t
overlap with the new allocation pools configured for Red&#160;Hat OpenStack Platform control plane services.</p>
</div>
<div class="paragraph">
<p>If you are especially constrained by the size of the existing subnet, you may
have to apply elaborate exclusion rules when defining allocation pools for the
new control plane services. For more information, see</p>
</div>
</div>
</div>
<div class="sect3">
<h4 id="configuring-isolated-networks_ipam-configuration">Configuring isolated networks</h4>
<div class="paragraph">
<p>At this point, you should have a good idea about VLAN and IPAM configuration
you would like to replicate in the new environment.</p>
</div>
<div class="paragraph">
<p>Before proceeding, you should have a list of the following IP address
allocations to be used for the new control plane services:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>1 IP address, per isolated network, per Red Hat OpenShift Container Platform worker node. (These
addresses configure openshift worker nodes to
<code>NodeNetworkConfigurationPolicy</code> custom resources (CRs).) For more information, see <a href="#configuring-openshift-worker-nodes_isolated-networks">Configuring Red Hat OpenShift Container Platform worker nodes</a>.</p>
</li>
<li>
<p>IP range, per isolated network, for the data plane nodes. (These ranges will
configure data plane nodes to <code>NetConfig</code> CRs.) For more information, see <a href="#configuring-data-plane-nodes_isolated-networks">Configuring data plane nodes</a>.</p>
</li>
<li>
<p>IP range, per isolated network, for control plane services. (These ranges
will enable pod connectivity to isolated networks to
<code>NetworkAttachmentDefinition</code> CRs.) For more information, see <a href="#configuring-networking-for-control-plane-services_isolated-networks">Configuring the networking for control plane services</a>.</p>
</li>
<li>
<p>IP range, per isolated network, for load balancer IP addresses. (These ranges will define load balancer IP addresses to <code>IPAddressPool</code> CRs for MetalLB.) For more information, see <a href="#configuring-networking-for-control-plane-services_isolated-networks">Configuring the networking for control plane services</a>.</p>
</li>
</ul>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
Make sure you have the information listed above before proceeding with the next steps.
</td>
</tr>
</table>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The exact list and configuration of isolated networks in the examples
listed below should reflect the actual adopted environment. The number of
isolated networks may differ from the example below. IPAM scheme may differ.
Only relevant parts of the configuration are shown. Examples are incomplete and
should be incorporated into the general configuration for the new deployment,
as described in the general Red&#160;Hat OpenStack Platform documentation.
</td>
</tr>
</table>
</div>
<div class="sect4">
<h5 id="configuring-openshift-worker-nodes_isolated-networks">Configuring Red Hat OpenShift Container Platform worker nodes</h5>
<div class="paragraph">
<p>Red Hat OpenShift Container Platform worker nodes that run Red&#160;Hat OpenStack Platform services need a way to connect the service
pods to isolated networks. This requires physical network configuration on the
hypervisor.</p>
</div>
<div class="paragraph">
<p>This configuration is managed by the NMState operator, which uses the custom resources (CRs) to
define the desired network configuration for the nodes.</p>
</div>
<div class="paragraph">
<p>For each node, define a <code>NodeNetworkConfigurationPolicy</code> CR that describes the
desired network configuration. See the example below.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">apiVersion: v1
items:
- apiVersion: nmstate.io/v1
  kind: NodeNetworkConfigurationPolicy
  spec:
      interfaces:
      - description: internalapi vlan interface
        ipv4:
          address:
          - ip: 172.17.0.10
            prefix-length: 24
          dhcp: false
          enabled: true
        ipv6:
          enabled: false
        name: enp6s0.20
        state: up
        type: vlan
        vlan:
          base-iface: enp6s0
          id: 20
          reorder-headers: true
      - description: storage vlan interface
        ipv4:
          address:
          - ip: 172.18.0.10
            prefix-length: 24
          dhcp: false
          enabled: true
        ipv6:
          enabled: false
        name: enp6s0.21
        state: up
        type: vlan
        vlan:
          base-iface: enp6s0
          id: 21
          reorder-headers: true
      - description: tenant vlan interface
        ipv4:
          address:
          - ip: 172.19.0.10
            prefix-length: 24
          dhcp: false
          enabled: true
        ipv6:
          enabled: false
        name: enp6s0.22
        state: up
        type: vlan
        vlan:
          base-iface: enp6s0
          id: 22
          reorder-headers: true
    nodeSelector:
      kubernetes.io/hostname: ocp-worker-0
      node-role.kubernetes.io/worker: ""</code></pre>
</div>
</div>
</div>
<div class="sect4">
<h5 id="configuring-networking-for-control-plane-services_isolated-networks">Configuring the networking for control plane services</h5>
<div class="paragraph">
<p>Once NMState operator created the desired hypervisor network configuration for
isolated networks, we need to configure Red&#160;Hat OpenStack Platform (RHOSP) services to use configured
interfaces. This is achieved by defining <code>NetworkAttachmentDefinition</code> custom resources (CRs) for
each isolated network. (In some clusters, these CRs are managed by the Cluster
Network Operator in which case <code>Network</code> CRs should be used instead. For more information, see
<a href="https://docs.openshift.com/container-platform/4.15/networking/cluster-network-operator.html">Cluster
Network Operator</a> in <em>OpenShift Container Platform 4.15 Documentation</em>.)</p>
</div>
<div class="paragraph">
<p>For example,</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
spec:
  config: |
    {
      "cniVersion": "0.3.1",
      "name": "internalapi",
      "type": "macvlan",
      "master": "enp6s0.20",
      "ipam": {
        "type": "whereabouts",
        "range": "172.17.0.0/24",
        "range_start": "172.17.0.20",
        "range_end": "172.17.0.50"
      }
    }</code></pre>
</div>
</div>
<div class="paragraph">
<p>Make sure that the interface name and IPAM range match the configuration used
in <code>NodeNetworkConfigurationPolicy</code> CRs.</p>
</div>
<div class="paragraph">
<p>When reusing existing IP ranges, you may exclude part of the range defined by
<code>range_start</code> and <code>range_end</code> that was already consumed in the existing
deployment. Please use <code>exclude</code> as follows.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
spec:
  config: |
    {
      "cniVersion": "0.3.1",
      "name": "internalapi",
      "type": "macvlan",
      "master": "enp6s0.20",
      "ipam": {
        "type": "whereabouts",
        "range": "172.17.0.0/24",
        "range_start": "172.17.0.20",
        "range_end": "172.17.0.50",
        "exclude": [
          "172.17.0.24/32",
          "172.17.0.44/31"
        ]
      }
    }</code></pre>
</div>
</div>
<div class="paragraph">
<p>The example above would exclude addresses <code>172.17.0.24</code> as well as
<code>172.17.0.44</code> and <code>172.17.0.45</code> from the allocation pool.</p>
</div>
<div class="paragraph">
<p>Some RHOSP services require load balancer IP addresses. These IP addresses
belong to the same IP range as the control plane services, and are managed by
MetalLB. The IP address pool is defined by <code>IPAllocationPool</code> CRs. This pool
should also be aligned with the adopted configuration.</p>
</div>
<div class="paragraph">
<p>For example,</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">- apiVersion: metallb.io/v1beta1
  kind: IPAddressPool
  spec:
    addresses:
    - 172.17.0.60-172.17.0.70</code></pre>
</div>
</div>
<div class="paragraph">
<p>Define <code>IPAddressPool</code> CRs for each isolated network that requires load
balancer IP addresses.</p>
</div>
<div class="paragraph">
<p>When reusing existing IP ranges, you may exclude part of the range by listing
multiple <code>addresses</code> entries.</p>
</div>
<div class="paragraph">
<p>For example,</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">- apiVersion: metallb.io/v1beta1
  kind: IPAddressPool
  spec:
    addresses:
    - 172.17.0.60-172.17.0.64
    - 172.17.0.66-172.17.0.70</code></pre>
</div>
</div>
<div class="paragraph">
<p>The example above would exclude the <code>172.17.0.65</code> address from the allocation
pool.</p>
</div>
</div>
<div class="sect4">
<h5 id="configuring-data-plane-nodes_isolated-networks">Configuring data plane nodes</h5>
<div class="paragraph">
<p>A complete Red&#160;Hat OpenStack Platform (RHOSP) cluster consists of Red Hat OpenShift Container Platform (RHOCP) nodes and data plane nodes. The
former use <code>NodeNetworkConfigurationPolicy</code> custom resource (CR) to configure physical
interfaces. Since data plane nodes are not RHOCP nodes, a different approach to
configure their network connectivity is used.</p>
</div>
<div class="paragraph">
<p>Instead, data plane nodes are configured by <code>openstack-operator</code> and its CRs. The CRs
define desired network configuration for the nodes.</p>
</div>
<div class="paragraph">
<p>In case of adoption, the configuration should reflect the existing network
setup. You should be able to pull <code>net_config.yaml</code> files from each node and
reuse it when defining <code>OpenstackDataplaneNodeSet</code>. The format of the
configuration hasn&#8217;t changed (<code>os-net-config</code> is still being used under the
hood), so you should be able to put network templates under
<code>edpm_network_config_template</code> variables (either common for all nodes, or
per-node).</p>
</div>
<div class="paragraph">
<p>To make sure the latest network configuration is used during the data plane adoption, you
should also set <code>edpm_network_config_update: true</code> in the <code>nodeTemplate</code>.</p>
</div>
<div class="paragraph">
<p>You will proceed with the data plane adoption once the RHOCP control plane is deployed in the
RHOCP cluster. When doing so, you will configure <code>NetConfig</code> and
<code>OpenstackDataplaneNodeSet</code> CRs, using the same VLAN tags and IPAM
configuration as determined in the previous steps.</p>
</div>
<div class="paragraph">
<p>For example,</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">apiVersion: network.openstack.org/v1beta1
kind: NetConfig
metadata:
  name: netconfig
spec:
  networks:
  - name: internalapi
    dnsDomain: internalapi.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.17.0.250
        start: 172.17.0.100
      cidr: 172.17.0.0/24
      vlan: 20
  - name: storage
    dnsDomain: storage.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.18.0.250
        start: 172.18.0.100
      cidr: 172.18.0.0/24
      vlan: 21
  - name: tenant
    dnsDomain: tenant.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.19.0.250
        start: 172.19.0.100
      cidr: 172.19.0.0/24
      vlan: 22</code></pre>
</div>
</div>
<div class="paragraph">
<p>List multiple <code>allocationRanges</code> entries to exclude some of the IP addresses,
e.g. to accommodate for addresses already consumed by the adopted environment.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">apiVersion: network.openstack.org/v1beta1
kind: NetConfig
metadata:
  name: netconfig
spec:
  networks:
  - name: internalapi
    dnsDomain: internalapi.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.17.0.199
        start: 172.17.0.100
      - end: 172.17.0.250
        start: 172.17.0.201
      cidr: 172.17.0.0/24
      vlan: 20</code></pre>
</div>
</div>
<div class="paragraph">
<p>The example above would exclude the <code>172.17.0.200</code> address from the pool.</p>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="storage-requirements_isolated-networks">Storage requirements</h3>
<div class="paragraph">
<p>When looking into the storage in an Red&#160;Hat OpenStack Platform (RHOSP) deployment you can differentiate
two kinds, the storage requirements of the services themselves and the
storage used for the RHOSP users that the services will manage.</p>
</div>
<div class="paragraph">
<p>These requirements may drive your Red Hat OpenShift Container Platform (RHOCP) node selection, as mentioned above,
and may require you to do some preparations on the RHOCP nodes before
you can deploy the services.</p>
</div>
<div class="sect3">
<h4 id="storage-driver-certification_storage-requirements">Storage driver certification</h4>
<div class="paragraph">
<p>Before you adopt your Red&#160;Hat OpenStack Platform 17.1 deployment to a Red&#160;Hat OpenStack Services on OpenShift (RHOSO) 18.0 deployment, confirm that your deployed storage drivers are certified for use with RHOSO 18.0.</p>
</div>
</div>
<div class="sect3">
<h4 id="block-storage-requirements_storage-requirements">Block Storage service requirements</h4>
<div class="paragraph">
<p>The Block Storage service (cinder) has both local storage used by the service and Red&#160;Hat OpenStack Platform (RHOSP) user requirements.</p>
</div>
<div class="paragraph">
<p>Local storage is used for example when downloading a Image Service (glance) image for the create volume from image operation, which can become considerable when having
concurrent operations and not using the Block Storage service volume cache.</p>
</div>
<div class="paragraph">
<p>In the Operator deployed RHOSP, there is a way to configure the
location of the conversion directory to be an NFS share (using the extra
volumes feature), something that needed to be done manually before.</p>
</div>
<div class="paragraph">
<p>Even if it&#8217;s an adoption and it may seem that there&#8217;s nothing to consider
regarding the Block Storage service backends, because you are using the same ones that you are
using in your current deployment, you should still evaluate it, because it may not be so straightforward.</p>
</div>
<div class="paragraph">
<p>First you need to check the transport protocol the Block Storage service backends are using:
RBD, iSCSI, FC, NFS, NVMe-oF, etc.</p>
</div>
<div class="paragraph">
<p>Once you know all the transport protocols that you are using, you can make
sure that you are taking them into consideration when placing the Block Storage services (as mentioned above in the Node Roles section) and the right storage transport related binaries are running on the Red Hat OpenShift Container Platform nodes.</p>
</div>
<div class="paragraph">
<p>Detailed information about the specifics for each storage transport protocol can be found in the <a href="#openshift-preparation-for-block-storage-adoption_adopting-block-storage">Red Hat OpenShift Container Platform preparation for Block Storage service adoption</a>.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="comparing-configuration-files-between-deployments_storage-requirements">Comparing configuration files between deployments</h3>
<div class="paragraph">
<p>In order to help users to handle the configuration for the director and Red&#160;Hat OpenStack Platform
services the tool: <a href="https://github.com/openstack-k8s-operators/os-diff" class="bare">https://github.com/openstack-k8s-operators/os-diff</a> has been
develop to compare the configuration files between the director deployment and the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) cloud.
Make sure Golang is installed and configured on your environment:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>dnf install -y golang-github-openstack-k8s-operators-os-diff</pre>
</div>
</div>
<div class="paragraph">
<p>Then configure the <code>/etc/os-diff/os-diff.cfg</code> file and the <code>/etc/os-diff/ssh.config</code> file according to your environment. To allow os-diff to connect to your clouds and pull files from the services that you describe in the <code>config.yaml</code> file you need to properly set the option in the <code>os-diff.cfg</code> file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">[Default]

local_config_dir=/tmp/
service_config_file=config.yaml

[Tripleo]

ssh_cmd=ssh -F ssh.config
director_host=standalone
container_engine=podman
connection=ssh
remote_config_path=/tmp/tripleo
local_config_path=/tmp/

[Openshift]

ocp_local_config_path=/tmp/ocp
connection=local
ssh_cmd=""</code></pre>
</div>
</div>
<div class="paragraph">
<p>Os-diff uses <code>ssh_cmd</code> to access your director host via SSH,
or the host where your cloud is accessible and the podman/docker binary is installed
and allowed to interact with the running containers. This option could have a different form:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>ssh_cmd=ssh -F ssh.config standalone
director_host=</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>ssh_cmd=ssh -F ssh.config
director_host=standalone</pre>
</div>
</div>
<div class="paragraph">
<p>or without an SSH config file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>ssh_cmd=ssh -i /home/user/.ssh/id_rsa stack@my.undercloud.local
director_host=</pre>
</div>
</div>
<div class="paragraph">
<p>or</p>
</div>
<div class="listingblock">
<div class="content">
<pre>ssh_cmd=ssh -i /home/user/.ssh/id_rsa stack@
director_host=my.undercloud.local</pre>
</div>
</div>
<div class="paragraph">
<p>Note that the result of using <code>ssh_cmd</code> and <code>director_host</code> should be a "successful ssh access".</p>
</div>
<div class="paragraph">
<p>Configure or generate the <code>ssh.config</code> file from inventory or hosts file, for example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">Host *
    IdentitiesOnly yes

Host virthost
    Hostname virthost
    IdentityFile ~/.ssh/id_rsa
    User root
    StrictHostKeyChecking no
    UserKnownHostsFile=/dev/null


Host standalone
    Hostname standalone
    IdentityFile <strong>&lt;path to SSH key&gt;</strong>
    User root
    StrictHostKeyChecking no
    UserKnownHostsFile=/dev/null

Host crc
    Hostname crc
    IdentityFile ~/.ssh/id_rsa
    User stack
    StrictHostKeyChecking no
    UserKnownHostsFile=/dev/null</code></pre>
</div>
</div>
<div class="paragraph">
<p>Os-diff can use an <code>ssh.config</code> file for getting access to your Red&#160;Hat OpenStack Platform environment.
The following command can help you generate this SSH config file from your Ansible inventory, for example, <code>tripleo-ansible-inventory.yaml</code> file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>os-diff configure -i tripleo-ansible-inventory.yaml -o ssh.config --yaml</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
You must set the <code>IdentityFile</code> key in the file to get full working access:
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre>Host standalone
  HostName standalone
  User root
  IdentityFile ~/.ssh/id_rsa
  StrictHostKeyChecking no
  UserKnownHostsFile /dev/null

Host undercloud
  HostName undercloud
  User root
  IdentityFile ~/.ssh/id_rsa
  StrictHostKeyChecking no
  UserKnownHostsFile /dev/null</pre>
</div>
</div>
<div class="paragraph">
<p>And test your connection:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>ssh -F ssh.config standalone</pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="migrating-tls-everywhere_storage-requirements">Migrating TLS-e to the RHOSO deployment</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The Red&#160;Hat OpenStack Services on OpenShift (RHOSO) deployment adopts the settings from the
Red&#160;Hat OpenStack Platform (RHOSP) 17.1  deployment. If TLS everywhere (TLS-e) is disabled in the RHOSP deployment, it is not enabled in the RHOSO deployment.</p>
</div>
<div class="paragraph">
<p>If the director deployment was deployed with TLS-e, FreeIPA (IdM) is used to issue certificates for the RHOSP services. Certmonger, a client process which is installed on all hosts, interacts with FreeIPA (IdM) to request, install, track and renew these certificates.</p>
</div>
<div class="paragraph">
<p>The RHOSO Operator-based deployment uses the cert-manager operator to issue, track, and renew the certificates.</p>
</div>
<div class="paragraph">
<p>Because the same root certificate authority (CA) is used to generate new certificates, you do not have to modify the currently used chain of trust.
<em>Disclaimer: the below steps were reproduced on a FreeIPA 4.10.1 server. The location of files and directories may slightly change on different versions.</em></p>
</div>
<div class="paragraph">
<p>These instructions explain how to extract the CA signing certificate from the FreeIPA instance that is used to provide the certificates in the source environment and import it into certmanager for use in the target environment. In this way, disruption on the Compute nodes can be minimized because a new chain of trust need not be installed.</p>
</div>
<div class="paragraph">
<p>It is expected that the old FreeIPA node is then decommissioned and no longer used to issue certificates. This might not be possible if the IPA server is used to issue certificates for non-RHOSP systems.</p>
</div>
<div class="paragraph">
<p>This procedure will also need to be modified if the signing keys are stored in an hardware security module (HSM) instead of an NSS shared database (NSSDB). In that case, if the key is retrievable, special HSM utilities might be required.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Your RHOSP deployment is using TLS-e.</p>
</li>
<li>
<p>Make sure the previous Adoption steps (if any) have been performed successfully.</p>
</li>
<li>
<p>Make sure the backend services on the new deployment are not started yet.</p>
</li>
<li>
<p>Define the following shell variables. The values that are used are examples and refer to a single node standalone director deployment. Replace these example values with values that are correct for your environment:</p>
<div class="listingblock">
<div class="content">
<pre>IPA_SSH="ssh -i &lt;path_to_ssh_key&gt; root@&lt;freeipa-server-ip-address&gt;"</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>To locate the CA certificate and key, list all the certificates inside your NSSDB:</p>
<div class="listingblock">
<div class="content">
<pre>$IPA_SSH certutil -L -d /etc/pki/pki-tomcat/alias</pre>
</div>
</div>
<div class="paragraph">
<p>The <code>-L</code> option lists all certificates, and <code>-d</code> specifies where they are stored. This will produce some output like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>Certificate Nickname                                         Trust Attributes
                                                             SSL,S/MIME,JAR/XPI

caSigningCert cert-pki-ca                                    CTu,Cu,Cu
ocspSigningCert cert-pki-ca                                  u,u,u
Server-Cert cert-pki-ca                                      u,u,u
subsystemCert cert-pki-ca                                    u,u,u
auditSigningCert cert-pki-ca                                 u,u,Pu</pre>
</div>
</div>
<div class="paragraph">
<p>The item you need to consider is the first one: <code>caSigningCert cert-pki-ca</code>.</p>
</div>
</li>
<li>
<p>Export the certificate and key from the <code>/etc/pki/pki-tomcat/alias</code> directory:</p>
<div class="listingblock">
<div class="content">
<pre>$IPA_SSH pk12util -o /tmp/freeipa.p12 -n 'caSigningCert\ cert-pki-ca' -d /etc/pki/pki-tomcat/alias -k /etc/pki/pki-tomcat/alias/pwdfile.txt -w /etc/pki/pki-tomcat/alias/pwdfile.txt</pre>
</div>
</div>
<div class="paragraph">
<p>The command generates a P12 file with both the certificate and the key. The <code>/etc/pki/pki-tomcat/alias/pwdfile.txt</code> file contains the password that protects the key. You can use it to both extract the key and generate the new file, <code>/tmp/freeipa.p12</code>. You can also choose another password. If you choose to apply a different password for the new file, replace the parameter of the <code>-w</code> option, or use the <code>-W</code> (capital W) option followed by the password (in clear text).</p>
</div>
<div class="paragraph">
<p>With that file, you can also separately get the certificate and the key by using the <code>openssl pkcs12</code> command.</p>
</div>
</li>
<li>
<p>Create the secret that contains the root CA:</p>
<div class="listingblock">
<div class="content">
<pre>oc create secret generic rootca-internal -n openstack</pre>
</div>
</div>
</li>
<li>
<p>Import the certificate and the key from FreeIPA:</p>
<div class="listingblock">
<div class="content">
<pre>oc patch secret rootca-internal -n openstack -p="{\"data\":{\"ca.crt\": \"`$IPA_SSH openssl pkcs12 -in /tmp/freeipa.p12 -passin file:/etc/pki/pki-tomcat/alias/pwdfile.txt -nokeys | openssl x509 | base64 -w 0`\"}}"

oc patch secret rootca-internal -n openstack -p="{\"data\":{\"tls.crt\": \"`$IPA_SSH openssl pkcs12 -in /tmp/freeipa.p12 -passin file:/etc/pki/pki-tomcat/alias/pwdfile.txt -nokeys | openssl x509 | base64 -w 0`\"}}"

oc patch secret rootca-internal -n openstack -p="{\"data\":{\"tls.key\": \"`$IPA_SSH openssl pkcs12 -in /tmp/freeipa.p12 -passin file:/etc/pki/pki-tomcat/alias/pwdfile.txt -nocerts -noenc | openssl rsa | base64 -w 0`\"}}"</pre>
</div>
</div>
</li>
<li>
<p>Create the cert-manager Issuer and reference the created secret:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">oc apply -f - &lt;&lt;EOF
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: rootca-internal
  namespace: openstack
  labels:
    osp-rootca-issuer-public: ""
    osp-rootca-issuer-internal: ""
    osp-rootca-issuer-libvirt: ""
    osp-rootca-issuer-ovn: ""
spec:
  ca:
    secretName: rootca-internal
EOF</code></pre>
</div>
</div>
</li>
<li>
<p>Delete the previously created p12 files:</p>
<div class="listingblock">
<div class="content">
<pre>$IPA_SSH rm /tmp/freeipa.p12</pre>
</div>
</div>
</li>
<li>
<p>Verify that the necessary resources were created by using the following commands:</p>
<div class="listingblock">
<div class="content">
<pre>oc get issuers -n openstack</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>oc get secret rootca-internal -n openstack -o yaml</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
After the adoption procedure is finished, the cert-manager operator is responsible for issuing and refreshing new certificates when they expire.
However, only the usual certificate rotation procedure is applied. This means that services and workloads that are not restarted and don&#8217;t support reloading of the certificates are still using the certificates originally issued by IPA and generated by the certmonger daemon. In particular, as of writing this procedure, TLS certs for VNC consoles of the libvirt managed qemu domains are not rotated. You may need to restart or migrate the domains before the certificates expire.
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="migrating-databases-to-the-control-plane_storage-requirements">Migrating databases to the control plane</h2>
<div class="sectionbody">
<div class="paragraph">
<p>To begin creating the control plane, enable back-end services and import the databases from your original Red&#160;Hat OpenStack Platform 17.1 deployment.</p>
</div>
<div class="sect2">
<h3 id="proc_retrieving-topology-specific-service-configuration_migrating-databases">Retrieving topology-specific service configuration</h3>
<div class="paragraph">
<p>Before you migrate your databases to the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) control plane, retrieve the topology-specific service configuration from your Red&#160;Hat OpenStack Platform (RHOSP) environment. You need this configuration for the following reasons:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>To check your current database for inaccuracies</p>
</li>
<li>
<p>To ensure that you have the data you need before the migration</p>
</li>
<li>
<p>To compare your RHOSP database with the adopted RHOSO database</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Define the following shell variables. Replace the example values with values that are correct for your environment:</p>
<div class="listingblock">
<div class="content">
<pre>CONTROLLER1_SSH="ssh -i *&lt;path to SSH key&gt;* root@*&lt;node IP&gt;*"
MARIADB_IMAGE=registry.redhat.io/rhosp-dev-preview/openstack-mariadb-rhel9:18.0
SOURCE_MARIADB_IP=172.17.0.2
SOURCE_DB_ROOT_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' MysqlRootPassword:' | awk -F ': ' '{ print $2; }')
MARIADB_CLIENT_ANNOTATIONS='--annotations=k8s.v1.cni.cncf.io/networks=internalapi'</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Export the shell variables for the following outputs and test the connection to the RHOSP database:</p>
<div class="listingblock">
<div class="content">
<pre>export PULL_OPENSTACK_CONFIGURATION_DATABASES=$(oc run mariadb-client ${MARIADB_CLIENT_ANNOTATIONS} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never -- \
    mysql -rsh "$SOURCE_MARIADB_IP" -uroot -p"$SOURCE_DB_ROOT_PASSWORD" -e 'SHOW databases;')
echo "$PULL_OPENSTACK_CONFIGURATION_DATABASES"</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The <code>nova</code>, <code>nova_api</code>, and <code>nova_cell0</code> databases are included in the same database host.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Run <code>mysqlcheck</code> on the RHOSP database to check for inaccuracies:</p>
<div class="listingblock">
<div class="content">
<pre>export PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK=$(oc run mariadb-client ${MARIADB_CLIENT_ANNOTATIONS} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never -- \
    mysqlcheck --all-databases -h $SOURCE_MARIADB_IP -u root -p"$SOURCE_DB_ROOT_PASSWORD" | grep -v OK)
echo "$PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK"</pre>
</div>
</div>
</li>
<li>
<p>Get the Compute service (nova) cell mappings:</p>
<div class="listingblock">
<div class="content">
<pre>export PULL_OPENSTACK_CONFIGURATION_NOVADB_MAPPED_CELLS=$(oc run mariadb-client ${MARIADB_CLIENT_ANNOTATIONS} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never -- \
    mysql -rsh "${SOURCE_MARIADB_IP}" -uroot -p"${SOURCE_DB_ROOT_PASSWORD}" nova_api -e \
    'select uuid,name,transport_url,database_connection,disabled from cell_mappings;')
echo "$PULL_OPENSTACK_CONFIGURATION_NOVADB_MAPPED_CELLS"</pre>
</div>
</div>
</li>
<li>
<p>Get the hostnames of the registered Compute services:</p>
<div class="listingblock">
<div class="content">
<pre>export PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES=$(oc run mariadb-client ${MARIADB_CLIENT_ANNOTATIONS} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never -- \
    mysql -rsh "$SOURCE_MARIADB_IP" -uroot -p"$SOURCE_DB_ROOT_PASSWORD" nova_api -e \
    "select host from nova.services where services.binary='nova-compute';")
echo "$PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES"</pre>
</div>
</div>
</li>
<li>
<p>Get the list of the mapped Compute service cells:</p>
<div class="listingblock">
<div class="content">
<pre>export PULL_OPENSTACK_CONFIGURATION_NOVAMANAGE_CELL_MAPPINGS=$($CONTROLLER1_SSH sudo podman exec -it nova_api nova-manage cell_v2 list_cells)
echo "$PULL_OPENSTACK_CONFIGURATION_NOVAMANAGE_CELL_MAPPINGS"</pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
After the RHOSP control plane services are shut down, if any of the exported values are lost, re-running the command fails because the control plane services are no longer running on the source cloud, and the data cannot be retrieved. To avoid data loss, preserve the exported values in an environment file before shutting down the control plane services.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Store the exported variables for future use:</p>
<div class="listingblock">
<div class="content">
<pre>$ cat &gt; ~/.source_cloud_exported_variables &lt;&lt; EOF
PULL_OPENSTACK_CONFIGURATION_DATABASES="$(oc run mariadb-client ${MARIADB_CLIENT_ANNOTATIONS} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never -- \
    mysql -rsh $SOURCE_MARIADB_IP -uroot -p$SOURCE_DB_ROOT_PASSWORD -e 'SHOW databases;')"
PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK="$(oc run mariadb-client ${MARIADB_CLIENT_ANNOTATIONS} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never -- \
    mysqlcheck --all-databases -h $SOURCE_MARIADB_IP -u root -p$SOURCE_DB_ROOT_PASSWORD | grep -v OK)"
PULL_OPENSTACK_CONFIGURATION_NOVADB_MAPPED_CELLS="$(oc run mariadb-client ${MARIADB_CLIENT_ANNOTATIONS} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never -- \
    mysql -rsh $SOURCE_MARIADB_IP -uroot -p$SOURCE_DB_ROOT_PASSWORD nova_api -e \
    'select uuid,name,transport_url,database_connection,disabled from cell_mappings;')"
PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES="$(oc run mariadb-client ${MARIADB_CLIENT_ANNOTATIONS} -q --image ${MARIADB_IMAGE} -i --rm --restart=Never -- \
    mysql -rsh $SOURCE_MARIADB_IP -uroot -p$SOURCE_DB_ROOT_PASSWORD nova_api -e \
    "select host from nova.services where services.binary='nova-compute';")"
PULL_OPENSTACK_CONFIGURATION_NOVAMANAGE_CELL_MAPPINGS="$($CONTROLLER1_SSH sudo podman exec -it nova_api nova-manage cell_v2 list_cells)"
EOF
chmod 0600 ~/.source_cloud_exported_variables</pre>
</div>
</div>
</li>
<li>
<p>If <code>neutron-sriov-nic-agent</code> agents are running in your RHOSP deployment, get the configuration to use for the data plane adoption:</p>
<div class="listingblock">
<div class="content">
<pre>$ podman run -i --rm --userns=keep-id -u $UID $MARIADB_IMAGE mysql \
    -rsh "$SOURCE_MARIADB_IP" -uroot -p"$SOURCE_DB_ROOT_PASSWORD" ovs_neutron -e \
    "select host, configurations from agents where agents.binary='neutron-sriov-nic-agent';"</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="deploying-backend-services_migrating-databases">Deploying back-end services</h3>
<div class="paragraph">
<p>Create the <code>OpenStackControlPlane</code> custom resource (CR) with the basic back-end services deployed, and disable all the Red&#160;Hat OpenStack Platform (RHOSP) services. This CR is the foundation of the control plane.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>The cloud that you want to adopt is running, and it is on the RHOSP 17.1 release.</p>
</li>
<li>
<p>All control plane and data plane hosts of the source cloud are running, and continue to run throughout the adoption procedure.</p>
</li>
<li>
<p>The <code>openstack-operator</code> is deployed, but <code>OpenStackControlPlane</code> is
not deployed.</p>
</li>
<li>
<p>Install the OpenStack Operators. For more information, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html-single/deploying_red_hat_openstack_services_on_openshift/index#assembly_installing-and-preparing-the-Operators">Installing and preparing the Operators</a> in <em>Deploying Red Hat OpenStack Services on OpenShift</em>.</p>
</li>
<li>
<p>If you enabled TLS everywhere (TLS-e) on the RHOSP environment, you must copy the <code>tls</code> root CA from the RHOSP environment to the <code>rootca-internal</code> issuer.</p>
</li>
<li>
<p>There are free PVs available for MariaDB and RabbitMQ.</p>
</li>
<li>
<p>Set the desired admin password for the control plane deployment. This can
be the admin password from your original deployment or a different password:</p>
<div class="listingblock">
<div class="content">
<pre>ADMIN_PASSWORD=SomePassword</pre>
</div>
</div>
<div class="paragraph">
<p>To use the existing RHOSP deployment password:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>ADMIN_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' AdminPassword:' | awk -F ': ' '{ print $2; }')</pre>
</div>
</div>
</li>
<li>
<p>Set the service password variables to match the original deployment.
Database passwords can differ in the control plane environment, but
you must synchronize the service account passwords.</p>
<div class="paragraph">
<p>For example, in developer environments with director Standalone, the passwords can be extracted:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>AODH_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' AodhPassword:' | awk -F ': ' '{ print $2; }')
BARBICAN_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' BarbicanPassword:' | awk -F ': ' '{ print $2; }')
CEILOMETER_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' CeilometerPassword:' | awk -F ': ' '{ print $2; }')
CINDER_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' CinderPassword:' | awk -F ': ' '{ print $2; }')
GLANCE_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' GlancePassword:' | awk -F ': ' '{ print $2; }')
HEAT_AUTH_ENCRYPTION_KEY=$(cat ~/tripleo-standalone-passwords.yaml | grep ' HeatAuthEncryptionKey:' | awk -F ': ' '{ print $2; }')
HEAT_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' HeatPassword:' | awk -F ': ' '{ print $2; }')
IRONIC_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' IronicPassword:' | awk -F ': ' '{ print $2; }')
MANILA_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' ManilaPassword:' | awk -F ': ' '{ print $2; }')
NEUTRON_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' NeutronPassword:' | awk -F ': ' '{ print $2; }')
NOVA_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' NovaPassword:' | awk -F ': ' '{ print $2; }')
OCTAVIA_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' OctaviaPassword:' | awk -F ': ' '{ print $2; }')
PLACEMENT_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' PlacementPassword:' | awk -F ': ' '{ print $2; }')
SWIFT_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' SwiftPassword:' | awk -F ': ' '{ print $2; }')</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Ensure that you are using the Red Hat OpenShift Container Platform namespace where you want the
control plane to be deployed:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc project openstack</pre>
</div>
</div>
</li>
<li>
<p>Create the RHOSP secret. For more information, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html-single/deploying_red_hat_openstack_services_on_openshift/index#proc_providing-secure-access-to-the-RHOSO-services_preparing">Providing secure access to the Red Hat OpenStack Services on OpenShift services</a> in <em>Deploying Red Hat OpenStack Services on OpenShift</em>.</p>
</li>
<li>
<p>If the <code>$ADMIN_PASSWORD</code> is different than the password you set
in <code>osp-secret</code>, amend the <code>AdminPassword</code> key in the <code>osp-secret</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc set data secret/osp-secret "AdminPassword=$ADMIN_PASSWORD"</pre>
</div>
</div>
</li>
<li>
<p>Set service account passwords in <code>osp-secret</code> to match the service
account passwords from the original deployment:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc set data secret/osp-secret "AodhPassword=$AODH_PASSWORD"
$ oc set data secret/osp-secret "BarbicanPassword=$BARBICAN_PASSWORD"
$ oc set data secret/osp-secret "CeilometerPassword=$CEILOMETER_PASSWORD"
$ oc set data secret/osp-secret "CinderPassword=$CINDER_PASSWORD"
$ oc set data secret/osp-secret "GlancePassword=$GLANCE_PASSWORD"
$ oc set data secret/osp-secret "HeatAuthEncryptionKey=$HEAT_AUTH_ENCRYPTION_KEY"
$ oc set data secret/osp-secret "HeatPassword=$HEAT_PASSWORD"
$ oc set data secret/osp-secret "IronicPassword=$IRONIC_PASSWORD"
$ oc set data secret/osp-secret "IronicInspectorPassword=$IRONIC_PASSWORD"
$ oc set data secret/osp-secret "ManilaPassword=$MANILA_PASSWORD"
$ oc set data secret/osp-secret "NeutronPassword=$NEUTRON_PASSWORD"
$ oc set data secret/osp-secret "NovaPassword=$NOVA_PASSWORD"
$ oc set data secret/osp-secret "OctaviaPassword=$OCTAVIA_PASSWORD"
$ oc set data secret/osp-secret "PlacementPassword=$PLACEMENT_PASSWORD"
$ oc set data secret/osp-secret "SwiftPassword=$SWIFT_PASSWORD"</pre>
</div>
</div>
</li>
<li>
<p>If you enabled TLS-e in your RHOSP environment, in the <code>spec:tls</code> section, set the <code>enabled</code> parameter to <code>true</code>:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: core.openstack.org/v1beta1
kind: OpenStackControlPlane
metadata:
  name: openstack
spec:
  tls:
    podLevel:
      enabled: true
      internal:
        ca:
          customIssuer: rootca-internal
      libvirt:
        ca:
          customIssuer: rootca-internal
      ovn:
        ca:
          customIssuer: rootca-internal
    ingress:
      ca:
        customIssuer: rootca-internal
      enabled: true</code></pre>
</div>
</div>
</li>
<li>
<p>If you did not enable TLS-e, in the <code>spec:tls`</code> section, set the <code>enabled</code> parameter to <code>false</code>:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: core.openstack.org/v1beta1
kind: OpenStackControlPlane
metadata:
  name: openstack
spec:
  tls:
    podLevel:
      enabled: false
    ingress:
      enabled: false</code></pre>
</div>
</div>
</li>
<li>
<p>Deploy the <code>OpenStackControlPlane</code> CR. Ensure that you only enable the DNS, MariaDB, Memcached, and RabbitMQ services. All other services must
be disabled:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">oc apply -f - &lt;&lt;EOF
apiVersion: core.openstack.org/v1beta1
kind: OpenStackControlPlane
metadata:
  name: openstack
spec:
  secret: osp-secret
  storageClass: local-storage

  barbican:
    enabled: false
    template:
      barbicanAPI: {}
      barbicanWorker: {}
      barbicanKeystoneListener: {}

  cinder:
    enabled: false
    template:
      cinderAPI: {}
      cinderScheduler: {}
      cinderBackup: {}
      cinderVolumes: {}

  dns:
    template:
      override:
        service:
          metadata:
            annotations:
              metallb.universe.tf/address-pool: ctlplane
              metallb.universe.tf/allow-shared-ip: ctlplane
              metallb.universe.tf/loadBalancerIPs: 192.168.122.80
          spec:
            type: LoadBalancer
      options:
      - key: server
        values:
        - 192.168.122.1
      replicas: 1

  glance:
    enabled: false
    template:
      glanceAPIs: {}

  heat:
    enabled: false
    template: {}

  horizon:
    enabled: false
    template: {}

  ironic:
    enabled: false
    template:
      ironicConductors: []

  keystone:
    enabled: false
    template: {}

  manila:
    enabled: false
    template:
      manilaAPI: {}
      manilaScheduler: {}
      manilaShares: {}

  mariadb:
    enabled: false
    templates: {}

  galera:
    enabled: true
    templates:
      openstack:
        secret: osp-secret
        replicas: 3
        storageRequest: 500M
      openstack-cell1:
        secret: osp-secret
        replicas: 3
        storageRequest: 500M

  memcached:
    enabled: true
    templates:
      memcached:
        replicas: 3

  neutron:
    enabled: false
    template: {}

  nova:
    enabled: false
    template: {}

  ovn:
    enabled: false
    template:
      ovnController:
        networkAttachment: tenant
        nodeSelector:
          node: non-existing-node-name
      ovnNorthd:
        replicas: 0
      ovnDBCluster:
        ovndbcluster-nb:
          dbType: NB
          networkAttachment: internalapi
        ovndbcluster-sb:
          dbType: SB
          networkAttachment: internalapi

  placement:
    enabled: false
    template: {}

  rabbitmq:
    templates:
      rabbitmq:
        override:
          service:
            metadata:
              annotations:
                metallb.universe.tf/address-pool: internalapi
                metallb.universe.tf/loadBalancerIPs: 172.17.0.85
            spec:
              type: LoadBalancer
      rabbitmq-cell1:
        override:
          service:
            metadata:
              annotations:
                metallb.universe.tf/address-pool: internalapi
                metallb.universe.tf/loadBalancerIPs: 172.17.0.86
            spec:
              type: LoadBalancer

  telemetry:
    enabled: false

  swift:
    enabled: false
    template:
      swiftRing:
        ringReplicas: 1
      swiftStorage:
        replicas: 0
      swiftProxy:
        replicas: 1
EOF</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Verification</div>
<ul>
<li>
<p>Verify that MariaDB is running:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get pod openstack-galera-0 -o jsonpath='{.status.phase}{"\n"}'
$ oc get pod openstack-cell1-galera-0 -o jsonpath='{.status.phase}{"\n"}'</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="configuring-a-ceph-backend_migrating-databases">Configuring a Ceph backend</h3>
<div class="paragraph">
<p>If the original deployment uses a Ceph storage backend for any service
(e.g. Image Service (glance), Block Storage service (cinder), Compute service (nova), Shared File Systems service (manila)), the same backend must be used in the
adopted deployment and custom resources (CRs) must be configured accordingly.</p>
</div>
<div class="paragraph">
<p>If you use Shared File Systems service (manila), on director environments, the CephFS driver in Shared File Systems service is configured to use
its own keypair. For convenience, modify the <code>openstack</code> user so that you
can use it across all Red&#160;Hat OpenStack Platform services.</p>
</div>
<div class="paragraph">
<p>Using the same user across the services serves two purposes:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The capabilities of the user required to interact with Shared File Systems service
became far simpler and hence, more became more secure with RHOSO 18.0.</p>
</li>
<li>
<p>It is simpler to create a common ceph secret (keyring and ceph config
file) and propagate the secret to all services that need it.</p>
</li>
</ul>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
To run <code>ceph</code> commands, you must use SSH to connect to a Ceph
storage node and run <code>sudo cephadm shell</code>. This brings up a ceph orchestrator
container that allows you to run administrative commands against the ceph
cluster. If you deployed the ceph cluster by using director, you may launch the <code>cephadm</code> shell from an RHOSP controller node.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre>ceph auth caps client.openstack \
  mgr 'allow *' \
  mon 'allow r, profile rbd' \
  osd 'profile rbd pool=vms, profile rbd pool=volumes, profile rbd pool=images, allow rw pool manila_data'</pre>
</div>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>The <code>OpenStackControlPlane</code> custom resource (CR) must already exist.</p>
</li>
<li>
<p>Define the following shell variables. The values that are used are examples. Replace these example values with values that are correct for your environment:</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre>CEPH_SSH="ssh -i <strong>&lt;path to SSH key&gt;</strong> root@<strong>&lt;node IP&gt;</strong>"
CEPH_KEY=$($CEPH_SSH "cat /etc/ceph/ceph.client.openstack.keyring | base64 -w 0")
CEPH_CONF=$($CEPH_SSH "cat /etc/ceph/ceph.conf | base64 -w 0")</pre>
</div>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create the <code>ceph-conf-files</code> secret, containing Ceph configuration:</p>
<div class="listingblock">
<div class="content">
<pre>oc apply -f - &lt;&lt;EOF
apiVersion: v1
data:
  ceph.client.openstack.keyring: $CEPH_KEY
  ceph.conf: $CEPH_CONF
kind: Secret
metadata:
  name: ceph-conf-files
  namespace: openstack
type: Opaque
EOF</pre>
</div>
</div>
<div class="paragraph">
<p>The content of the file should look something like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: v1
kind: Secret
metadata:
  name: ceph-conf-files
  namespace: openstack
stringData:
  ceph.client.openstack.keyring: |
    [client.openstack]
        key = &lt;secret key&gt;
        caps mgr = "allow *"
        caps mon = "allow r, profile rbd"
        caps osd = "pool=vms, profile rbd pool=volumes, profile rbd pool=images, allow rw pool manila_data'
  ceph.conf: |
    [global]
    fsid = 7a1719e8-9c59-49e2-ae2b-d7eb08c695d4
    mon_host = 10.1.1.2,10.1.1.3,10.1.1.4</code></pre>
</div>
</div>
</li>
<li>
<p>Configure <code>extraMounts</code> within the <code>OpenStackControlPlane</code> CR:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  extraMounts:
    - name: v1
      region: r1
      extraVol:
        - propagation:
          - CinderVolume
          - CinderBackup
          - GlanceAPI
          - ManilaShare
          extraVolType: Ceph
          volumes:
          - name: ceph
            projected:
              sources:
              - secret:
                  name: ceph-conf-files
          mounts:
          - name: ceph
            mountPath: "/etc/ceph"
            readOnly: true
'</code></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="creating-a-ceph-nfs-cluster_migrating-databases">Creating a NFS Ganesha cluster</h3>
<div class="paragraph">
<p>If you use the Ceph via NFS backend with Shared File Systems service (manila), prior to adoption, you must create a new clustered NFS service on the Ceph cluster. This service will replace the standalone, pacemaker-controlled <code>ceph-nfs</code> service that was used on Red&#160;Hat OpenStack Platform 17.1.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>You must identify the ceph nodes to deploy the new clustered NFS service.This service must be deployed on the <code>StorageNFS</code> isolated network so that
it is easier for clients to mount their existing shares through the new NFS
export locations.</p>
</li>
<li>
<p>You must propagate the <code>StorageNFS</code> network to the target nodes
where the <code>ceph-nfs</code> service will be deployed.
The following steps will be relevant if the Ceph Storage nodes were
deployed via director.</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Identify the node definition file used in the environment. This is
the input file associated with the <code>openstack overcloud node provision</code>
command. For example, this file may be called <code>overcloud-baremetal-deploy.yaml</code></p>
</li>
<li>
<p>Edit the networks associated with the Red Hat Ceph Storage nodes to include the
<code>StorageNFS</code> network:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">- name: CephStorage
  count: 3
  hostname_format: cephstorage-%index%
  instances:
  - hostname: cephstorage-0
    name: ceph-0
  - hostname: cephstorage-1
    name: ceph-1
  - hostname: cephstorage-2
    name: ceph-2
  defaults:
    profile: ceph-storage
    network_config:
      template: /home/stack/network/nic-configs/ceph-storage.j2
      network_config_update: true
    networks:
    - network: ctlplane
      vif: true
    - network: storage
    - network: storage_mgmt
    - network: storage_nfs</code></pre>
</div>
</div>
</li>
<li>
<p>Edit the network configuration template file for the Red Hat Ceph Storage nodes
to include an interface connecting to the <code>StorageNFS</code> network. In the
example above, the path to the network configuration template file is
<code>/home/stack/network/nic-configs/ceph-storage.j2</code>. This file is modified
to include the following NIC template:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">- type: vlan
  device: nic2
  vlan_id: {{ storage_nfs_vlan_id }}
  addresses:
  - ip_netmask: {{ storage_nfs_ip }}/{{ storage_nfs_cidr }}
  routes: {{ storage_nfs_host_routes }}</code></pre>
</div>
</div>
</li>
<li>
<p>Re-run the <code>openstack overcloud node provision</code> command to update the
Red Hat Ceph Storage nodes.</p>
<div class="listingblock">
<div class="content">
<pre>openstack overcloud node provision \
    --stack overcloud   \
    --network-config -y  \
    -o overcloud-baremetal-deployed-storage_nfs.yaml \
    --concurrency 2 \
    /home/stack/network/baremetal_deployment.yaml</pre>
</div>
</div>
<div class="paragraph">
<p>When the update is complete, ensure that the Red Hat Ceph Storage nodes have a
new interface created and tagged with the appropriate VLAN associated with
<code>StorageNFS</code>.</p>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Identify an IP address from the <code>StorageNFS</code> network to use as the Virtual IP
address for the Ceph NFS service. This IP address must be provided in place of
the <code>{{ VIP }}</code> in the example below. You can query used IP addresses with:</p>
<div class="listingblock">
<div class="content">
<pre>openstack port list -c "Fixed IP Addresses" --network storage_nfs</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Pick an appropriate size for the NFS cluster. The NFS service provides
active/active high availability when the cluster size is more than
one node. It is recommended that the ``{{ cluster_size }}`` is at least one
less than the number of hosts identified. This solution has been well tested
with a 3-node NFS cluster.</p>
</li>
<li>
<p>The <code>ingress-mode</code> argument must be set to ``haproxy-protocol``. No other
ingress-mode will be supported. This ingress mode will allow enforcing client
restrictions through Shared File Systems service.
For more information on deploying the clustered Ceph NFS service, see the
<a href="https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/7/html-single/operations_guide/index#management-of-nfs-ganesha-gateway-using-the-ceph-orchestrator">Management of NFS-Ganesha gateway using the Ceph Orchestrator (Limited Availability)</a> in <em>Red Hat Ceph Storage 7 Operations Guide</em>.</p>
</li>
<li>
<p>The following commands are run inside a <code>cephadm shell</code> to create a clustered
Ceph NFS service.</p>
<div class="listingblock">
<div class="content">
<pre># wait for shell to come up, then execute:
ceph orch host ls

# Identify the hosts that can host the NFS service.
# Repeat the following command to label each host identified:
ceph orch host label add &lt;HOST&gt; nfs

# Set the appropriate {{ cluster_size }} and {{ VIP }}:
ceph nfs cluster create cephfs \
    "{{ cluster_size }} label:nfs" \
    --ingress \
    --virtual-ip={{ VIP }}
    --ingress-mode=haproxy-protocol
}}

# Check the status of the nfs cluster with these commands
ceph nfs cluster ls
ceph nfs cluster info cephfs</pre>
</div>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="stopping-openstack-services_migrating-databases">Stopping Red&#160;Hat OpenStack Platform services</h3>
<div class="paragraph">
<p>Before you start the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) adoption, you must stop the Red&#160;Hat OpenStack Platform (RHOSP) services to avoid inconsistencies in the data that you migrate for the data plane adoption. Inconsistencies are caused by resource changes after the database is copied to the new deployment.</p>
</div>
<div class="paragraph">
<p>You should not stop the infrastructure management services yet, such as:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Database</p>
</li>
<li>
<p>RabbitMQ</p>
</li>
<li>
<p>HAProxy Load Balancer</p>
</li>
<li>
<p>Ceph-nfs</p>
</li>
<li>
<p>Compute service</p>
</li>
<li>
<p>Containerized modular libvirt daemons</p>
</li>
<li>
<p>Object Storage service (swift) back-end services</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Ensure that there no long-running tasks that require the services that you plan to stop, such as instance live migrations, volume migrations, volume creation, backup and restore, attaching, detaching, and other similar operations:</p>
<div class="listingblock">
<div class="content">
<pre>openstack server list --all-projects -c ID -c Status |grep -E '\| .+ing \|'
openstack volume list --all-projects -c ID -c Status |grep -E '\| .+ing \|'| grep -vi error
openstack volume backup list --all-projects -c ID -c Status |grep -E '\| .+ing \|' | grep -vi error
openstack share list --all-projects -c ID -c Status |grep -E '\| .+ing \|'| grep -vi error
openstack image list -c ID -c Status |grep -E '\| .+ing \|'</pre>
</div>
</div>
</li>
<li>
<p>Collect the services topology-specific configuration. For more information, see <a href="#proc_retrieving-topology-specific-service-configuration_migrating-databases">Retrieving topology-specific service configuration</a>.</p>
</li>
<li>
<p>Define the following shell variables. The values are examples and refer to a single node standalone director deployment. Replace these example values with values that are correct for your environment:</p>
<div class="listingblock">
<div class="content">
<pre>CONTROLLER1_SSH="ssh -i <strong>&lt;path to SSH key&gt;</strong> root@<strong>&lt;node IP&gt;</strong>"
CONTROLLER2_SSH=""
CONTROLLER3_SSH=""</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>If your deployment enables CephFS through NFS as a back end for Shared File Systems service (manila), remove the following Pacemaker ordering and co-location constraints that govern the Virtual IP address of the <code>ceph-nfs</code> service and the <code>manila-share</code> service:</p>
<div class="listingblock">
<div class="content">
<pre># check the co-location and ordering constraints concerning "manila-share"
sudo pcs constraint list --full

# remove these constraints
sudo pcs constraint remove colocation-openstack-manila-share-ceph-nfs-INFINITY
sudo pcs constraint remove order-ceph-nfs-openstack-manila-share-Optional</pre>
</div>
</div>
</li>
<li>
<p>Disable RHOSP control plane services:</p>
<div class="listingblock">
<div class="content">
<pre># Update the services list to be stopped
ServicesToStop=("tripleo_aodh_api.service"
                "tripleo_aodh_api_cron.service"
                "tripleo_aodh_evaluator.service"
                "tripleo_aodh_listener.service"
                "tripleo_aodh_notifier.service"
                "tripleo_ceilometer_agent_central.service"
                "tripleo_ceilometer_agent_notification.service"
                "tripleo_horizon.service"
                "tripleo_keystone.service"
                "tripleo_barbican_api.service"
                "tripleo_barbican_worker.service"
                "tripleo_barbican_keystone_listener.service"
                "tripleo_cinder_api.service"
                "tripleo_cinder_api_cron.service"
                "tripleo_cinder_scheduler.service"
                "tripleo_cinder_volume.service"
                "tripleo_cinder_backup.service"
                "tripleo_collectd.service"
                "tripleo_glance_api.service"
                "tripleo_gnocchi_api.service"
                "tripleo_gnocchi_metricd.service"
                "tripleo_gnocchi_statsd.service"
                "tripleo_manila_api.service"
                "tripleo_manila_api_cron.service"
                "tripleo_manila_scheduler.service"
                "tripleo_neutron_api.service"
                "tripleo_placement_api.service"
                "tripleo_nova_api_cron.service"
                "tripleo_nova_api.service"
                "tripleo_nova_conductor.service"
                "tripleo_nova_metadata.service"
                "tripleo_nova_scheduler.service"
                "tripleo_nova_vnc_proxy.service"
                "tripleo_aodh_api.service"
                "tripleo_aodh_api_cron.service"
                "tripleo_aodh_evaluator.service"
                "tripleo_aodh_listener.service"
                "tripleo_aodh_notifier.service"
                "tripleo_ceilometer_agent_central.service"
                "tripleo_ceilometer_agent_compute.service"
                "tripleo_ceilometer_agent_ipmi.service"
                "tripleo_ceilometer_agent_notification.service"
                "tripleo_ovn_cluster_northd.service"
                "tripleo_ironic_neutron_agent.service"
                "tripleo_ironic_api.service"
                "tripleo_ironic_inspector.service"
                "tripleo_ironic_conductor.service")

PacemakerResourcesToStop=("openstack-cinder-volume"
                          "openstack-cinder-backup"
                          "openstack-manila-share")

echo "Stopping systemd OpenStack services"
for service in ${ServicesToStop[*]}; do
    for i in {1..3}; do
        SSH_CMD=CONTROLLER${i}_SSH
        if [ ! -z "${!SSH_CMD}" ]; then
            echo "Stopping the $service in controller $i"
            if ${!SSH_CMD} sudo systemctl is-active $service; then
                ${!SSH_CMD} sudo systemctl stop $service
            fi
        fi
    done
done

echo "Checking systemd OpenStack services"
for service in ${ServicesToStop[*]}; do
    for i in {1..3}; do
        SSH_CMD=CONTROLLER${i}_SSH
        if [ ! -z "${!SSH_CMD}" ]; then
            if ! ${!SSH_CMD} systemctl show $service | grep ActiveState=inactive &gt;/dev/null; then
                echo "ERROR: Service $service still running on controller $i"
            else
                echo "OK: Service $service is not running on controller $i"
            fi
        fi
    done
done

echo "Stopping pacemaker OpenStack services"
for i in {1..3}; do
    SSH_CMD=CONTROLLER${i}_SSH
    if [ ! -z "${!SSH_CMD}" ]; then
        echo "Using controller $i to run pacemaker commands"
        for resource in ${PacemakerResourcesToStop[*]}; do
            if ${!SSH_CMD} sudo pcs resource config $resource &amp;&gt;/dev/null; then
                echo "Stopping $resource"
                ${!SSH_CMD} sudo pcs resource disable $resource
            else
                echo "Service $resource not present"
            fi
        done
        break
    fi
done

echo "Checking pacemaker OpenStack services"
for i in {1..3}; do
    SSH_CMD=CONTROLLER${i}_SSH
    if [ ! -z "${!SSH_CMD}" ]; then
        echo "Using controller $i to run pacemaker commands"
        for resource in ${PacemakerResourcesToStop[*]}; do
            if ${!SSH_CMD} sudo pcs resource config $resource &amp;&gt;/dev/null; then
                if ! ${!SSH_CMD} sudo pcs resource status $resource | grep Started; then
                    echo "OK: Service $resource is stopped"
                else
                    echo "ERROR: Service $resource is started"
                fi
            fi
        done
        break
    fi
done</pre>
</div>
</div>
<div class="paragraph">
<p>If the status of each service is <code>OK</code>, then the services stopped successfully.</p>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="migrating-databases-to-mariadb-instances_migrating-databases">Migrating databases to MariaDB instances</h3>
<div class="paragraph">
<p>Migrate your databases from the original Red&#160;Hat OpenStack Platform (RHOSP) deployment to the MariaDB instances in the Red Hat OpenShift Container Platform cluster.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Ensure that the control plane MariaDB and RabbitMQ are running, and that no other control plane services are running.</p>
</li>
<li>
<p>Retrieve the topology-specific service configuration. For more information, see <a href="#proc_retrieving-topology-specific-service-configuration_migrating-databases">Retrieving topology-specific service configuration</a>.</p>
</li>
<li>
<p>Stop the RHOSP services. For more information, see <a href="#stopping-openstack-services_migrating-databases">Stopping Red&#160;Hat OpenStack Platform services</a>.</p>
</li>
<li>
<p>Ensure that there is network routability between the original MariaDB and the MariaDB for the control plane.</p>
</li>
<li>
<p>Define the following shell variables. Replace the following example values with values that are correct for your environment:</p>
<div class="listingblock">
<div class="content">
<pre>PODIFIED_MARIADB_IP=$(oc get svc --selector "mariadb/name=openstack" -ojsonpath='{.items[0].spec.clusterIP}')
PODIFIED_CELL1_MARIADB_IP=$(oc get svc --selector "mariadb/name=openstack-cell1" -ojsonpath='{.items[0].spec.clusterIP}')
PODIFIED_DB_ROOT_PASSWORD=$(oc get -o json secret/osp-secret | jq -r .data.DbRootPassword | base64 -d)

# The CHARACTER_SET and collation should match the source DB
# if the do not then it will break foreign key relationships
# for any tables that are created in the future as part of db sync
CHARACTER_SET=utf8
COLLATION=utf8_general_ci

STORAGE_CLASS=local-storage
MARIADB_IMAGE=registry.redhat.io/rhosp-dev-preview/openstack-mariadb-rhel9:18.0
# Replace with your environment's MariaDB Galera cluster VIP and backend IPs:
SOURCE_MARIADB_IP=172.17.0.2
declare -A SOURCE_GALERA_MEMBERS
SOURCE_GALERA_MEMBERS=(
  ["standalone.localdomain"]=172.17.0.100
  # ...
)
SOURCE_DB_ROOT_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' MysqlRootPassword:' | awk -F ': ' '{ print $2; }')</pre>
</div>
</div>
</li>
<li>
<p>Prepare the MariaDB copy directory and the adoption helper pod:</p>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create a temporary folder to store the adoption helper pod. Choose storage requests that fit the MySQL database size:</p>
<div class="listingblock">
<div class="content">
<pre>$ mkdir ~/adoption-db
$ cd ~/adoption-db</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">oc apply -f - &lt;&lt;EOF
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mariadb-data
spec:
  storageClassName: $STORAGE_CLASS
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: mariadb-copy-data
  annotations:
    openshift.io/scc: anyuid
    k8s.v1.cni.cncf.io/networks: internalapi
  labels:
    app: adoption
spec:
  containers:
  - image: $MARIADB_IMAGE
    command: [ "sh", "-c", "sleep infinity"]
    name: adoption
    volumeMounts:
    - mountPath: /backup
      name: mariadb-data
  securityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop: ALL
    runAsNonRoot: true
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - name: mariadb-data
    persistentVolumeClaim:
      claimName: mariadb-data
EOF</code></pre>
</div>
</div>
</li>
<li>
<p>Wait for the pod to be ready:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait --for condition=Ready pod/mariadb-copy-data --timeout=30s</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Check that the source Galera database cluster members are online and synced:</p>
<div class="listingblock">
<div class="content">
<pre>for i in "${!SOURCE_GALERA_MEMBERS[@]}"; do
  echo "Checking for the database node $i WSREP status Synced"
  oc rsh mariadb-copy-data mysql \
    -h "${SOURCE_GALERA_MEMBERS[$i]}" -uroot -p"$SOURCE_DB_ROOT_PASSWORD" \
    -e "show global status like 'wsrep_local_state_comment'" | \
    grep -qE "\bSynced\b"
done</pre>
</div>
</div>
</li>
<li>
<p>Get the count of source databases with the <code>NOK</code> (not-OK) status:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc rsh mariadb-copy-data mysql -h "${SOURCE_MARIADB_IP}" -uroot -p"${SOURCE_DB_ROOT_PASSWORD}" -e "SHOW databases;"</pre>
</div>
</div>
</li>
<li>
<p>Check that <code>mysqlcheck</code> had no errors:</p>
<div class="listingblock">
<div class="content">
<pre>. ~/.source_cloud_exported_variables
test -z "$PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK"  || [ "$PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK" = " " ]</pre>
</div>
</div>
</li>
<li>
<p>Test the connection to the control plane databases:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc run mariadb-client --image $MARIADB_IMAGE -i --rm --restart=Never -- \
    mysql -rsh "$PODIFIED_MARIADB_IP" -uroot -p"$PODIFIED_DB_ROOT_PASSWORD" -e 'SHOW databases;'
$ oc run mariadb-client --image $MARIADB_IMAGE -i --rm --restart=Never -- \
    mysql -rsh "$PODIFIED_CELL1_MARIADB_IP" -uroot -p"$PODIFIED_DB_ROOT_PASSWORD" -e 'SHOW databases;'</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
You must transition Compute service (nova) services that are imported later into a superconductor architecture by deleting the old service records in the cell databases, starting with <code>cell1</code>. New records are registered with different hostnames provided by the Compute service operator. All Compute services, except the Compute agent, have no internal state, and their service records can be safely deleted. You also need to rename the former <code>default</code> cell to <code>cell1</code>.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Create a dump of the original databases:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc rsh mariadb-copy-data &lt;&lt; EOF
  mysql -h"${SOURCE_MARIADB_IP}" -uroot -p"${SOURCE_DB_ROOT_PASSWORD}" \
  -N -e "show databases" | grep -E -v "schema|mysql|gnocchi" | \
  while read dbname; do
    echo "Dumping \${dbname}";
    mysqldump -h"${SOURCE_MARIADB_IP}" -uroot -p"${SOURCE_DB_ROOT_PASSWORD}" \
      --single-transaction --complete-insert --skip-lock-tables --lock-tables=0 \
      "\${dbname}" &gt; /backup/"\${dbname}".sql;
   done
EOF</pre>
</div>
</div>
</li>
<li>
<p>Restore the databases from <code>.sql</code> files into the control plane MariaDB:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc rsh mariadb-copy-data &lt;&lt; EOF
  # db schemas to rename on import
  declare -A db_name_map
  db_name_map['nova']='nova_cell1'
  db_name_map['ovs_neutron']='neutron'
  db_name_map['ironic-inspector']='ironic_inspector'

  # db servers to import into
  declare -A db_server_map
  db_server_map['default']=${PODIFIED_MARIADB_IP}
  db_server_map['nova_cell1']=${PODIFIED_CELL1_MARIADB_IP}

  # db server root password map
  declare -A db_server_password_map
  db_server_password_map['default']=${PODIFIED_DB_ROOT_PASSWORD}
  db_server_password_map['nova_cell1']=${PODIFIED_DB_ROOT_PASSWORD}

  cd /backup
  for db_file in \$(ls *.sql); do
    db_name=\$(echo \${db_file} | awk -F'.' '{ print \$1; }')
    if [[ -v "db_name_map[\${db_name}]" ]]; then
      echo "renaming \${db_name} to \${db_name_map[\${db_name}]}"
      db_name=\${db_name_map[\${db_name}]}
    fi
    db_server=\${db_server_map["default"]}
    if [[ -v "db_server_map[\${db_name}]" ]]; then
      db_server=\${db_server_map[\${db_name}]}
    fi
    db_password=\${db_server_password_map['default']}
    if [[ -v "db_server_password_map[\${db_name}]" ]]; then
      db_password=\${db_server_password_map[\${db_name}]}
    fi
    echo "creating \${db_name} in \${db_server}"
    mysql -h"\${db_server}" -uroot "-p\${db_password}" -e \
      "CREATE DATABASE IF NOT EXISTS \${db_name} DEFAULT \
      CHARACTER SET ${CHARACTER_SET} DEFAULT COLLATE ${COLLATION};"
    echo "importing \${db_name} into \${db_server}"
    mysql -h "\${db_server}" -uroot "-p\${db_password}" "\${db_name}" &lt; "\${db_file}"
  done

  mysql -h "\${db_server_map['default']}" -uroot -p"\${db_server_password_map['default']}" -e \
    "update nova_api.cell_mappings set name='cell1' where name='default';"
  mysql -h "\${db_server_map['nova_cell1']}" -uroot -p"\${db_server_password_map['nova_cell1']}" -e \
    "delete from nova_cell1.services where host not like '%nova-cell1-%' and services.binary != 'nova-compute';"
EOF</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<div class="title">Verification</div>
<p>Compare the following outputs with the topology-specific service configuration.
For more information, see <a href="#proc_retrieving-topology-specific-service-configuration_migrating-databases">Retrieving topology-specific service configuration</a>.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Check that the databases are imported correctly:</p>
<div class="listingblock">
<div class="content">
<pre>. ~/.source_cloud_exported_variables

# use 'oc exec' and 'mysql -rs' to maintain formatting
dbs=$(oc exec openstack-galera-0 -c galera -- mysql -rs -uroot "-p$PODIFIED_DB_ROOT_PASSWORD" -e 'SHOW databases;')
echo $dbs | grep -Eq '\bkeystone\b'

# ensure neutron db is renamed from ovs_neutron
echo $dbs | grep -Eq '\bneutron\b'
echo $PULL_OPENSTACK_CONFIGURATION_DATABASES | grep -Eq '\bovs_neutron\b'

# ensure nova cell1 db is extracted to a separate db server and renamed from nova to nova_cell1
c1dbs=$(oc exec openstack-cell1-galera-0 -c galera -- mysql -rs -uroot "-p$PODIFIED_DB_ROOT_PASSWORD" -e 'SHOW databases;')
echo $c1dbs | grep -Eq '\bnova_cell1\b'

# ensure default cell renamed to cell1, and the cell UUIDs retained intact
novadb_mapped_cells=$(oc exec openstack-galera-0 -c galera -- mysql -rs -uroot "-p$PODIFIED_DB_ROOT_PASSWORD" \
  nova_api -e 'select uuid,name,transport_url,database_connection,disabled from cell_mappings;')
uuidf='\S{8,}-\S{4,}-\S{4,}-\S{4,}-\S{12,}'
left_behind=$(comm -23 \
  &lt;(echo $PULL_OPENSTACK_CONFIGURATION_NOVADB_MAPPED_CELLS | grep -oE " $uuidf \S+") \
  &lt;(echo $novadb_mapped_cells | tr -s "| " " " | grep -oE " $uuidf \S+"))
changed=$(comm -13 \
  &lt;(echo $PULL_OPENSTACK_CONFIGURATION_NOVADB_MAPPED_CELLS | grep -oE " $uuidf \S+") \
  &lt;(echo $novadb_mapped_cells | tr -s "| " " " | grep -oE " $uuidf \S+"))
test $(grep -Ec ' \S+$' &lt;&lt;&lt;$left_behind) -eq 1
default=$(grep -E ' default$' &lt;&lt;&lt;$left_behind)
test $(grep -Ec ' \S+$' &lt;&lt;&lt;$changed) -eq 1
grep -qE " $(awk '{print $1}' &lt;&lt;&lt;$default) cell1$" &lt;&lt;&lt;$changed

# ensure the registered Compute service name has not changed
novadb_svc_records=$(oc exec openstack-cell1-galera-0 -c galera -- mysql -rs -uroot "-p$PODIFIED_DB_ROOT_PASSWORD" \
  nova_cell1 -e "select host from services where services.binary='nova-compute' order by host asc;")
diff -Z &lt;(echo $novadb_svc_records) &lt;(echo $PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES)</pre>
</div>
</div>
</li>
<li>
<p>Delete the <code>mariadb-data</code> pod and the <code>mariadb-copy-data</code> persistent volume claim that contains the database backup:</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Consider taking a snapshot of them before deleting.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc delete pod mariadb-copy-data
$ oc delete pvc mariadb-data</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
During the pre-checks and post-checks, the <code>mariadb-client</code> pod might return a pod security warning related to the <code>restricted:latest</code> security context constraint. This warning is due to default security context constraints and does not prevent the admission controller from creating a pod. You see a warning for the short-lived pod, but it does not interfere with functionality.
For more information, see <a href="https://learn.redhat.com/t5/DO280-Red-Hat-OpenShift/About-pod-security-standards-and-warnings/m-p/32502">About pod security standards and warnings</a>.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="migrating-ovn-data_migrating-databases">Migrating OVN data</h3>
<div class="paragraph">
<p>Migrate the data in the OVN databases from the original Red&#160;Hat OpenStack Platform deployment to <code>ovsdb-server</code> instances that are running in the Red Hat OpenShift Container Platform (RHOCP) cluster.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>The <code>OpenStackControlPlane</code> resource is created.</p>
</li>
<li>
<p><code>NetworkAttachmentDefinition</code> custom resources (CRs) for the original cluster are defined. Specifically, the <code>internalapi</code> network is defined.</p>
</li>
<li>
<p>The original Networking service (neutron) and OVN <code>northd</code> are not running.</p>
</li>
<li>
<p>There is network routability between the control plane services and the adopted cluster.</p>
</li>
<li>
<p>The cloud is migrated to the Modular Layer 2 plug-in with Open Virtual Networking (ML2/OVN) mechanism driver.</p>
</li>
<li>
<p>Define the following shell variables. Replace the example values with values that are correct for your environment:</p>
<div class="listingblock">
<div class="content">
<pre>STORAGE_CLASS=local-storage
OVSDB_IMAGE=registry.redhat.io/rhosp-dev-preview/openstack-ovn-base-rhel9:18.0
SOURCE_OVSDB_IP=172.17.0.100</pre>
</div>
</div>
<div class="paragraph">
<p>To get the value to set <code>SOURCE_OVSDB_IP</code>, query the puppet-generated configurations in a Controller node:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ grep -rI 'ovn_[ns]b_conn' /var/lib/config-data/puppet-generated/</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Prepare a temporary <code>PersistentVolume</code> claim and the helper pod for the OVN backup. Adjust the storage requests for a large database, if needed:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">$ oc apply -f - &lt;&lt;EOF
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: ovn-data-cert
  namespace: openstack
spec:
  commonName: ovn-data-cert
  secretName: ovn-data-cert
  issuerRef:
    name: rootca-internal
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ovn-data
spec:
  storageClassName: $STORAGE_CLASS_NAME
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: ovn-copy-data
  annotations:
    openshift.io/scc: anyuid
    k8s.v1.cni.cncf.io/networks: internalapi
  labels:
    app: adoption
spec:
  containers:
  - image: $OVSDB_IMAGE
    command: [ "sh", "-c", "sleep infinity"]
    name: adoption
    volumeMounts:
    - mountPath: /backup
      name: ovn-data
    - mountPath: /etc/pki/tls/misc
      name: ovn-data-cert
      readOnly: true
  securityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop: ALL
    runAsNonRoot: true
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - name: ovn-data
    persistentVolumeClaim:
      claimName: ovn-data
  - name: ovn-data-cert
    secret:
      secretName: ovn-data-cert
EOF</code></pre>
</div>
</div>
</li>
<li>
<p>Wait for the pod to be ready:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait --for=condition=Ready pod/ovn-copy-data --timeout=30s</pre>
</div>
</div>
</li>
<li>
<p>Back up your OVN databases:</p>
<div class="ulist">
<ul>
<li>
<p>If you did not enable TLS everywhere, run the following command:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec ovn-copy-data -- bash -c "ovsdb-client backup tcp:$SOURCE_OVSDB_IP:6641 &gt; /backup/ovs-nb.db"
$ oc exec ovn-copy-data -- bash -c "ovsdb-client backup tcp:$SOURCE_OVSDB_IP:6642 &gt; /backup/ovs-sb.db"</pre>
</div>
</div>
</li>
<li>
<p>If you enabled TLS everywhere, run the following command:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec ovn-copy-data -- bash -c "ovsdb-client backup --ca-cert=/etc/pki/tls/misc/ca.crt --private-key=/etc/pki/tls/misc/tls.key --certificate=/etc/pki/tls/misc/tls.crt ssl:$SOURCE_OVSDB_IP:6641 &gt; /backup/ovs-nb.db"
$ oc exec ovn-copy-data -- bash -c "ovsdb-client backup --ca-cert=/etc/pki/tls/misc/ca.crt --private-key=/etc/pki/tls/misc/tls.key --certificate=/etc/pki/tls/misc/tls.crt ssl:$SOURCE_OVSDB_IP:6642 &gt; /backup/ovs-sb.db"</pre>
</div>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Start the control plane OVN database services prior to import, with <code>northd</code> and <code>ovn-controller</code> disabled:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">$ oc patch openstackcontrolplane openstack-galera-network-isolation --type=merge --patch '
spec:
  ovn:
    enabled: true
    template:
      ovnDBCluster:
        ovndbcluster-nb:
          dbType: NB
          storageRequest: 10G
          networkAttachment: internalapi
        ovndbcluster-sb:
          dbType: SB
          storageRequest: 10G
          networkAttachment: internalapi
      ovnNorthd:
        replicas: 1
      ovnController:
        networkAttachment: tenant
        nodeSelector:
          node: non-existing-node-name
'</code></pre>
</div>
</div>
</li>
<li>
<p>Wait for the OVN database services to reach the <code>Running</code> phase:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait --for=jsonpath='{.status.phase}'=Running pod --selector=service=ovsdbserver-nb
$ oc wait --for=jsonpath='{.status.phase}'=Running pod --selector=service=ovsdbserver-sb</pre>
</div>
</div>
</li>
<li>
<p>Fetch the OVN database IP addresses on the <code>clusterIP</code> service network:</p>
<div class="listingblock">
<div class="content">
<pre>PODIFIED_OVSDB_NB_IP=$(oc get svc --selector "statefulset.kubernetes.io/pod-name=ovsdbserver-nb-0" -ojsonpath='{.items[0].spec.clusterIP}')
PODIFIED_OVSDB_SB_IP=$(oc get svc --selector "statefulset.kubernetes.io/pod-name=ovsdbserver-sb-0" -ojsonpath='{.items[0].spec.clusterIP}')</pre>
</div>
</div>
</li>
<li>
<p>Upgrade the database schema for the backup files:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>If you did not enable TLS everywhere, use the following command:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec ovn-copy-data -- bash -c "ovsdb-client get-schema tcp:$PODIFIED_OVSDB_NB_IP:6641 &gt; /backup/ovs-nb.ovsschema &amp;&amp; ovsdb-tool convert /backup/ovs-nb.db /backup/ovs-nb.ovsschema"
$ oc exec ovn-copy-data -- bash -c "ovsdb-client get-schema tcp:$PODIFIED_OVSDB_SB_IP:6642 &gt; /backup/ovs-sb.ovsschema &amp;&amp; ovsdb-tool convert /backup/ovs-sb.db /backup/ovs-sb.ovsschema"</pre>
</div>
</div>
</li>
<li>
<p>If you enabled TLS everywhere, use the following command:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec ovn-copy-data -- bash -c "ovsdb-client get-schema --ca-cert=/etc/pki/tls/misc/ca.crt --private-key=/etc/pki/tls/misc/tls.key --certificate=/etc/pki/tls/misc/tls.crt ssl:$PODIFIED_OVSDB_NB_IP:6641 &gt; /backup/ovs-nb.ovsschema &amp;&amp; ovsdb-tool convert /backup/ovs-nb.db /backup/ovs-nb.ovsschema"
$ oc exec ovn-copy-data -- bash -c "ovsdb-client get-schema --ca-cert=/etc/pki/tls/misc/ca.crt --private-key=/etc/pki/tls/misc/tls.key --certificate=/etc/pki/tls/misc/tls.crt ssl:$PODIFIED_OVSDB_SB_IP:6642 &gt; /backup/ovs-sb.ovsschema &amp;&amp; ovsdb-tool convert /backup/ovs-sb.db /backup/ovs-sb.ovsschema"</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Restore the database backup to the new OVN database servers:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>If you did not enable TLS everywhere, use the following command:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec ovn-copy-data -- bash -c "ovsdb-client restore tcp:$PODIFIED_OVSDB_NB_IP:6641 &lt; /backup/ovs-nb.db"
$ oc exec ovn-copy-data -- bash -c "ovsdb-client restore tcp:$PODIFIED_OVSDB_SB_IP:6642 &lt; /backup/ovs-sb.db"</pre>
</div>
</div>
</li>
<li>
<p>If you enabled TLS everywhere, use the following command:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec ovn-copy-data -- bash -c "ovsdb-client restore --ca-cert=/etc/pki/tls/misc/ca.crt --private-key=/etc/pki/tls/misc/tls.key --certificate=/etc/pki/tls/misc/tls.crt ssl:$PODIFIED_OVSDB_NB_IP:6641 &lt; /backup/ovs-nb.db"
$ oc exec ovn-copy-data -- bash -c "ovsdb-client restore --ca-cert=/etc/pki/tls/misc/ca.crt --private-key=/etc/pki/tls/misc/tls.key --certificate=/etc/pki/tls/misc/tls.crt ssl:$PODIFIED_OVSDB_SB_IP:6642 &lt; /backup/ovs-sb.db"</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Check that the data was successfully migrated by running the following commands against the new database servers, for example:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec -it ovsdbserver-nb-0 -- ovn-nbctl show
$ oc exec -it ovsdbserver-sb-0 -- ovn-sbctl list Chassis</pre>
</div>
</div>
</li>
<li>
<p>Start the control plane <code>ovn-northd</code> service to keep both OVN databases in sync:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">$ oc patch openstackcontrolplane openstack-galera-network-isolation --type=merge --patch '
spec:
  ovn:
    enabled: true
    template:
      ovnNorthd:
        replicas: 1
'</code></pre>
</div>
</div>
</li>
<li>
<p>If you are running OVN gateway services on RHOCP nodes, enable the control plane <code>ovn-controller</code> service:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">$ oc patch openstackcontrolplane openstack-galera-network-isolation --type=json -p="[{'op': 'remove', 'path': '/spec/ovn/template/ovnController/nodeSelector'}]"</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Running OVN gateways on RHOCP nodes might be prone to data plane downtime during Open vSwitch upgrades. Consider running OVN gateways on dedicated <code>Networker</code> data plane nodes for production deployments instead.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Delete the <code>ovn-data</code> helper pod and the temporary <code>PersistentVolumeClaim</code> that is used to store OVN database backup files:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc delete pod ovn-copy-data
$ oc delete pvc ovn-data</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Consider taking a snapshot of the <code>ovn-data</code> helper pod and the temporary <code>PersistentVolumeClaim</code> before deleting them. For more information, see <a href="https://docs.openshift.com/container-platform/4.15//storage/index#lvms-about-volume-snapsot_logical-volume-manager-storage">About volume snapshots</a> in <em>OpenShift Container Platform storage overview</em>.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Stop the adopted OVN database servers:</p>
<div class="listingblock">
<div class="content">
<pre>ServicesToStop=("tripleo_ovn_cluster_north_db_server.service"
                "tripleo_ovn_cluster_south_db_server.service")

echo "Stopping systemd OpenStack services"
for service in ${ServicesToStop[*]}; do
    for i in {1..3}; do
        SSH_CMD=CONTROLLER${i}_SSH
        if [ ! -z "${!SSH_CMD}" ]; then
            echo "Stopping the $service in controller $i"
            if ${!SSH_CMD} sudo systemctl is-active $service; then
                ${!SSH_CMD} sudo systemctl stop $service
            fi
        fi
    done
done

echo "Checking systemd OpenStack services"
for service in ${ServicesToStop[*]}; do
    for i in {1..3}; do
        SSH_CMD=CONTROLLER${i}_SSH
        if [ ! -z "${!SSH_CMD}" ]; then
            if ! ${!SSH_CMD} systemctl show $service | grep ActiveState=inactive &gt;/dev/null; then
                echo "ERROR: Service $service still running on controller $i"
            else
                echo "OK: Service $service is not running on controller $i"
            fi
        fi
    done
done</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="adopting-openstack-control-plane-services_migrating-databases">Adopting Red&#160;Hat OpenStack Platform control plane services</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Adopt your Red&#160;Hat OpenStack Platform 17.1 control plane services to deploy them in the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) 18.0 control plane.</p>
</div>
<div class="sect2">
<h3 id="adopting-the-identity-service_adopt-control-plane">Adopting the Identity service</h3>
<div class="paragraph">
<p>To adopt the Identity service (keystone), you patch an existing <code>OpenStackControlPlane</code> custom resource (CR) where the Identity service is disabled. The patch starts the service with the configuration parameters that are provided by the Red&#160;Hat OpenStack Platform (RHOSP) environment.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Create the keystone secret that includes the Fernet keys that were copied from the RHOSP environment:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: v1
data:
  CredentialKeys0: $($CONTROLLER1_SSH sudo cat /var/lib/config-data/puppet-generated/keystone/etc/keystone/credential-keys/0 | base64 -w 0)
  CredentialKeys1: $($CONTROLLER1_SSH sudo cat /var/lib/config-data/puppet-generated/keystone/etc/keystone/credential-keys/1 | base64 -w 0)
  FernetKeys0: $($CONTROLLER1_SSH sudo cat /var/lib/config-data/puppet-generated/keystone/etc/keystone/fernet-keys/0 | base64 -w 0)
  FernetKeys1: $($CONTROLLER1_SSH sudo cat /var/lib/config-data/puppet-generated/keystone/etc/keystone/fernet-keys/1 | base64 -w 0)
kind: Secret
metadata:
  name: keystone
  namespace: openstack
type: Opaque
EOF</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to deploy the Identity service:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  keystone:
    enabled: true
    apiOverride:
      route: {}
    template:
      override:
        service:
          internal:
            metadata:
              annotations:
                metallb.universe.tf/address-pool: internalapi
                metallb.universe.tf/allow-shared-ip: internalapi
                metallb.universe.tf/loadBalancerIPs: 172.17.0.80
            spec:
              type: LoadBalancer
      databaseInstance: openstack
      secret: osp-secret
'</pre>
</div>
</div>
</li>
<li>
<p>Create an alias to use the <code>openstack</code> command in the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) deployment:</p>
<div class="listingblock">
<div class="content">
<pre>$ alias openstack="oc exec -t openstackclient -- openstack"</pre>
</div>
</div>
</li>
<li>
<p>Remove services and endpoints that still point to the RHOSP
control plane, excluding the Identity service and its endpoints:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack endpoint list | grep keystone | awk '/admin/{ print $2; }' | xargs ${BASH_ALIASES[openstack]} endpoint delete || true

for service in aodh heat heat-cfn barbican cinderv3 glance manila manilav2 neutron nova placement swift ironic-inspector ironic; do
  openstack service list | awk "/ $service /{ print \$2; }" | xargs ${BASH_ALIASES[openstack]} service delete || true
done</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Verification</div>
<ul>
<li>
<p>Confirm that the Identity service endpoints are defined and are pointing to the control plane FQDNs:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack endpoint list | grep keystone</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="adopting-the-key-manager-service_adopt-control-plane">Adopting the Key Manager service</h3>
<div class="paragraph">
<p>To adopt the Key Manager service (barbican), you patch an existing <code>OpenStackControlPlane</code> custom resource (CR) where Key Manager service is disabled. The patch starts the service with the configuration parameters that are provided by the Red&#160;Hat OpenStack Platform (RHOSP) environment.</p>
</div>
<div class="paragraph">
<p>The Key Manager service adoption is complete if you see the following results:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The <code>BarbicanAPI</code>, <code>BarbicanWorker</code>, and <code>BarbicanKeystoneListener</code> services are up and running.</p>
</li>
<li>
<p>Keystone endpoints are updated, and the same crypto plugin of the source cloud is available.</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
This procedure configures the Key Manager service to use the <code>simple_crypto</code> back end. Additional back ends, such as PKCS11 and DogTag, are currently not supported in Red&#160;Hat OpenStack Services on OpenShift (RHOSO).
</td>
</tr>
</table>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Add the kek secret:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc set data secret/osp-secret "BarbicanSimpleCryptoKEK=$($CONTROLLER1_SSH "python3 -c \"import configparser; c = configparser.ConfigParser(); c.read('/var/lib/config-data/puppet-generated/barbican/etc/barbican/barbican.conf'); print(c['simple_crypto_plugin']['kek'])\"")"</pre>
</div>
</div>
</li>
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to deploy the Key Manager service:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  barbican:
    enabled: true
    apiOverride:
      route: {}
    template:
      databaseInstance: openstack
      databaseAccount: barbican
      rabbitMqClusterName: rabbitmq
      secret: osp-secret
      simpleCryptoBackendSecret: osp-secret
      serviceAccount: barbican
      serviceUser: barbican
      passwordSelectors:
        database: BarbicanDatabasePassword
        service: BarbicanPassword
        simplecryptokek: BarbicanSimpleCryptoKEK
      barbicanAPI:
        replicas: 1
        override:
          service:
            internal:
              metadata:
                annotations:
                  metallb.universe.tf/address-pool: internalapi
                  metallb.universe.tf/allow-shared-ip: internalapi
                  metallb.universe.tf/loadBalancerIPs: 172.17.0.80
              spec:
                type: LoadBalancer
      barbicanWorker:
        replicas: 1
      barbicanKeystoneListener:
        replicas: 1
'</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Verification</div>
<ul>
<li>
<p>Ensure that the Identity service (keystone) endpoints are defined and are pointing to the control plane FQDNs:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack endpoint list | grep key-manager</pre>
</div>
</div>
</li>
<li>
<p>Ensure that Barbican API service is registered in the Identity service:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack service list | grep key-manager</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>$ openstack endpoint list | grep key-manager</pre>
</div>
</div>
</li>
<li>
<p>List the secrets:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack secret list</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="adopting-the-object-storage-service_adopt-control-plane">Adopting the Object Storage service</h3>
<div class="paragraph">
<p>If you are using Object Storage as a service, adopt the Object Storage service (swift) to the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) environment. If you are using the Object Storage API of the Ceph Object Gateway (RGW), skip the following procedure.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Ensure that the Object Storage service storage back-end services are running.</p>
</li>
<li>
<p>Ensure that the storage network is properly configured on the Red Hat OpenShift Container Platform cluster. For more information, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0-beta/html/deploying_red_hat_openstack_services_on_openshift/assembly_preparing-rhocp-for-rhoso#proc_configuring-the-data-plane-network_preparing">Configuring the data plane network</a> in <em>Deploying Red Hat OpenStack Services on OpenShift</em>.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create the <code>swift-conf</code> secret that includes the Object Storage service hash path suffix and prefix:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">$ oc apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Secret
metadata:
  name: swift-conf
  namespace: openstack
type: Opaque
data:
  swift.conf: $($CONTROLLER1_SSH sudo cat /var/lib/config-data/puppet-generated/swift/etc/swift/swift.conf | base64 -w0)
EOF</code></pre>
</div>
</div>
</li>
<li>
<p>Create the <code>swift-ring-files</code> <code>ConfigMap</code> that includes the Object Storage service ring files:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">$ oc apply -f - &lt;&lt;EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: swift-ring-files
binaryData:
  swiftrings.tar.gz: $($CONTROLLER1_SSH "cd /var/lib/config-data/puppet-generated/swift/etc/swift &amp;&amp; tar cz *.builder *.ring.gz backups/ | base64 -w0")
  account.ring.gz: $($CONTROLLER1_SSH "base64 -w0 /var/lib/config-data/puppet-generated/swift/etc/swift/account.ring.gz")
  container.ring.gz: $($CONTROLLER1_SSH "base64 -w0 /var/lib/config-data/puppet-generated/swift/etc/swift/container.ring.gz")
  object.ring.gz: $($CONTROLLER1_SSH "base64 -w0 /var/lib/config-data/puppet-generated/swift/etc/swift/object.ring.gz")
EOF</code></pre>
</div>
</div>
</li>
<li>
<p>Patch the <code>OpenStackControlPlane</code> custom resource to deploy the Object Storage service:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  swift:
    enabled: true
    template:
      memcachedInstance: memcached
      swiftRing:
        ringReplicas: 1
      swiftStorage:
        replicas: 0
        networkAttachments:
        - storage
        storageClass: local-storage
        storageRequest: 10Gi
      swiftProxy:
        secret: osp-secret
        replicas: 1
        passwordSelectors:
          service: SwiftPassword
        serviceUser: swift
        override:
          service:
            internal:
              metadata:
                annotations:
                  metallb.universe.tf/address-pool: internalapi
                  metallb.universe.tf/allow-shared-ip: internalapi
                  metallb.universe.tf/loadBalancerIPs: 172.17.0.80
              spec:
                type: LoadBalancer
        networkAttachments:
        - storage
'</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Verification</div>
<ul>
<li>
<p>Inspect the resulting Object Storage service pods:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get pods -l component=swift-proxy</pre>
</div>
</div>
</li>
<li>
<p>Verify that the Object Storage proxy service is registered in the Identity service (keystone):</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack service list | grep swift
| b5b9b1d3c79241aa867fa2d05f2bbd52 | swift    | object-store |</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>$ openstack endpoint list | grep swift
| 32ee4bd555414ab48f2dc90a19e1bcd5 | regionOne | swift        | object-store | True    | public    | https://swift-public-openstack.apps-crc.testing/v1/AUTH_%(tenant_id)s |
| db4b8547d3ae4e7999154b203c6a5bed | regionOne | swift        | object-store | True    | internal  | http://swift-internal.openstack.svc:8080/v1/AUTH_%(tenant_id)s        |</pre>
</div>
</div>
</li>
<li>
<p>Verify that you are able to upload and download objects:</p>
<div class="listingblock">
<div class="content">
<pre>openstack container create test
+---------------------------------------+-----------+------------------------------------+
| account                               | container | x-trans-id                         |
+---------------------------------------+-----------+------------------------------------+
| AUTH_4d9be0a9193e4577820d187acdd2714a | test      | txe5f9a10ce21e4cddad473-0065ce41b9 |
+---------------------------------------+-----------+------------------------------------+

openstack object create test --name obj &lt;(echo "Hello World!")
+--------+-----------+----------------------------------+
| object | container | etag                             |
+--------+-----------+----------------------------------+
| obj    | test      | d41d8cd98f00b204e9800998ecf8427e |
+--------+-----------+----------------------------------+

openstack object save test obj --file -
Hello World!</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The Object Storage data is still stored on the existing RHOSP nodes. For more information about migrating the actual data from the RHOSP deployment to the RHOSO deployment, see <a href="#migrating-object-storage-data-to-rhoso-nodes_migrate-object-storage-service">Migrating the Object Storage service (swift) data from RHOSP to Red&#160;Hat OpenStack Services on OpenShift (RHOSO) nodes</a>.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="adopting-the-image-service_adopt-control-plane">Adopting the Image service</h3>
<div class="paragraph">
<p>To adopt the Image Service (glance) you patch an existing <code>OpenStackControlPlane</code> custom resource (CR) that has the Image service disabled. The patch starts the service with the configuration parameters that are provided by the Red&#160;Hat OpenStack Platform (RHOSP) environment.</p>
</div>
<div class="paragraph">
<p>The Image service adoption is complete if you see the following results:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The <code>GlanceAPI</code> service up and running.</p>
</li>
<li>
<p>The Identity service endpoints are updated, and the same back end of the source cloud is available.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>To complete the Image service adoption, ensure that your environment meets the following criteria:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>You have a running director environment (the source cloud).</p>
</li>
<li>
<p>You have a Single Node OpenShift or OpenShift Local that is running in the Red Hat OpenShift Container Platform cluster.</p>
</li>
<li>
<p>Optional: You can reach an internal/external <code>Ceph</code> cluster by both <code>crc</code> and director.</p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="adopting-image-service-with-object-storage-backend_image-service">Adopting the Image service that is deployed with a Object Storage service back end</h4>
<div class="paragraph">
<p>Adopt the Image Service (glance) that you deployed with an Object Storage service (swift) back end in the Red&#160;Hat OpenStack Platform (RHOSP) environment. The control plane <code>glanceAPI</code> instance is deployed with the following configuration. You use this configuration in the patch manifest that deploys the Image service with the object storage back end:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>..
spec
  glance:
   ...
      customServiceConfig: |
          [DEFAULT]
          enabled_backends = default_backend:swift
          [glance_store]
          default_backend = default_backend
          [default_backend]
          swift_store_create_container_on_put = True
          swift_store_auth_version = 3
          swift_store_auth_address = {{ .KeystoneInternalURL }}
          swift_store_endpoint_type = internalURL
          swift_store_user = service:glance
          swift_store_key = {{ .ServicePassword }}</pre>
</div>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You have completed the previous adoption steps.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create a new file, for example, <code>glance_swift.patch</code>, and include the following content:</p>
<div class="listingblock">
<div class="content">
<pre>spec:
  glance:
    enabled: true
    apiOverride:
      route: {}
    template:
      secret: osp-secret
      databaseInstance: openstack
      storage:
        storageRequest: 10G
      customServiceConfig: |
        [DEFAULT]
        enabled_backends = default_backend:swift
        [glance_store]
        default_backend = default_backend
        [default_backend]
        swift_store_create_container_on_put = True
        swift_store_auth_version = 3
        swift_store_auth_address = {{ .KeystoneInternalURL }}
        swift_store_endpoint_type = internalURL
        swift_store_user = service:glance
        swift_store_key = {{ .ServicePassword }}
      glanceAPIs:
        default:
          replicas: 1
          override:
            service:
              internal:
                metadata:
                  annotations:
                    metallb.universe.tf/address-pool: internalapi
                    metallb.universe.tf/allow-shared-ip: internalapi
                    metallb.universe.tf/loadBalancerIPs: 172.17.0.80
                spec:
                  type: LoadBalancer
          networkAttachments:
            - storage</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The Object Storage service as a back end establishes a dependency with the Image service. Any deployed <code>GlanceAPI</code> instances do not work if the Image service is configured with the Object Storage service that is not available in the <code>OpenStackControlPlane</code> custom resource.
After the Object Storage service, and in particular <code>SwiftProxy</code>, is adopted, you can proceed with the <code>GlanceAPI</code> adoption. For more information, see <a href="#adopting-the-object-storage-service_adopt-control-plane">Adopting the Object Storage service</a>.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Verify that <code>SwiftProxy</code> is available:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get pod -l component=swift-proxy | grep Running
swift-proxy-75cb47f65-92rxq   3/3     Running   0</pre>
</div>
</div>
</li>
<li>
<p>Patch the <code>GlanceAPI</code> service that is deployed in the control plane:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch-file=glance_swift.patch</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="adopting-image-service-with-block-storage-backend_image-service">Adopting the Image service that is deployed with a Block Storage service back end</h4>
<div class="paragraph">
<p>Adopt the Image Service (glance) that you deployed with a Block Storage service (cinder) back end in the Red&#160;Hat OpenStack Platform (RHOSP) environment. The control plane <code>glanceAPI</code> instance is deployed with the following configuration. You use this configuration in the patch manifest that deploys the Image service with the block storage back end:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>..
spec
  glance:
   ...
      customServiceConfig: |
          [DEFAULT]
          enabled_backends = default_backend:cinder
          [glance_store]
          default_backend = default_backend
          [default_backend]
          rootwrap_config = /etc/glance/rootwrap.conf
          description = Default cinder backend
          cinder_store_auth_address = {{ .KeystoneInternalURL }}
          cinder_store_user_name = {{ .ServiceUser }}
          cinder_store_password = {{ .ServicePassword }}
          cinder_store_project_name = service
          cinder_catalog_info = volumev3::internalURL
          cinder_use_multipath = true</pre>
</div>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You have completed the previous adoption steps.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create a new file, for example <code>glance_cinder.patch</code>, and include the following content:</p>
<div class="listingblock">
<div class="content">
<pre>spec:
  glance:
    enabled: true
    apiOverride:
      route: {}
    template:
      secret: osp-secret
      databaseInstance: openstack
      storage:
        storageRequest: 10G
      customServiceConfig: |
        [DEFAULT]
        enabled_backends = default_backend:cinder
        [glance_store]
        default_backend = default_backend
        [default_backend]
        rootwrap_config = /etc/glance/rootwrap.conf
        description = Default cinder backend
        cinder_store_auth_address = {{ .KeystoneInternalURL }}
        cinder_store_user_name = {{ .ServiceUser }}
        cinder_store_password = {{ .ServicePassword }}
        cinder_store_project_name = service
        cinder_catalog_info = volumev3::internalURL
        cinder_use_multipath = true
      glanceAPIs:
        default:
          replicas: 1
          override:
            service:
              internal:
                metadata:
                  annotations:
                    metallb.universe.tf/address-pool: internalapi
                    metallb.universe.tf/allow-shared-ip: internalapi
                    metallb.universe.tf/loadBalancerIPs: 172.17.0.80
                spec:
                  type: LoadBalancer
          networkAttachments:
            - storage</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The Block Storage service as a back end establishes a dependency with the Image service. Any deployed <code>GlanceAPI</code> instances do not work if the Image service is configured with the Block Storage service that is not available in the <code>OpenStackControlPlane</code> custom resource.
After the Block Storage service, and in particular <code>CinderVolume</code>, is adopted, you can proceed with the <code>GlanceAPI</code> adoption. For more information, see <a href="#adopting-the-block-storage-service_adopt-control-plane">Adopting the Block Storage service</a>.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Verify that <code>CinderVolume</code> is available:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get pod -l component=cinder-volume | grep Running
cinder-volume-75cb47f65-92rxq   3/3     Running   0</pre>
</div>
</div>
</li>
<li>
<p>Patch the <code>GlanceAPI</code> service that is deployed in the control plane:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch-file=glance_cinder.patch</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="adopting-image-service-with-nfs-backend_image-service">Adopting the Image service that is deployed with an NFS back end</h4>
<div class="paragraph">
<p>Adopt the Image Service (glance) that you deployed with an NFS back end. To complete the following procedure, ensure that your environment meets the following criteria:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The Storage network is propagated to the Red&#160;Hat OpenStack Platform (RHOSP) control plane.</p>
</li>
<li>
<p>The Image service can reach the Storage network and connect to the nfs-server through the port <code>2049</code>.</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You have completed the previous adoption steps.</p>
</li>
<li>
<p>In the source cloud, verify the NFS parameters that the overcloud uses to configure the Image service back end. Specifically, in yourdirector heat templates, find the following variables that override the default content that is provided by the <code>glance-nfs.yaml</code> file in the
<code>/usr/share/openstack-tripleo-heat-templates/environments/storage</code> directory:</p>
<div class="listingblock">
<div class="content">
<pre>**GlanceBackend**: file

**GlanceNfsEnabled**: true

**GlanceNfsShare**: 192.168.24.1:/var/nfs</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>In this example, the <code>GlanceBackend</code> variable shows that the Image service has no notion of an NFS back end. The variable is using the <code>File</code> driver and, in the background, the <code>filesystem_store_datadir</code>. The <code>filesystem_store_datadir</code> is mapped to the export value provided by the <code>GlanceNfsShare</code> variable instead of <code>/var/lib/glance/images/</code>.
If you do not export the <code>GlanceNfsShare</code> through a network that is propagated to the adopted Red&#160;Hat OpenStack Services on OpenShift (RHOSO) control plane, you must stop the <code>nfs-server</code> and remap the export to the <code>storage</code> network. Before doing so, ensure that the Image service is stopped in the source Controller nodes.</p>
</div>
<div class="paragraph">
<p>In the control plane, the Image service is attached to the Storage network, then propagated through the associated <code>NetworkAttachmentsDefinition</code> custom resource (CR), and the resulting pods already have the right permissions to handle the Image service traffic through this network.
In a deployed RHOSP control plane, you can verify that the network mapping matches with what has been deployed in the director-based environment by checking both the <code>NodeNetworkConfigPolicy</code> (<code>nncp</code>) and the <code>NetworkAttachmentDefinition</code> (<code>net-attach-def</code>). The following is an example of the output that you should check in the Red Hat OpenShift Container Platform environment to make sure that there are no issues with the propagated networks:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc get nncp
NAME                        STATUS      REASON
enp6s0-crc-8cf2w-master-0   Available   SuccessfullyConfigured

$ oc get net-attach-def
NAME
ctlplane
internalapi
storage
tenant

$ oc get ipaddresspool -n metallb-system
NAME          AUTO ASSIGN   AVOID BUGGY IPS   ADDRESSES
ctlplane      true          false             ["192.168.122.80-192.168.122.90"]
internalapi   true          false             ["172.17.0.80-172.17.0.90"]
storage       true          false             ["172.18.0.80-172.18.0.90"]
tenant        true          false             ["172.19.0.80-172.19.0.90"]</pre>
</div>
</div>
</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Adopt the Image service and create a new <code>default</code> <code>GlanceAPI</code> instance that is connected with the existing NFS share:</p>
<div class="listingblock">
<div class="content">
<pre>$ cat &lt;&lt; EOF &gt; glance_nfs_patch.yaml

spec:
  extraMounts:
  - extraVol:
    - extraVolType: Nfs
      mounts:
      - mountPath: /var/lib/glance/images
        name: nfs
      propagation:
      - Glance
      volumes:
      - name: nfs
        nfs:
          path: &lt;exported_path&gt;
          server: &lt;ip_address&gt;
    name: r1
    region: r1
  glance:
    enabled: true
    template:
      databaseInstance: openstack
      customServiceConfig: |
        [DEFAULT]
        enabled_backends = default_backend:file
        [glance_store]
        default_backend = default_backend
        [default_backend]
        filesystem_store_datadir = /var/lib/glance/images/
      storage:
        storageRequest: 10G
      glanceAPIs:
        default:
          replicas: 0
          type: single
          override:
            service:
              internal:
                metadata:
                  annotations:
                    metallb.universe.tf/address-pool: internalapi
                    metallb.universe.tf/allow-shared-ip: internalapi
                    metallb.universe.tf/loadBalancerIPs: 172.17.0.80
                spec:
                  type: LoadBalancer
          networkAttachments:
          - storage
EOF</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;ip_address&gt;</code> with the IP address that you use to reach the <code>nfs-server</code>.</p>
</li>
<li>
<p>Replace <code>&lt;exported_path&gt;</code> with the exported path in the <code>nfs-server</code>.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to deploy the Image service with an NFS back end:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch-file glance_nfs_patch.yaml</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Verification</div>
<ul>
<li>
<p>When <code>GlanceAPI</code> is active, confirm that you can see a single API instance:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get pods -l service=glance
NAME                      READY   STATUS    RESTARTS
glance-default-single-0   3/3     Running   0
```</pre>
</div>
</div>
</li>
<li>
<p>Ensure that the description of the pod reports the following output:</p>
<div class="listingblock">
<div class="content">
<pre>Mounts:
...
  nfs:
    Type:      NFS (an NFS mount that lasts the lifetime of a pod)
    Server:    {{ server ip address }}
    Path:      {{ nfs export path }}
    ReadOnly:  false
...</pre>
</div>
</div>
</li>
<li>
<p>Check that the mountpoint that points to <code>/var/lib/glance/images</code> is mapped to the expected <code>nfs server ip</code> and <code>nfs path</code> that you defined in the new default <code>GlanceAPI</code> instance:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc rsh -c glance-api glance-default-single-0

sh-5.1# mount
...
...
{{ ip address }}:/var/nfs on /var/lib/glance/images type nfs4 (rw,relatime,vers=4.2,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=172.18.0.5,local_lock=none,addr=172.18.0.5)
...
...</pre>
</div>
</div>
</li>
<li>
<p>Confirm that the UUID is created in the exported directory on the NFS node. For example:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc rsh openstackclient
$ openstack image list

sh-5.1$  curl -L -o /tmp/cirros-0.5.2-x86_64-disk.img http://download.cirros-cloud.net/0.5.2/cirros-0.5.2-x86_64-disk.img
...
...

sh-5.1$ openstack image create --container-format bare --disk-format raw --file /tmp/cirros-0.5.2-x86_64-disk.img cirros
...
...

sh-5.1$ openstack image list
+--------------------------------------+--------+--------+
| ID                                   | Name   | Status |
+--------------------------------------+--------+--------+
| 634482ca-4002-4a6d-b1d5-64502ad02630 | cirros | active |
+--------------------------------------+--------+--------+</pre>
</div>
</div>
</li>
<li>
<p>On the <code>nfs-server</code> node, the same <code>uuid</code> is in the exported <code>/var/nfs</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ ls /var/nfs/
634482ca-4002-4a6d-b1d5-64502ad02630</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="adopting-image-service-with-ceph-backend_image-service">Adopting the Image service that is deployed with a Red Hat Ceph Storage back end</h4>
<div class="paragraph">
<p>Adopt the Image Service (glance) that you deployed with a Red Hat Ceph Storage back end. Use the <code>customServiceConfig</code> parameter to inject the right configuration to the <code>GlanceAPI</code> instance.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You have completed the previous adoption steps.</p>
</li>
<li>
<p>Ensure that the Ceph-related secret (<code>ceph-conf-files</code>) is created in
the <code>openstack</code> namespace and that the <code>extraMounts</code> property of the
<code>OpenStackControlPlane</code> custom resource (CR) is configured properly. For more information, see <a href="#configuring-a-ceph-backend_migrating-databases">Configuring a Ceph back end</a>.</p>
<div class="listingblock">
<div class="content">
<pre>$ cat &lt;&lt; EOF &gt; glance_patch.yaml
spec:
  glance:
    enabled: true
    template:
      databaseInstance: openstack
      customServiceConfig: |
        [DEFAULT]
        enabled_backends=default_backend:rbd
        [glance_store]
        default_backend=default_backend
        [default_backend]
        rbd_store_ceph_conf=/etc/ceph/ceph.conf
        rbd_store_user=openstack
        rbd_store_pool=images
        store_description=Ceph glance store backend.
      storage:
        storageRequest: 10G
      glanceAPIs:
        default:
          replicas: 0
          override:
            service:
              internal:
                metadata:
                  annotations:
                    metallb.universe.tf/address-pool: internalapi
                    metallb.universe.tf/allow-shared-ip: internalapi
                    metallb.universe.tf/loadBalancerIPs: 172.17.0.80
                spec:
                  type: LoadBalancer
          networkAttachments:
          - storage
EOF</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If you backed up your Red&#160;Hat OpenStack Platform (RHOSP) services configuration file from the original environment, you can compare it with the confgiuration file that you adopted and ensure that the configuration is correct.
For more information, see <a href="#pulling-configuration-from-tripleo-deployment_reviewing-configuration">Pulling the configuration from a director deployment</a>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>os-diff diff /tmp/collect_tripleo_configs/glance/etc/glance/glance-api.conf glance_patch.yaml --crd</pre>
</div>
</div>
<div class="paragraph">
<p>This command produces the difference between both ini configuration files.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="ulist">
<div class="title">Procedure</div>
<ul>
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to deploy the Image service with a Red Hat Ceph Storage back end:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch-file glance_patch.yaml</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="verifying-the-image-service-adoption_image-service">Verifying the Image service adoption</h4>
<div class="paragraph">
<p>Verify that you adopted the Image Service (glance) to the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) 18.0 deployment.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Test the Image service from the Red&#160;Hat OpenStack Platform CLI. You can compare and ensure that the configuration is applied to the Image service pods:</p>
<div class="listingblock">
<div class="content">
<pre>$ os-diff diff /etc/glance/glance.conf.d/02-config.conf glance_patch.yaml --frompod -p glance-api</pre>
</div>
</div>
<div class="paragraph">
<p>If no line appears, then the configuration is correct.</p>
</div>
</li>
<li>
<p>Inspect the resulting Image service pods:</p>
<div class="listingblock">
<div class="content">
<pre>GLANCE_POD=`oc get pod |grep glance-default | cut -f 1 -d' ' | head -n 1`
oc exec -t $GLANCE_POD -c glance-api -- cat /etc/glance/glance.conf.d/02-config.conf

[DEFAULT]
enabled_backends=default_backend:rbd
[glance_store]
default_backend=default_backend
[default_backend]
rbd_store_ceph_conf=/etc/ceph/ceph.conf
rbd_store_user=openstack
rbd_store_pool=images
store_description=Ceph glance store backend.</pre>
</div>
</div>
</li>
<li>
<p>If you use a Red Hat Ceph Storage back end, ensure that the Red Hat Ceph Storage secrets are mounted:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec -t $GLANCE_POD -c glance-api -- ls /etc/ceph
ceph.client.openstack.keyring
ceph.conf</pre>
</div>
</div>
</li>
<li>
<p>Check that the service is active, and that the endpoints are updated in the RHOSP CLI:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc rsh openstackclient -n openstackclient
$ openstack service list | grep image

| fc52dbffef36434d906eeb99adfc6186 | glance    | image        |

$ openstack endpoint list | grep image

| 569ed81064f84d4a91e0d2d807e4c1f1 | regionOne | glance       | image        | True    | internal  | http://glance-internal-openstack.apps-crc.testing   |
| 5843fae70cba4e73b29d4aff3e8b616c | regionOne | glance       | image        | True    | public    | http://glance-public-openstack.apps-crc.testing     |</pre>
</div>
</div>
</li>
<li>
<p>Check that the images that you previously listed in the source cloud are available in the adopted service:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack image list
+--------------------------------------+--------+--------+
| ID                                   | Name   | Status |
+--------------------------------------+--------+--------+
| c3158cad-d50b-452f-bec1-f250562f5c1f | cirros | active |
+--------------------------------------+--------+--------+</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="adopting-the-placement-service_image-service">Adopting the Placement service</h3>
<div class="paragraph">
<p>To adopt the Placement service, you patch an existing <code>OpenStackControlPlane</code> custom resource (CR) that has the Placement service disabled. The patch starts the service with the configuration parameters that are provided by the Red&#160;Hat OpenStack Platform (RHOSP) environment.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You import your databases to MariaDB instances on the control plane. For more information, see <a href="#migrating-databases-to-mariadb-instances_migrating-databases">Migrating databases to MariaDB instances</a>.</p>
</li>
<li>
<p>You adopt the Identity service (keystone). For more information, see <a href="#adopting-the-identity-service_adopt-control-plane">Adopting the Identity service</a>.</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Procedure</div>
<ul>
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to deploy the Placement service:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  placement:
    enabled: true
    apiOverride:
      route: {}
    template:
      databaseInstance: openstack
      databaseAccount: placement
      secret: osp-secret
      override:
        service:
          internal:
            metadata:
              annotations:
                metallb.universe.tf/address-pool: internalapi
                metallb.universe.tf/allow-shared-ip: internalapi
                metallb.universe.tf/loadBalancerIPs: 172.17.0.80
            spec:
              type: LoadBalancer
'</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Verification</div>
<ul>
<li>
<p>Check that the Placement service endpoints are defined and pointing to the
control plane FQDNs, and that the Placement API responds:</p>
<div class="listingblock">
<div class="content">
<pre>$ alias openstack="oc exec -t openstackclient -- openstack"

$ openstack endpoint list | grep placement


# Without OpenStack CLI placement plugin installed:
PLACEMENT_PUBLIC_URL=$(openstack endpoint list -c 'Service Name' -c 'Service Type' -c URL | grep placement | grep public | awk '{ print $6; }')
oc exec -t openstackclient -- curl "$PLACEMENT_PUBLIC_URL"

# With OpenStack CLI placement plugin installed:
openstack resource class list</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="adopting-the-compute-service_image-service">Adopting the Compute service</h3>
<div class="paragraph">
<p>To adopt the Compute service (nova), you patch an existing <code>OpenStackControlPlane</code> custom resource (CR) where the Compute service is disabled. The patch starts the service with the configuration parameters that are provided by the Red&#160;Hat OpenStack Platform (RHOSP) environment. The following procedure describes a single-cell setup.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You have completed the previous adoption steps.</p>
</li>
<li>
<p>You have defined the following shell variables. Replace the following example values with the values that are correct for your environment:</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre>$ alias openstack="oc exec -t openstackclient -- openstack"</pre>
</div>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to deploy the Compute service:</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
This procedure assumes that Compute service metadata is deployed on the top level and not on each cell level. If the RHOSP deployment has a per-cell metadata deployment, adjust the following patch as needed. You cannot run the metadata service in <code>cell0</code>.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">$ oc patch openstackcontrolplane openstack -n openstack --type=merge --patch '
spec:
  nova:
    enabled: true
    apiOverride:
      route: {}
    template:
      secret: osp-secret
      apiServiceTemplate:
        override:
          service:
            internal:
              metadata:
                annotations:
                  metallb.universe.tf/address-pool: internalapi
                  metallb.universe.tf/allow-shared-ip: internalapi
                  metallb.universe.tf/loadBalancerIPs: 172.17.0.80
              spec:
                type: LoadBalancer
        customServiceConfig: |
          [workarounds]
          disable_compute_service_check_for_ffu=true
      metadataServiceTemplate:
        enabled: true # deploy single nova metadata on the top level
        override:
          service:
            metadata:
              annotations:
                metallb.universe.tf/address-pool: internalapi
                metallb.universe.tf/allow-shared-ip: internalapi
                metallb.universe.tf/loadBalancerIPs: 172.17.0.80
            spec:
              type: LoadBalancer
        customServiceConfig: |
          [workarounds]
          disable_compute_service_check_for_ffu=true
      schedulerServiceTemplate:
        customServiceConfig: |
          [workarounds]
          disable_compute_service_check_for_ffu=true
      cellTemplates:
        cell0:
          conductorServiceTemplate:
            customServiceConfig: |
              [workarounds]
              disable_compute_service_check_for_ffu=true
        cell1:
          metadataServiceTemplate:
            enabled: false # enable here to run it in a cell instead
            override:
                service:
                  metadata:
                    annotations:
                      metallb.universe.tf/address-pool: internalapi
                      metallb.universe.tf/allow-shared-ip: internalapi
                      metallb.universe.tf/loadBalancerIPs: 172.17.0.80
                  spec:
                    type: LoadBalancer
            customServiceConfig: |
              [workarounds]
              disable_compute_service_check_for_ffu=true
          conductorServiceTemplate:
            customServiceConfig: |
              [workarounds]
              disable_compute_service_check_for_ffu=true
'</code></pre>
</div>
</div>
</li>
<li>
<p>If you are adopting the Compute service with the Bare Metal Provisioning service (ironic), append the following <code>novaComputeTemplates</code> in the <code>cell1</code> section of the Compute service CR patch:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">        cell1:
          novaComputeTemplates:
            standalone:
              customServiceConfig: |
                [DEFAULT]
                host = &lt;hostname&gt;
                [workarounds]
                disable_compute_service_check_for_ffu=true</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace &lt;hostname&gt; with the hostname of the node that is running the <code>ironic</code> Compute driver in the source cloud.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Wait for the CRs for the Compute control plane services to be ready:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait --for condition=Ready --timeout=300s Nova/nova</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The local Conductor services are started for each cell, while the superconductor runs in <code>cell0</code>.
Note that <code>disable_compute_service_check_for_ffu</code> is mandatory for all imported Compute services until the external data plane is imported, and until Compute services are fast-forward upgraded. For more information, see <a href="#adopting-compute-services-to-the-data-plane_data-plane">Adopting Compute services to the RHOSO data plane</a> and <a href="#performing-a-fast-forward-upgrade-on-compute-services_data-plane">Performing a fast-forward upgrade on Compute services</a>.
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Verification</div>
<ul>
<li>
<p>Check that Compute service endpoints are defined and pointing to the
control plane FQDNs, and that the Nova API responds:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack endpoint list | grep nova
$ openstack server list</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Compare the outputs with the topology-specific configuration in <a href="#proc_retrieving-topology-specific-service-configuration_migrating-databases">Retrieving topology-specific service configuration</a>.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Query the superconductor to check that <code>cell1</code> exists, and compare it to pre-adoption values:</p>
<div class="listingblock">
<div class="content">
<pre>. ~/.source_cloud_exported_variables
echo $PULL_OPENSTACK_CONFIGURATION_NOVAMANAGE_CELL_MAPPINGS
oc rsh nova-cell0-conductor-0 nova-manage cell_v2 list_cells | grep -F '| cell1 |'</pre>
</div>
</div>
<div class="paragraph">
<p>The following changes are expected:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The <code>cell1</code> <code>nova</code> database and username become <code>nova_cell1</code>.</p>
</li>
<li>
<p>The default cell is renamed to <code>cell1</code>.</p>
</li>
<li>
<p>RabbitMQ transport URL no longer uses <code>guest</code>.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
At this point, the Compute service control plane services do not control the existing Compute service workloads. The control plane manages the data plane only after the data adoption process is completed. For more information, see <a href="#adopting-compute-services-to-the-data-plane_data-plane">Adopting Compute services to the RHOSO data plane</a>.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="adopting-the-block-storage-service_image-service">Adopting the Block Storage service</h3>
<div class="paragraph">
<p>Adopting a director-deployed Block Storage service (cinder) service into Red&#160;Hat OpenStack Platform usually entails:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Checking existing limitations.</p>
</li>
<li>
<p>Considering the placement of the Block Storage service services.</p>
</li>
<li>
<p>Preparing the Red Hat OpenShift Container Platform nodes where volume and backup services will run.</p>
</li>
<li>
<p>Crafting the manifest based on the existing <code>cinder.conf</code> file.</p>
</li>
<li>
<p>Deploying Block Storage service.</p>
</li>
<li>
<p>Validating the new deployment.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This guide provides necessary knowledge to complete these steps in most
situations, but it still requires knowledge on how RHOSP services work and the structure of a Block Storage service configuration file.</p>
</div>
<div class="sect3">
<h4 id="block-storage-limitations_adopting-block-storage">Limitations for adopting the Block Storage service</h4>
<div class="paragraph">
<p>There are currently limitations that are worth highlighting; some are
related to this guideline while some to the operator:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>There is no global <code>nodeSelector</code> for all Block Storage service (cinder) volumes, so it needs to be
specified per backend.</p>
</li>
<li>
<p>There is no global <code>customServiceConfig</code> or <code>customServiceConfigSecrets</code> for
all Block Storage service volumes, so it needs to be specified per backend.</p>
</li>
<li>
<p>Adoption of LVM backends, where the volume data is stored in the compute
nodes, is not currently being documented in this process.</p>
</li>
<li>
<p>Support for Block Storage service backends that require kernel modules not included in RHEL
has not been tested in Operator deployed Red&#160;Hat OpenStack Platform.</p>
</li>
<li>
<p>Adoption of DCN/Edge deployment is not currently described in this guide.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="openshift-preparation-for-block-storage-adoption_adopting-block-storage">Red Hat OpenShift Container Platform preparation for Block Storage service adoption</h4>
<div class="paragraph">
<p>Before deploying Red&#160;Hat OpenStack Platform (RHOSP) in Red Hat OpenShift Container Platform, you must ensure that the networks are ready, that you have decided the node selection, and also make sure any necessary changes to the RHOCP nodes have been made. For Block Storage service (cinder) volume and backup services all these 3 must be carefully considered.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Node Selection</dt>
<dd>
<p>You might need, or want, to restrict the RHOCP nodes where Block Storage service volume and
backup services can run.</p>
<div class="paragraph">
<p>The best example of when you need to do node selection for a specific Block Storage service is when you deploy the Block Storage service with the LVM driver. In that scenario, the
LVM data where the volumes are stored only exists in a specific host, so you
need to pin the Block Storage-volume service to that specific RHOCP node. Running
the service on any other RHOCP node would not work.  Since <code>nodeSelector</code>
only works on labels, you cannot use the RHOCP host node name to restrict
the LVM backend and you need to identify it using a unique label, an existing label, or new label:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc label nodes worker0 lvm=cinder-volumes</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: core.openstack.org/v1beta1
kind: OpenStackControlPlane
metadata:
  name: openstack
spec:
  secret: osp-secret
  storageClass: local-storage
  cinder:
    enabled: true
    template:
      cinderVolumes:
        lvm-iscsi:
          nodeSelector:
            lvm: cinder-volumes
&lt; . . . &gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>As mentioned in the <a href="#about-node-selector_planning">About node selector</a>, an example where you need to use labels is when using FC storage and you do not have HBA cards in all your RHOCP nodes. In this scenario you need to restrict all the Block Storage service volume backends (not only the FC one) as well as the backup services.</p>
</div>
<div class="paragraph">
<p>Depending on the Block Storage service backends, their configuration, and the usage of Block Storage service,
you can have network intensive Block Storage service volume services with lots of I/O as well as
Block Storage service backup services that are not only network intensive but also memory and
CPU intensive. This may be a concern for the RHOCP human operators, and
they may want to use the <code>nodeSelector</code> to prevent these service from
interfering with their other RHOCP workloads. For more information about node selection, see <a href="#about-node-selector_planning">About node selector</a>.</p>
</div>
<div class="paragraph">
<p>When selecting the nodes where the Block Storage service volume is going to run remember that Block Storage service-volume may also use local storage when downloading a Image Service (glance) image for the create volume from image operation, and it can require a considerable
amount of space when having concurrent operations and not using Block Storage service volume
cache.</p>
</div>
<div class="paragraph">
<p>If you do not have nodes with enough local disk space for the temporary images, you can use a remote NFS location for the images. You had to manually set this up in director deployments, but with operators, you can do it
automatically using the extra volumes feature ()<code>extraMounts</code>.</p>
</div>
</dd>
<dt class="hdlist1">Transport protocols</dt>
<dd>
<p>Due to the specifics of the storage transport protocols some changes may be
required on the RHOCP side, and although this is something that must be
documented by the Vendor here wer are going to provide some generic
instructions that can serve as a guide for the different transport protocols.</p>
<div class="paragraph">
<p>Check the backend sections in your <code>cinder.conf</code> file that are listed in the
<code>enabled_backends</code> configuration option to figure out the transport storage
protocol used by the backend.</p>
</div>
<div class="paragraph">
<p>Depending on the backend, you can find the transport protocol:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Looking at the <code>volume_driver</code> configuration option, as it may contain the
protocol itself: RBD, iSCSI, FC&#8230;&#8203;</p>
</li>
<li>
<p>Looking at the <code>target_protocol</code> configuration option</p>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
Any time a <code>MachineConfig</code> is used to make changes to RHOCP
nodes the node will reboot!!  Act accordingly.
</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1">NFS</dt>
<dd>
<p>There is nothing to do for NFS. RHOCP can connect to NFS backends without
any additional changes.</p>
</dd>
<dt class="hdlist1">RBD/Ceph</dt>
<dd>
<p>There is nothing to do for RBD/Ceph in terms of preparing the nodes, RHOCP
can connect to Ceph backends without any additional changes. Credentials and
configuration files will need to be provided to the services though.</p>
</dd>
<dt class="hdlist1">iSCSI</dt>
<dd>
<p>Connecting to iSCSI volumes requires that the iSCSI initiator is running on the
RHOCP hosts where volume and backup services are going to run, because
the Linux Open iSCSI initiator does not currently support network namespaces, so
you must only run 1 instance of the service for the normal RHOCP usage, plus
the RHOCP CSI plugins, plus the RHOSP services.</p>
<div class="paragraph">
<p>If you are not already running <code>iscsid</code> on the RHOCP nodes, then you need
to apply a <code>MachineConfig</code> similar to this one:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
    service: cinder
  name: 99-master-cinder-enable-iscsid
spec:
  config:
    ignition:
      version: 3.2.0
    systemd:
      units:
      - enabled: true
        name: iscsid.service</code></pre>
</div>
</div>
<div class="paragraph">
<p>If you are using labels to restrict the nodes where the Block Storage services are running you need to use a <code>MachineConfigPool</code> as described in
the <a href="#about-node-selector_planning">About node selector</a> to limit the effects of the
<code>MachineConfig</code> to only the nodes where your services may run.</p>
</div>
<div class="paragraph">
<p>If you are using a single node deployment to test the process, replace <code>worker</code> with <code>master</code> in the <code>MachineConfig</code>.</p>
</div>
</dd>
</dl>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">FC</dt>
<dd>
<p>There is nothing to do for FC volumes to work, but the Block Storage service volume and Block Storage service backup services need to run in an RHOCP host that has HBAs, so if there
are nodes that do not have HBAs then you need to use labels to restrict where
these services can run, as mentioned in <a href="#about-node-selector_planning">About node selector</a>.</p>
<div class="paragraph">
<p>This also means that for virtualized RHOCP clusters using FC you need to
expose the host&#8217;s HBAs inside the VM.</p>
</div>
</dd>
</dl>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">NVMe-oF</dt>
<dd>
<p>Connecting to NVMe-oF volumes requires that the nvme kernel modules are loaded
on the RHOCP hosts.</p>
<div class="paragraph">
<p>If you are not already loading the <code>nvme-fabrics</code> module on the RHOCP nodes
where volume and backup services are going to run then you need to apply a
<code>MachineConfig</code> similar to this one:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
    service: cinder
  name: 99-master-cinder-load-nvme-fabrics
spec:
  config:
    ignition:
      version: 3.2.0
    storage:
      files:
        - path: /etc/modules-load.d/nvme_fabrics.conf
          overwrite: false
          # Mode must be decimal, this is 0644
          mode: 420
          user:
            name: root
          group:
            name: root
          contents:
            # Source can be a http, https, tftp, s3, gs, or data as defined in rfc2397.
            # This is the rfc2397 text/plain string format
            source: data:,nvme-fabrics</pre>
</div>
</div>
<div class="paragraph">
<p>If you are using labels to restrict the nodes where Block Storage
services are running, you need to use a <code>MachineConfigPool</code> as described in
the <a href="#about-node-selector_planning">About node selector</a> to limit the effects of the
<code>MachineConfig</code> to only the nodes where your services may run.</p>
</div>
<div class="paragraph">
<p>If you are using a single node deployment to test the process,replace <code>worker</code> with <code>master</code> in the <code>MachineConfig</code>.</p>
</div>
<div class="paragraph">
<p>You are only loading the <code>nvme-fabrics</code> module because it takes care of loading
the transport specific modules (tcp, rdma, fc) as needed.</p>
</div>
<div class="paragraph">
<p>For production deployments using NVMe-oF volumes it is recommended that you use
multipathing. For NVMe-oF volumes RHOSP uses native multipathing, called ANA.</p>
</div>
<div class="paragraph">
<p>Once the RHOCP nodes have rebooted and are loading the <code>nvme-fabrics</code> module
you can confirm that the Operating System is configured and supports ANA by
checking on the host:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>cat /sys/module/nvme_core/parameters/multipath</pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
ANA does not use the Linux Multipathing Device Mapper, but the
current RHOSP code requires <code>multipathd</code> on Compute nodes to be running for Compute service (nova) to be able to use multipathing.
</td>
</tr>
</table>
</div>
</dd>
</dl>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Multipathing</dt>
<dd>
<p>For iSCSI and FC protocols, using multipathing is recommended, which
has 4 parts:</p>
<div class="ulist">
<ul>
<li>
<p>Prepare the RHOCP hosts</p>
</li>
<li>
<p>Configure the Block Storage services</p>
</li>
<li>
<p>Prepare the Compute service computes</p>
</li>
<li>
<p>Configure the Compute service service</p>
<div class="paragraph">
<p>To prepare the RHOCP hosts, you need to ensure that the Linux Multipath
Device Mapper is configured and running on the RHOCP hosts, and you do
that using <code>MachineConfig</code> like this one:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># Includes the /etc/multipathd.conf contents and the systemd unit changes
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
    service: cinder
  name: 99-master-cinder-enable-multipathd
spec:
  config:
    ignition:
      version: 3.2.0
    storage:
      files:
        - path: /etc/multipath.conf
          overwrite: false
          # Mode must be decimal, this is 0600
          mode: 384
          user:
            name: root
          group:
            name: root
          contents:
            # Source can be a http, https, tftp, s3, gs, or data as defined in rfc2397.
            # This is the rfc2397 text/plain string format
            source: data:,defaults%20%7B%0A%20%20user_friendly_names%20no%0A%20%20recheck_wwid%20yes%0A%20%20skip_kpartx%20yes%0A%20%20find_multipaths%20yes%0A%7D%0A%0Ablacklist%20%7B%0A%7D
    systemd:
      units:
      - enabled: true
        name: multipathd.service</code></pre>
</div>
</div>
<div class="paragraph">
<p>If you are using labels to restrict the nodes where Block Storage
services are running you need to use a <code>MachineConfigPool</code> as described in
the <a href="#about-node-selector_planning">About node selector</a> to limit the effects of the
<code>MachineConfig</code> to only the nodes where your services may run.</p>
</div>
<div class="paragraph">
<p>If you are using a single node deployment to test the process, replace <code>worker</code> with <code>master</code> in the <code>MachineConfig</code>.</p>
</div>
<div class="paragraph">
<p>To configure the Block Storage services to use multipathing, enable the
<code>use_multipath_for_image_xfer</code> configuration option in all the backend sections
and in the <code>[DEFAULT]</code> section for the backup service. This is the default in control plane deployments. Multipathing works as long as the service is running on the RHOCP host. Do not override this option by setting <code>use_multipath_for_image_xfer = false</code>.</p>
</div>
</li>
</ul>
</div>
</dd>
</dl>
</div>
</div>
<div class="sect3">
<h4 id="preparing-the-block-storage-service_adopting-block-storage">Preparing the Block Storage service configurations for adoption</h4>
<div class="paragraph">
<p>The Block Storage service (cinder) is configured using
configuration snippets instead of using configuration parameters
defined by the installer. For more information, see <a href="#service-configurations_planning">Service configurations</a>.</p>
</div>
<div class="paragraph">
<p>The recommended way to deploy Block Storage service volume backends has changed to remove old
limitations, add flexibility, and improve operations.</p>
</div>
<div class="paragraph">
<p>When deploying with director you used to run a single Block Storage service volume service with
all your backends (each backend would run on its own process), and even though
that way of deploying is still supported, it is not recommended. It is recommended to use a volume service per backend since it is a superior deployment model.</p>
</div>
<div class="paragraph">
<p>With an LVM and a Ceph backend you have 2 entries in <code>cinderVolume</code> and,
as mentioned in the limitations section, you cannot set global defaults for all
volume services, so you have to define it for each of them, like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: core.openstack.org/v1beta1
kind: OpenStackControlPlane
metadata:
  name: openstack
spec:
  cinder:
    enabled: true
    template:
      cinderVolume:
        lvm:
          customServiceConfig: |
            [DEFAULT]
            debug = True
            [lvm]
&lt; . . . &gt;
        ceph:
          customServiceConfig: |
            [DEFAULT]
            debug = True
            [ceph]
&lt; . . . &gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>Reminder that for volume backends that have sensitive information using <code>Secret</code>
and the <code>customServiceConfigSecrets</code> key is the recommended way to go.</p>
</div>
<div class="paragraph">
<p>For adoption instead of using a whole deployment manifest you use a targeted
patch, like you did with other services, and in this patch you will enable the
different Block Storage services with their specific configurations.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
Check that all configuration options are still valid for the new
Red&#160;Hat OpenStack Platform version. Configuration options may have been deprecated,
removed, or added. This applies to both backend driver specific configuration
options and other generic options.
</td>
</tr>
</table>
</div>
<div class="sect4">
<h5 id="preparing-block-storage-by-customizing-configuration_preparing-block-storage">Preparing the Block Storage service configuration</h5>
<div class="paragraph">
<p>Creating the Cinder configuration entails:</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Determine what part of the configuration is generic for all the Block Storage service (cinder)
services and remove anything that would change when deployed in Red Hat OpenShift Container Platform, like
the <code>connection</code> in the <code>[dabase]</code> section, the <code>transport_url</code> and <code>log_dir</code> in
<code>[DEFAULT]</code>, the whole <code>[coordination]</code> and <code>[barbican]</code> sections.  This
configuration goes into the <code>customServiceConfig</code> (or a <code>Secret</code> and then used
in <code>customServiceConfigSecrets</code>) at the <code>cinder: template:</code> level.</p>
</li>
<li>
<p>Determine if there&#8217;s any scheduler specific configuration and add it to the
<code>customServiceConfig</code> section in <code>cinder: template: cinderScheduler</code>.</p>
</li>
<li>
<p>Determine if there&#8217;s any API specific configuration and add it to the
<code>customServiceConfig</code> section in <code>cinder: template: cinderAPI</code>.</p>
</li>
<li>
<p>If you have Block Storage service backup deployed, then you get the Block Storage service backup relevant
configuration options and add them to <code>customServiceConfig</code> (or a <code>Secret</code> and
then used in <code>customServiceConfigSecrets</code>) at the <code>cinder: template:
cinderBackup:</code> level. You should remove the <code>host</code> configuration in the
<code>[DEFAULT]</code> section to facilitate supporting multiple replicas in the future.</p>
</li>
<li>
<p>Determine the individual volume backend configuration for each of the
drivers. The configuration will not only be the specific driver section, it
should also include the <code>[backend_defaults]</code> section and FC zoning sections is
they are being used, because the Block Storage service operator doesn&#8217;t support a
<code>customServiceConfig</code> section global for all volume services.  Each backend
would have its own section under <code>cinder: template: cinderVolumes</code> and the
configuration would go in <code>customServiceConfig</code> (or a <code>Secret</code> and then used in
<code>customServiceConfigSecrets</code>).</p>
</li>
<li>
<p>Check if any of the Block Storage service volume drivers being used requires a custom vendor
image. If they do, find the location of the image in the vendor&#8217;s instruction
available in the <a href="https://catalog.redhat.com/software/search?target_platforms=Red%20Hat%20OpenStack%20Platform&amp;p=1&amp;functionalCategories=Data%20storage">Red&#160;Hat OpenStack Platform Block Storage service ecosystem
page</a>
and create or modify an <code>OpenStackVersion</code> manifest to specify the custom
image using the key from the <code>cinderVolumes</code> section.</p>
<div class="paragraph">
<p>For example, if we had this configuration:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  cinder:
    enabled: true
    template:
      cinderVolume:
        pure:
          customServiceConfigSecrets:
            - openstack-cinder-pure-cfg
&lt; . . . &gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>Then the <code>OpenStackVersion</code> describing the container image for that backend
would look like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: core.openstack.org/v1beta1
kind: OpenStackVersion
metadata:
  name: openstack
spec:
  customContainerImages:
    cinderVolumeImages:
      pure: registry.connect.redhat.com/purestorage/openstack-cinder-volume-pure-rhosp-18-0'</code></pre>
</div>
</div>
<div class="paragraph">
<p>The name of the <code>OpenStackVersion</code> must match the name of your <code>OpenStackControlPlane</code>, so in your case it may be other than <code>openstack</code>.</p>
</div>
</li>
<li>
<p>External files: Block Storage services sometimes use external files, for example for
a custom policy, or to store credentials, or SSL CA bundles to connect to a
storage array, and you need to make those files available to the right
containers. To achieve this, you use <code>Secrets</code> or <code>ConfigMap</code> to store the
information in RHOCP and then the <code>extraMounts</code> key. For example, for the
Ceph credentials stored in a <code>Secret</code> called <code>ceph-conf-files</code> you patch
the top level <code>extraMounts</code> in <code>OpenstackControlPlane</code>:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  extraMounts:
  - extraVol:
    - extraVolType: Ceph
      mounts:
      - mountPath: /etc/ceph
        name: ceph
        readOnly: true
      propagation:
      - CinderVolume
      - CinderBackup
      - Glance
      volumes:
      - name: ceph
        projected:
          sources:
          - secret:
              name: ceph-conf-files</code></pre>
</div>
</div>
<div class="paragraph">
<p>But for a service specific one, like the API policy, you do it directly
on the service itself. In this example, you include the Block Storage API
configuration that references the policy you are adding from a <code>ConfigMap</code>
called <code>my-cinder-conf</code> that has a key <code>policy</code> with the contents of the
policy:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  cinder:
    enabled: true
    template:
      cinderAPI:
        customServiceConfig: |
           [oslo_policy]
           policy_file=/etc/cinder/api/policy.yaml
      extraMounts:
      - extraVol:
        - extraVolType: Ceph
          mounts:
          - mountPath: /etc/cinder/api
            name: policy
            readOnly: true
          propagation:
          - CinderAPI
          volumes:
          - name: policy
            projected:
              sources:
              - configMap:
                  name: my-cinder-conf
                  items:
                    - key: policy
                      path: policy.yaml</code></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect3">
<h4 id="deploying-the-block-storage-services_preparing-block-storage">Deploying the Block Storage services</h4>
<div class="paragraph">
<p>Assuming you have already stopped Block Storage service (cinder) services, prepared the Red Hat OpenShift Container Platform nodes,
deployed the Red&#160;Hat OpenStack Platform (RHOSP) operators and a bare RHOSP manifest, and migrated the
database, and prepared the patch manifest with the Block Storage service configuration,
you must apply the patch and wait for the operator to apply the changes and deploy the Block Storage services.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Previous Adoption steps completed. Notably, Block Storage service must have been
stopped and the service databases must already be imported into the control plane MariaDB.</p>
</li>
<li>
<p>Identity service (keystone) and Key Manager service (barbican) should be already adopted.</p>
</li>
<li>
<p>Storage network has been properly configured on the RHOCP cluster.</p>
</li>
<li>
<p>You need the contents of <code>cinder.conf</code> file. Download the file so that you can access it locally:</p>
<div class="listingblock">
<div class="content">
<pre>$CONTROLLER1_SSH cat /var/lib/config-data/puppet-generated/cinder/etc/cinder/cinder.conf &gt; cinder.conf</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>It is recommended to write the patch manifest into a file, for example
<code>cinder.patch</code> and then apply it with something like:</p>
<div class="listingblock">
<div class="content">
<pre>oc patch openstackcontrolplane openstack --type=merge --patch-file=cinder.patch</pre>
</div>
</div>
<div class="paragraph">
<p>For example, for the RBD deployment from the Development Guide the
<code>cinder.patch</code> would look like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  extraMounts:
  - extraVol:
    - extraVolType: Ceph
      mounts:
      - mountPath: /etc/ceph
        name: ceph
        readOnly: true
      propagation:
      - CinderVolume
      - CinderBackup
      - Glance
      volumes:
      - name: ceph
        projected:
          sources:
          - secret:
              name: ceph-conf-files
  cinder:
    enabled: true
    apiOverride:
      route: {}
    template:
      databaseInstance: openstack
      databaseAccount: cinder
      secret: osp-secret
      cinderAPI:
        override:
          service:
            internal:
              metadata:
                annotations:
                  metallb.universe.tf/address-pool: internalapi
                  metallb.universe.tf/allow-shared-ip: internalapi
                  metallb.universe.tf/loadBalancerIPs: 172.17.0.80
              spec:
                type: LoadBalancer
        replicas: 1
        customServiceConfig: |
          [DEFAULT]
          default_volume_type=tripleo
      cinderScheduler:
        replicas: 1
      cinderBackup:
        networkAttachments:
        - storage
        replicas: 1
        customServiceConfig: |
          [DEFAULT]
          backup_driver=cinder.backup.drivers.ceph.CephBackupDriver
          backup_ceph_conf=/etc/ceph/ceph.conf
          backup_ceph_user=openstack
          backup_ceph_pool=backups
      cinderVolumes:
        ceph:
          networkAttachments:
          - storage
          replicas: 1
          customServiceConfig: |
            [tripleo_ceph]
            backend_host=hostgroup
            volume_backend_name=tripleo_ceph
            volume_driver=cinder.volume.drivers.rbd.RBDDriver
            rbd_ceph_conf=/etc/ceph/ceph.conf
            rbd_user=openstack
            rbd_pool=volumes
            rbd_flatten_volume_from_snapshot=False
            report_discard_supported=True</code></pre>
</div>
</div>
</li>
<li>
<p>Once the services have been deployed you need to clean up the old scheduler
and backup services which will appear as being down while you have others that appear as being up:</p>
<div class="listingblock">
<div class="content">
<pre>openstack volume service list

+------------------+------------------------+------+---------+-------+----------------------------+
| Binary           | Host                   | Zone | Status  | State | Updated At                 |
+------------------+------------------------+------+---------+-------+----------------------------+
| cinder-backup    | standalone.localdomain | nova | enabled | down  | 2023-06-28T11:00:59.000000 |
| cinder-scheduler | standalone.localdomain | nova | enabled | down  | 2023-06-28T11:00:29.000000 |
| cinder-volume    | hostgroup@tripleo_ceph | nova | enabled | up    | 2023-06-28T17:00:03.000000 |
| cinder-scheduler | cinder-scheduler-0     | nova | enabled | up    | 2023-06-28T17:00:02.000000 |
| cinder-backup    | cinder-backup-0        | nova | enabled | up    | 2023-06-28T17:00:01.000000 |
+------------------+------------------------+------+---------+-------+----------------------------+</pre>
</div>
</div>
</li>
<li>
<p>In this case you need to remove services for hosts <code>standalone.localdomain</code></p>
<div class="listingblock">
<div class="content">
<pre>oc exec -it cinder-scheduler-0 -- cinder-manage service remove cinder-backup standalone.localdomain
oc exec -it cinder-scheduler-0 -- cinder-manage service remove cinder-scheduler standalone.localdomain</pre>
</div>
</div>
<div class="paragraph">
<p>The reason why we haven&#8217;t preserved the name of the backup service is because
we have taken the opportunity to change its configuration to support
Active-Active, even though we are not doing so right now because we have 1
replica.</p>
</div>
</li>
<li>
<p>Now that the Block Storage services are running, the DB schema migration has been completed and you can proceed to apply the DB data migrations.
While it is not necessary to run these data migrations at this precise moment,
because you can run them right before the next upgrade, for adoption it is best to run them now to make sure there are no issues before running production workloads on the deployment.</p>
<div class="paragraph">
<p>The command to run the DB data migrations is:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>oc exec -it cinder-scheduler-0 -- cinder-manage db online_data_migrations</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<div class="title">Verification</div>
<p>Before you can run any checks you need to set the right cloud configuration for
the <code>openstack</code> command to be able to connect to your RHOCP control plane.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Ensure that the <code>openstack</code> alias is defined:</p>
<div class="listingblock">
<div class="content">
<pre>alias openstack="oc exec -t openstackclient -- openstack"</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>Now you can run a set of tests to confirm that the deployment is using your
old database contents:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>See that Block Storage service endpoints are defined and pointing to the control plane FQDNs:</p>
<div class="listingblock">
<div class="content">
<pre>openstack endpoint list --service cinderv3</pre>
</div>
</div>
</li>
<li>
<p>Check that the Block Storage services are running and up. The API won&#8217;t show but if
you get a response you know it&#8217;s up as well:</p>
<div class="listingblock">
<div class="content">
<pre>openstack volume service list</pre>
</div>
</div>
</li>
<li>
<p>Check that your old volume types, volumes, snapshots, and backups are there:</p>
<div class="listingblock">
<div class="content">
<pre>openstack volume type list
openstack volume list
openstack volume snapshot list
openstack volume backup list</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>To confirm that the configuration is working, the following basic operations are recommended:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create a volume from an image to check that the connection to Image Service (glance) is working.</p>
<div class="listingblock">
<div class="content">
<pre>openstack volume create --image cirros --bootable --size 1 disk_new</pre>
</div>
</div>
</li>
<li>
<p>Backup the old attached volume to a new backup. Example:</p>
<div class="listingblock">
<div class="content">
<pre>openstack --os-volume-api-version 3.47 volume create --backup backup restored</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
You do not boot a Compute service (nova) instance using the new volume from image or try to detach the old volume because Compute service and the Block Storage service are still not connected.
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect2">
<h3 id="adopting-the-openstack-dashboard_preparing-block-storage">Adopting the Dashboard service</h3>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Previous Adoption steps completed. Notably, Memcached and
Identity service (keystone) should be already adopted.</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Procedure</div>
<ul>
<li>
<p>Patch <code>OpenStackControlPlane</code> to deploy the Dashboard service:</p>
<div class="listingblock">
<div class="content">
<pre>oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  horizon:
    enabled: true
    apiOverride:
      route: {}
    template:
      memcachedInstance: memcached
      secret: osp-secret
'</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>See that the Dashboard service instance is successfully deployed and ready</p>
<div class="listingblock">
<div class="content">
<pre>oc get horizon</pre>
</div>
</div>
</li>
<li>
<p>Check that the Dashboard service is reachable and returns status code <code>200</code></p>
<div class="listingblock">
<div class="content">
<pre>PUBLIC_URL=$(oc get horizon horizon -o jsonpath='{.status.endpoint}')
curl --silent --output /dev/stderr --head --write-out "%{http_code}" "$PUBLIC_URL/dashboard/auth/login/?next=/dashboard/" -k | grep 200</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="adopting-the-shared-file-systems-service_preparing-block-storage">Adopting the Shared File Systems service</h3>
<div class="paragraph">
<p>The Shared File Systems service (manila) provides Red&#160;Hat OpenStack Platform (RHOSP)
users with a self-service API to create and manage file shares. File
shares (or simply, "shares"), are built for concurrent read/write access by
any number of clients. This, coupled with the inherent elasticity of the
underlying storage makes the Shared File Systems service essential in
cloud environments with require RWX ("read write many") persistent storage.</p>
</div>
<div class="paragraph">
<p>File shares in RHOSP are accessed directly over a network. Hence, it is essential to plan the networking of the cloud to create a successful and sustainable orchestration layer for shared file systems.</p>
</div>
<div class="paragraph">
<p>The Shared File Systems service supports two levels of storage networking abstractions - one where users can directly control the networking for their respective file shares; and another where the storage networking is configured by the RHOSP administrator. It is important to ensure that the networking in the RHOSP 17.1 environment matches the network plans for your new cloud after adoption. This ensures that tenant workloads remain connected to storage through the adoption process, even as the control plane suffers a minor interruption. The Shared File Systems service control plane services are not in the data path; and shutting down the API, scheduler and share manager services will not impact access to existing shared file systems.</p>
</div>
<div class="paragraph">
<p>Typically, storage and storage device management networks are separate.
Shared File Systems services only need access to the storage device management network.
For example, if a Ceph cluster was used in the deployment, the "storage"
network refers to the Ceph cluster&#8217;s public network, and the Shared File Systems service&#8217;s share manager service needs to be able to reach it.</p>
</div>
<div class="sect3">
<h4 id="changes-to-cephFS-through-NFS_adopting-shared-file-systems">Changes to CephFS through NFS</h4>
<div class="paragraph">
<p>If the Red&#160;Hat OpenStack Platform (RHOSP) 17.1 deployment uses CephFS through NFS as a backend for Shared File Systems service (manila), there&#8217;s a <code>ceph-nfs</code> service on the RHOSP controller nodes deployed and managed by director. This service cannot be directly imported into Red&#160;Hat OpenStack Services on OpenShift (RHOSO) 18.0. On RHOSO 18.0, the Shared File Systems service only supports using a "clustered" NFS service that is directly managed on the Red Hat Ceph Storage cluster. So, adoption with this service will involve a data path disruption to existing NFS clients. The timing of this disruption can be controlled by the deployer independent of this adoption procedure.</p>
</div>
<div class="paragraph">
<p>On RHOSP 17.1, pacemaker controls the high availability of the <code>ceph-nfs</code> service. This service is assigned a Virtual IP (VIP) address that is also managed by pacemaker. The VIP is typically created on an isolated <code>StorageNFS</code> network. There are ordering and collocation constraints established between this VIP, <code>ceph-nfs</code> and the Shared File Systems service&#8217;s share manager service on the
controller nodes. Prior to adopting Shared File Systems service, pacemaker&#8217;s ordering and collocation constraints must be adjusted to separate the share manager service. This establishes <code>ceph-nfs</code> with its VIP as an isolated, standalone NFS service that can be decommissioned at will after completing the RHOSP adoption.</p>
</div>
<div class="paragraph">
<p>Red Hat Ceph Storage 7.0 introduced a native <code>clustered Ceph NFS service</code>. This service has to be deployed on the Red Hat Ceph Storage cluster using the Ceph orchestrator prior to adopting the Shared File Systems service. This NFS service will eventually replace the standalone NFS service from RHOSP 17.1 in your deployment. When the Shared File Systems service is adopted into the RHOSO 18.0 environment, it will establish all the existing
exports and client restrictions on the new clustered Ceph NFS service. Clients can continue to read and write data on their existing NFS shares, and are not affected until the old standalone NFS service is decommissioned. This switchover window allows clients to re-mount the same share from the new
clustered Ceph NFS service during a scheduled downtime.</p>
</div>
<div class="paragraph">
<p>In order to ensure that existing clients can easily switchover to the new NFS
service, it is necessary that the clustered Ceph NFS service is assigned an
IP address from the same isolated <code>StorageNFS</code> network. Doing this will ensure that NFS users aren&#8217;t expected to make any networking changes to their
existing workloads. These users only need to discover and re-mount their shares using new export paths. When the adoption procedure is complete, RHOSP users can query the Shared File Systems service API to list the export locations on existing shares to identify the <code>preferred</code> paths to mount these shares. These <code>preferred</code> paths
will correspond to the new clustered Ceph NFS service in contrast to other
non-preferred export paths that continue to be displayed until the old
isolated, standalone NFS service is decommissioned.</p>
</div>
<div class="paragraph">
<p>See <a href="#creating-a-ceph-nfs-cluster_migrating-databases">Creating a NFS Ganesha cluster</a> for instructions on setting up a clustered NFS service.</p>
</div>
</div>
<div class="sect3">
<h4 id="deploying-file-systems-service-control-plane_adopting-shared-file-systems">Deploying the Shared File Systems service control plane</h4>
<div class="paragraph">
<p>Copy the Shared File Systems service (manila) configuration from the Red&#160;Hat OpenStack Platform 17.1 deployment, and then deploy the Shared File Systems service on the control plane.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Ensure that Shared File Systems service systemd services (<code>api</code>, <code>cron</code>, <code>scheduler</code>) are
stopped. For more information, see <a href="#stopping-openstack-services_migrating-databases">Stopping Red&#160;Hat OpenStack Platform services</a>.</p>
</li>
<li>
<p>If the deployment uses CephFS through NFS as a storage backend, ensure that
pacemaker ordering and collocation constraints are adjusted. For more
information, see <a href="#stopping-openstack-services_migrating-databases">Stopping Red&#160;Hat OpenStack Platform services</a>.</p>
</li>
<li>
<p>Ensure that the Shared File Systems service pacemaker service (<code>openstack-manila-share</code>) is
stopped. For more information, see <a href="#stopping-openstack-services_migrating-databases">Stopping Red&#160;Hat OpenStack Platform services</a>.</p>
</li>
<li>
<p>Ensure that the database migration has completed. For more information, see <a href="#migrating-databases-to-mariadb-instances_migrating-databases">Migrating databases to MariaDB instances</a>.</p>
</li>
<li>
<p>Ensure that Red Hat OpenShift Container Platform nodes where <code>manila-share</code> service will be deployed
can reach the management network that the storage system is in.</p>
</li>
<li>
<p>If the deployment uses CephFS through NFS as a storage backend, ensure that
a new clustered Ceph NFS service is deployed on the Ceph cluster with the help
of Ceph orchestrator. For more information, see
<a href="#creating-a-ceph-nfs-cluster_migrating-databases">Creating a Ceph NFS cluster</a>.</p>
</li>
<li>
<p>Ensure that services such as Identity service (keystone) and memcached are available prior to
adopting the Shared File Systems services.</p>
</li>
<li>
<p>If tenant-driven networking was enabled (<code>driver_handles_share_servers=True</code>),
ensure that Networking service (neutron) has been deployed prior to adopting Shared File Systems services.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Define the <code>CONTROLLER1_SSH</code> environment variable, if it hasn&#8217;t been
defined already. Then copy the configuration file from RHOSP 17.1 for reference.</p>
<div class="listingblock">
<div class="content">
<pre>$CONTROLLER1_SSH cat /var/lib/config-data/puppet-generated/manila/etc/manila/manila.conf | awk '!/^ *#/ &amp;&amp; NF' &gt; ~/manila.conf</pre>
</div>
</div>
</li>
<li>
<p>Review this configuration alongside any configuration changes that were noted since RHOSP 17.1. Not all of it makes sense to bring into the new cloud environment:</p>
<div class="ulist">
<ul>
<li>
<p>The Shared File Systems service operator is capable of setting up database related configuration
(<code>[database]</code>), service authentication (<code>auth_strategy</code>,
<code>[keystone_authtoken]</code>), message bus configuration
(<code>transport_url</code>, <code>control_exchange</code>), the default paste config
(<code>api_paste_config</code>) and inter-service communication configuration (
<code>[neutron]</code>, <code>[nova]</code>, <code>[cinder]</code>, <code>[glance]</code> <code>[oslo_messaging_*]</code>). So
all of these can be ignored.</p>
</li>
<li>
<p>Ignore the <code>osapi_share_listen</code> configuration. In Red&#160;Hat OpenStack Services on OpenShift (RHOSO) 18.0, you rely on
Red Hat OpenShift Container Platform routes and ingress.</p>
</li>
<li>
<p>Pay attention to policy overrides. In RHOSO 18.0, the Shared File Systems service ships with a secure
default RBAC, and overrides may not be necessary.
If a custom policy is necessary, you must provide it as a
<code>ConfigMap</code>. The following sample spec illustrates how a
<code>ConfigMap</code> called <code>manila-policy</code> can be set up with the contents of a
file called <code>policy.yaml</code>.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">  spec:
    manila:
      enabled: true
      template:
        manilaAPI:
          customServiceConfig: |
             [oslo_policy]
             policy_file=/etc/manila/policy.yaml
        extraMounts:
        - extraVol:
          - extraVolType: Undefined
            mounts:
            - mountPath: /etc/manila/
              name: policy
              readOnly: true
            propagation:
            - ManilaAPI
            volumes:
            - name: policy
              projected:
                sources:
                - configMap:
                    name: manila-policy
                    items:
                      - key: policy
                        path: policy.yaml</code></pre>
</div>
</div>
</li>
<li>
<p>You must preserve the value of the <code>host</code> option under the <code>[DEFAULT]</code>
section as <code>hostgroup</code>.</p>
</li>
<li>
<p>The Shared File Systems service API service needs the <code>enabled_share_protocols</code> option to be
added in the <code>customServiceConfig</code> section in <code>manila: template: manilaAPI</code>.</p>
</li>
<li>
<p>If you had scheduler overrides, add them to the <code>customServiceConfig</code>
section in <code>manila: template: manilaScheduler</code>.</p>
</li>
<li>
<p>If you had multiple storage backend drivers configured with RHOSP 17.1,
you will need to split them up when deploying RHOSO 18.0. Each storage
backend driver needs to use its own instance of the <code>manila-share</code>
service.</p>
</li>
<li>
<p>If a storage backend driver needs a custom container image, find it on the
<a href="https://catalog.redhat.com/software/containers/search?gs&amp;q=manila">RHOSP Ecosystem Catalog</a>
and create or modify an <code>OpenStackVersion</code> manifest to specify the custom image
using the same <code>&lt;custom name&gt;</code>.</p>
<div class="paragraph">
<p>In the following example that illustrates multiple storage backend drivers,
where only one is using a custom container image, we have the manila spec from
<code>OpenStackControlPlane</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">  spec:
    manila:
      enabled: true
      template:
        manilaAPI:
          customServiceConfig: |
            [DEFAULT]
            enabled_share_protocols = nfs
          replicas: 3
        manilaScheduler:
          replicas: 3
        manilaShares:
         netapp:
           customServiceConfig: |
             [DEFAULT]
             debug = true
             enabled_share_backends = netapp
             host = hostgroup
             [netapp]
             driver_handles_share_servers = False
             share_backend_name = netapp
             share_driver = manila.share.drivers.netapp.common.NetAppDriver
             netapp_storage_family = ontap_cluster
             netapp_transport_type = http
           replicas: 1
         pure:
            customServiceConfig: |
             [DEFAULT]
             debug = true
             enabled_share_backends=pure-1
             host = hostgroup
             [pure-1]
             driver_handles_share_servers = False
             share_backend_name = pure-1
             share_driver = manila.share.drivers.purestorage.flashblade.FlashBladeShareDriver
             flashblade_mgmt_vip = 203.0.113.15
             flashblade_data_vip = 203.0.10.14
            replicas: 1</code></pre>
</div>
</div>
<div class="paragraph">
<p>And then we have the <code>OpenStackVersion</code> to define the custom container image:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: core.openstack.org/v1beta1
kind: OpenStackVersion
metadata:
  name: openstack
spec:
  customContainerImages:
    cinderVolumeImages:
      pure: registry.connect.redhat.com/purestorage/openstack-manila-share-pure-rhosp-18-0</code></pre>
</div>
</div>
<div class="paragraph">
<p>The name of the <code>OpenStackVersion</code> must match the name of your <code>OpenStackControlPlane</code>, so in your case it may be other than <code>openstack</code>.</p>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>If providing sensitive information, such as passwords, hostnames and
usernames, it is recommended to use Red Hat OpenShift Container Platform secrets, and the
<code>customServiceConfigSecrets</code> key. An example:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">cat &lt;&lt; __EOF__ &gt; ~/netapp_secrets.conf

[netapp]
netapp_server_hostname = 203.0.113.10
netapp_login = fancy_netapp_user
netapp_password = secret_netapp_password
netapp_vserver = mydatavserver
__EOF__</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>oc create secret generic osp-secret-manila-netapp --from-file=~/netapp_secrets.conf -n openstack</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><code>customConfigSecrets</code> can be used in any service, the following is a
config example using the secret you created above.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">  spec:
    manila:
      enabled: true
      template:
        &lt; . . . &gt;
        manilaShares:
         netapp:
           customServiceConfig: |
             [DEFAULT]
             debug = true
             enabled_share_backends = netapp
             host = hostgroup
             [netapp]
             driver_handles_share_servers = False
             share_backend_name = netapp
             share_driver = manila.share.drivers.netapp.common.NetAppDriver
             netapp_storage_family = ontap_cluster
             netapp_transport_type = http
           customServiceConfigSecrets:
             - osp-secret-manila-netapp
           replicas: 1
    &lt; . . . &gt;</code></pre>
</div>
</div>
</li>
<li>
<p>If you need to present extra files to any of the services, you can use
<code>extraMounts</code>. For example, when using ceph, you&#8217;d need the Shared File Systems service ceph
user&#8217;s keyring file as well as the <code>ceph.conf</code> configuration file
available. These are mounted via <code>extraMounts</code> as done with the example
below.</p>
</li>
<li>
<p>Ensure that the names of the backends (<code>share_backend_name</code>) remain as they
did on RHOSP 17.1.</p>
</li>
<li>
<p>It is recommended to set the replica count of the <code>manilaAPI</code> service and
the <code>manilaScheduler</code> service to 3. You should ensure to set the replica
count of the <code>manilaShares</code> service/s to 1.</p>
</li>
<li>
<p>Ensure that the appropriate storage management network is specified in the
<code>manilaShares</code> section. The example below connects the <code>manilaShares</code>
instance with the CephFS backend driver to the <code>storage</code> network.</p>
</li>
<li>
<p>Prior to adopting the <code>manilaShares</code> service for CephFS through NFS, ensure that
you have a clustered Ceph NFS service created. You will need to provide the
name of the service as ``cephfs_nfs_cluster_id``.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Patch <code>OpenStackControlPlane</code> to deploy the Shared File Systems service; here&#8217;s an example that uses
Native CephFS:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">cat &lt;&lt; __EOF__ &gt; ~/manila.patch
spec:
  manila:
    enabled: true
    apiOverride:
      route: {}
    template:
      databaseInstance: openstack
      databaseAccount: manila
      secret: osp-secret
      manilaAPI:
        replicas: 3
        customServiceConfig: |
          [DEFAULT]
          enabled_share_protocols = cephfs
        override:
          service:
            internal:
              metadata:
                annotations:
                  metallb.universe.tf/address-pool: internalapi
                  metallb.universe.tf/allow-shared-ip: internalapi
                  metallb.universe.tf/loadBalancerIPs: 172.17.0.80
              spec:
                type: LoadBalancer
      manilaScheduler:
        replicas: 3
      manilaShares:
        cephfs:
          replicas: 1
          customServiceConfig: |
            [DEFAULT]
            enabled_share_backends = tripleo_ceph
            host = hostgroup
            [cephfs]
            driver_handles_share_servers=False
            share_backend_name=cephfs
            share_driver=manila.share.drivers.cephfs.driver.CephFSDriver
            cephfs_conf_path=/etc/ceph/ceph.conf
            cephfs_auth_id=openstack
            cephfs_cluster_name=ceph
            cephfs_volume_mode=0755
            cephfs_protocol_helper_type=CEPHFS
          networkAttachments:
              - storage
__EOF__</code></pre>
</div>
</div>
<div class="paragraph">
<p>Below is an example that uses CephFS through NFS. In this example:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The <code>cephfs_ganesha_server_ip</code> option is preserved from the configuration on
the old RHOSP 17.1 environment.</p>
</li>
<li>
<p>The <code>cephfs_nfs_cluster_id</code> option is set with the name of the NFS cluster
created on Ceph.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">cat &lt;&lt; __EOF__ &gt; ~/manila.patch
spec:
  manila:
    enabled: true
    apiOverride:
      route: {}
    template:
      databaseInstance: openstack
      secret: osp-secret
      manilaAPI:
        replicas: 3
        customServiceConfig: |
          [DEFAULT]
          enabled_share_protocols = cephfs
        override:
          service:
            internal:
              metadata:
                annotations:
                  metallb.universe.tf/address-pool: internalapi
                  metallb.universe.tf/allow-shared-ip: internalapi
                  metallb.universe.tf/loadBalancerIPs: 172.17.0.80
              spec:
                type: LoadBalancer
      manilaScheduler:
        replicas: 3
      manilaShares:
        cephfs:
          replicas: 1
          customServiceConfig: |
            [DEFAULT]
            enabled_share_backends = cephfs
            host = hostgroup
            [cephfs]
            driver_handles_share_servers=False
            share_backend_name=tripleo_ceph
            share_driver=manila.share.drivers.cephfs.driver.CephFSDriver
            cephfs_conf_path=/etc/ceph/ceph.conf
            cephfs_auth_id=openstack
            cephfs_cluster_name=ceph
            cephfs_protocol_helper_type=NFS
            cephfs_nfs_cluster_id=cephfs
            cephfs_ganesha_server_ip=172.17.5.47
          networkAttachments:
              - storage
__EOF__</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>oc patch openstackcontrolplane openstack --type=merge --patch-file=~/manila.patch</pre>
</div>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>Inspect the resulting Shared File Systems service pods:</p>
<div class="listingblock">
<div class="content">
<pre>oc get pods -l service=manila</pre>
</div>
</div>
</li>
<li>
<p>Check that the Shared File Systems service API service is registered in Identity service (keystone):</p>
<div class="listingblock">
<div class="content">
<pre>openstack service list | grep manila</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>openstack endpoint list | grep manila

| 1164c70045d34b959e889846f9959c0e | regionOne | manila       | share        | True    | internal  | http://manila-internal.openstack.svc:8786/v1/%(project_id)s        |
| 63e89296522d4b28a9af56586641590c | regionOne | manilav2     | sharev2      | True    | public    | https://manila-public-openstack.apps-crc.testing/v2                |
| af36c57adcdf4d50b10f484b616764cc | regionOne | manila       | share        | True    | public    | https://manila-public-openstack.apps-crc.testing/v1/%(project_id)s |
| d655b4390d7544a29ce4ea356cc2b547 | regionOne | manilav2     | sharev2      | True    | internal  | http://manila-internal.openstack.svc:8786/v2                       |</pre>
</div>
</div>
</li>
<li>
<p>Test the health of the service:</p>
<div class="listingblock">
<div class="content">
<pre>openstack share service list
openstack share pool list --detail</pre>
</div>
</div>
</li>
<li>
<p>Check on existing workloads:</p>
<div class="listingblock">
<div class="content">
<pre>openstack share list
openstack share snapshot list</pre>
</div>
</div>
</li>
<li>
<p>You can create further resources:</p>
<div class="listingblock">
<div class="content">
<pre>openstack share create cephfs 10 --snapshot mysharesnap --name myshareclone
openstack share create nfs 10 --name mynfsshare
openstack share export location list mynfsshare</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="decommissioning-RHOSP-standalone-Ceph-NFS-service_adopting-shared-file-systems">Decommissioning the Red&#160;Hat OpenStack Platform standalone Ceph NFS service</h4>
<div class="paragraph">
<p>If the deployment uses CephFS through NFS, you must inform your Red&#160;Hat OpenStack Platform(RHOSP) users
that the old, standalone NFS service will be decommissioned. Users can discover
the new export locations for their pre-existing shares by querying the Shared File Systems service API.
To stop using the old NFS server, they need to unmount and remount their
shared file systems on each client. If users are consuming the Shared File Systems service shares via
the Shared File Systems service CSI plugin for Red Hat OpenShift Container Platform, this migration can be done by scaling down
the application pods and scaling them back up. Clients spawning new workloads
must be discouraged from using share exports via the old NFS service. The Shared File Systems service
will no longer communicate with the old NFS service, and so it cannot apply or
alter any export rules on the old NFS service.</p>
</div>
<div class="paragraph">
<p>Since the old NFS service will no longer be supported by future software
upgrades, it is recommended that the decommissioning period is short.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Once the old NFS service is no longer used, you can adjust the configuration
for the <code>manila-share</code> service to remove the <code>cephfs_ganesha_server_ip</code> option.
Doing this will restart the <code>manila-share</code> process and remove the export
locations that pertained to the old NFS service from all the shares.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">cat &lt;&lt; __EOF__ &gt; ~/manila.patch
spec:
  manila:
    enabled: true
    apiOverride:
      route: {}
    template:
      manilaShares:
        cephfs:
          replicas: 1
          customServiceConfig: |
            [DEFAULT]
            enabled_share_backends = cephfs
            host = hostgroup
            [cephfs]
            driver_handles_share_servers=False
            share_backend_name=cephfs
            share_driver=manila.share.drivers.cephfs.driver.CephFSDriver
            cephfs_conf_path=/etc/ceph/ceph.conf
            cephfs_auth_id=openstack
            cephfs_cluster_name=ceph
            cephfs_protocol_helper_type=NFS
            cephfs_nfs_cluster_id=cephfs
          networkAttachments:
              - storage
__EOF__</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>oc patch openstackcontrolplane openstack --type=merge --patch-file=~/manila.patch</pre>
</div>
</div>
</li>
<li>
<p>To cleanup the standalone ceph nfs service from the RHOSP control plane
nodes, you can disable and delete the pacemaker resources associated with the
service. Replace <code>&lt;VIP&gt;</code> in the following commands with the IP address assigned
to the ceph-nfs service in your environment.</p>
<div class="listingblock">
<div class="content">
<pre>sudo pcs resource disable ceph-nfs
sudo pcs resource disable ip-&lt;VIP&gt;
sudo pcs resource unmanage ceph-nfs
sudo pcs resource unmanage ip-&lt;VIP&gt;</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="adopting-the-orchestration-service_adopting-shared-file-systems">Adopting the Orchestration service</h3>
<div class="paragraph">
<p>Adopting the Orchestration service (heat) means that an existing <code>OpenStackControlPlane</code> custom resource (CR), where Orchestration service
is supposed to be disabled, should be patched to start the service with the
configuration parameters provided by the source environment.</p>
</div>
<div class="paragraph">
<p>After the adoption process has been completed, a user can expect that they
will then have CRs for <code>Heat</code>, <code>HeatAPI</code>, <code>HeatEngine</code> and <code>HeatCFNAPI</code>.
Additionally, a user should have endpoints created within Identity service (keystone) to facilitate
the above mentioned servies.</p>
</div>
<div class="paragraph">
<p>This guide also assumes that:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>A director environment (the source Cloud) is running on one side;</p>
</li>
<li>
<p>A Red Hat OpenShift Container Platform environment is running on the other side.</p>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Previous Adoption steps completed. Notably, MariaDB and Identity service
should be already adopted.</p>
</li>
<li>
<p>In addition, if your existing Orchestration service stacks contain resources from other services
such as Networking service (neutron), Compute service (nova), Object Storage service (swift), etc. Those services should be adopted first before
trying to adopt Orchestration service.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Patch the <code>osp-secret</code> to update the <code>HeatAuthEncryptionKey</code> and <code>HeatPassword</code>. This needs to match what you have configured in the existing director Orchestration service configuration.
You can retrieve and verify the existing <code>auth_encryption_key</code> and <code>service</code> passwords via:</p>
<div class="listingblock">
<div class="content">
<pre>[stack@rhosp17 ~]$ grep -E 'HeatPassword|HeatAuth' ~/overcloud-deploy/overcloud/overcloud-passwords.yaml
  HeatAuthEncryptionKey: Q60Hj8PqbrDNu2dDCbyIQE2dibpQUPg2
  HeatPassword: dU2N0Vr2bdelYH7eQonAwPfI3</pre>
</div>
</div>
</li>
<li>
<p>And verifying on one of the Controllers that this is indeed the value in use:</p>
<div class="listingblock">
<div class="content">
<pre>[stack@rhosp17 ~]$ ansible -i overcloud-deploy/overcloud/config-download/overcloud/tripleo-ansible-inventory.yaml overcloud-controller-0 -m shell -a "grep auth_encryption_key /var/lib/config-data/puppet-generated/heat/etc/heat/heat.conf | grep -Ev '^#|^$'" -b
overcloud-controller-0 | CHANGED | rc=0 &gt;&gt;
auth_encryption_key=Q60Hj8PqbrDNu2dDCbyIQE2dibpQUPg2</pre>
</div>
</div>
</li>
<li>
<p>This password needs to be base64 encoded and added to the <code>osp-secret</code></p>
<div class="listingblock">
<div class="content">
<pre> echo Q60Hj8PqbrDNu2dDCbyIQE2dibpQUPg2 | base64
UTYwSGo4UHFickROdTJkRENieUlRRTJkaWJwUVVQZzIK

 oc patch secret osp-secret --type='json' -p='[{"op" : "replace" ,"path" : "/data/HeatAuthEncryptionKey" ,"value" : "UTYwSGo4UHFickROdTJkRENieUlRRTJkaWJwUVVQZzIK"}]'
secret/osp-secret patched</pre>
</div>
</div>
</li>
<li>
<p>Patch <code>OpenStackControlPlane</code> to deploy the Orchestration service:</p>
<div class="listingblock">
<div class="content">
<pre>oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  heat:
    enabled: true
    apiOverride:
      route: {}
    template:
      databaseInstance: openstack
      databaseAccount: heat
      secret: osp-secret
      memcachedInstance: memcached
      passwordSelectors:
        authEncryptionKey: HeatAuthEncryptionKey
        service: HeatPassword
'</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>Ensure all of the CRs reach the "Setup Complete" state:</p>
<div class="listingblock">
<div class="content">
<pre> oc get Heat,HeatAPI,HeatEngine,HeatCFNAPI
NAME                           STATUS   MESSAGE
heat.heat.openstack.org/heat   True     Setup complete

NAME                                  STATUS   MESSAGE
heatapi.heat.openstack.org/heat-api   True     Setup complete

NAME                                        STATUS   MESSAGE
heatengine.heat.openstack.org/heat-engine   True     Setup complete

NAME                                        STATUS   MESSAGE
heatcfnapi.heat.openstack.org/heat-cfnapi   True     Setup complete</pre>
</div>
</div>
</li>
<li>
<p>Check that the Orchestration service is registered in Identity service:</p>
<div class="listingblock">
<div class="content">
<pre> oc exec -it openstackclient -- openstack service list -c Name -c Type
+------------+----------------+
| Name       | Type           |
+------------+----------------+
| heat       | orchestration  |
| glance     | image          |
| heat-cfn   | cloudformation |
| ceilometer | Ceilometer     |
| keystone   | identity       |
| placement  | placement      |
| cinderv3   | volumev3       |
| nova       | compute        |
| neutron    | network        |
+------------+----------------+</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre> oc exec -it openstackclient -- openstack endpoint list --service=heat -f yaml
- Enabled: true
  ID: 1da7df5b25b94d1cae85e3ad736b25a5
  Interface: public
  Region: regionOne
  Service Name: heat
  Service Type: orchestration
  URL: http://heat-api-public-openstack-operators.apps.okd.bne-shift.net/v1/%(tenant_id)s
- Enabled: true
  ID: 414dd03d8e9d462988113ea0e3a330b0
  Interface: internal
  Region: regionOne
  Service Name: heat
  Service Type: orchestration
  URL: http://heat-api-internal.openstack-operators.svc:8004/v1/%(tenant_id)s</pre>
</div>
</div>
</li>
<li>
<p>Check the Orchestration service engine services are up:</p>
<div class="listingblock">
<div class="content">
<pre> oc exec -it openstackclient -- openstack orchestration service list -f yaml
- Binary: heat-engine
  Engine ID: b16ad899-815a-4b0c-9f2e-e6d9c74aa200
  Host: heat-engine-6d47856868-p7pzz
  Hostname: heat-engine-6d47856868-p7pzz
  Status: up
  Topic: engine
  Updated At: '2023-10-11T21:48:01.000000'
- Binary: heat-engine
  Engine ID: 887ed392-0799-4310-b95c-ac2d3e6f965f
  Host: heat-engine-6d47856868-p7pzz
  Hostname: heat-engine-6d47856868-p7pzz
  Status: up
  Topic: engine
  Updated At: '2023-10-11T21:48:00.000000'
- Binary: heat-engine
  Engine ID: 26ed9668-b3f2-48aa-92e8-2862252485ea
  Host: heat-engine-6d47856868-p7pzz
  Hostname: heat-engine-6d47856868-p7pzz
  Status: up
  Topic: engine
  Updated At: '2023-10-11T21:48:00.000000'
- Binary: heat-engine
  Engine ID: 1011943b-9fea-4f53-b543-d841297245fd
  Host: heat-engine-6d47856868-p7pzz
  Hostname: heat-engine-6d47856868-p7pzz
  Status: up
  Topic: engine
  Updated At: '2023-10-11T21:48:01.000000'</pre>
</div>
</div>
</li>
<li>
<p>Verify you can now see your the Orchestration service stacks again. Test whether you can create networks, subnets, ports, or routers:</p>
<div class="listingblock">
<div class="content">
<pre> openstack stack list -f yaml
- Creation Time: '2023-10-11T22:03:20Z'
  ID: 20f95925-7443-49cb-9561-a1ab736749ba
  Project: 4eacd0d1cab04427bc315805c28e66c9
  Stack Name: test-networks
  Stack Status: CREATE_COMPLETE
  Updated Time: null</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="adopting-telemetry-services_adopting-shared-file-systems">Adopting Telemetry services</h3>
<div class="paragraph">
<p>Adopting Telemetry means that an existing <code>OpenStackControlPlane</code> custom resource (CR), where Telemetry services are supposed to be disabled, should be patched to start the service with the configuration parameters provided by the source environment.</p>
</div>
<div class="paragraph">
<p>This guide also assumes that:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>A director environment (the source Cloud) is running on one side;</p>
</li>
<li>
<p>A <code>SNO</code> / <code>CodeReadyContainers</code> is running on the other side.</p>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Previous Adoption steps completed. MariaDB, the Identity service (keystone) and the data plane should be already adopted.</p>
</li>
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to deploy <code>cluster-observability-operator</code>:</p>
<div class="listingblock">
<div class="content">
<pre>oc create -f - &lt;&lt;EOF
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: cluster-observability-operator
  namespace: openshift-operators
spec:
  channel: development
  installPlanApproval: Automatic
  name: cluster-observability-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
EOF</pre>
</div>
</div>
</li>
<li>
<p>Wait for the installation to succeed</p>
<div class="listingblock">
<div class="content">
<pre>oc wait --for jsonpath="{.status.phase}"=Succeeded csv --namespace=openshift-operators -l operators.coreos.com/cluster-observability-operator.openshift-operators</pre>
</div>
</div>
</li>
<li>
<p>Enable metrics storage backend</p>
<div class="listingblock">
<div class="content">
<pre>oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  telemetry:
    template:
      metricStorage:
        enabled: true
        monitoringStack:
          alertingEnabled: true
          scrapeInterval: 30s
          storage:
            strategy: persistent
            retention: 24h
            persistent:
              pvcStorageRequest: 20G
'</pre>
</div>
</div>
</li>
<li>
<p>Verify that <code>alertmanager</code> and <code>prometheus</code> pods are available</p>
<div class="listingblock">
<div class="content">
<pre>oc get pods -l alertmanager=metric-storage -n openstack
NAME                            READY   STATUS    RESTARTS   AGE
alertmanager-metric-storage-0   2/2     Running   0          46s
alertmanager-metric-storage-1   2/2     Running   0          46s

oc get pods -l prometheus=metric-storage -n openstack
NAME                          READY   STATUS    RESTARTS   AGE
prometheus-metric-storage-0   3/3     Running   0          46s</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Procedure</div>
<ul>
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to deploy Ceilometer services:</p>
<div class="listingblock">
<div class="content">
<pre>oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  telemetry:
    enabled: true
    template:
      ceilometer:
        passwordSelector:
          ceilometerService: CeilometerPassword
        enabled: true
        secret: osp-secret
        serviceUser: ceilometer
'</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>Inspect the resulting Ceilometer pods:</p>
<div class="listingblock">
<div class="content">
<pre>CEILOMETETR_POD=`oc get pods -l service=ceilometer -n openstack | tail -n 1 | cut -f 1 -d' '`
oc exec -t $CEILOMETETR_POD -c ceilometer-central-agent -- cat /etc/ceilometer/ceilometer.conf</pre>
</div>
</div>
</li>
<li>
<p>Inspect enabled pollsters:</p>
<div class="listingblock">
<div class="content">
<pre>oc get secret ceilometer-config-data -o jsonpath="{.data['polling\.yaml\.j2']}"  | base64 -d</pre>
</div>
</div>
</li>
<li>
<p>Optional: Override default pollsters according to requirements:</p>
<div class="listingblock">
<div class="content">
<pre>oc patch openstackcontrolplane controlplane --type=merge --patch '
spec:
  telemetry:
    template:
      ceilometer:
          defaultConfigOverwrite:
            polling.yaml.j2: |
              ---
              sources:
                - name: pollsters
                  interval: 100
                  meters:
                    - volume.*
                    - image.size
          enabled: true
          secret: osp-secret
'</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to include <code>logging</code></p>
<div class="listingblock">
<div class="content">
<pre>oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  telemetry:
    template:
      logging:
      enabled: false
      ipaddr: 172.17.0.80
      port: 10514
      cloNamespace: openshift-logging
'</pre>
</div>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="adopting-autoscaling_adopting-shared-file-systems">Adopting Autoscaling services</h3>
<div class="paragraph">
<p>Adopting autoscaling means that an existing <code>OpenStackControlPlane</code> custom resource (CR), where Aodh services are supposed to be disabled, should be patched to start the service with the configuration parameters provided by the source environment.</p>
</div>
<div class="paragraph">
<p>This guide also assumes that:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>A director environment (the source Cloud) is running on one side;</p>
</li>
<li>
<p>A <code>SNO</code> / <code>CodeReadyContainers</code> is running on the other side.</p>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Previous Adoption steps completed. MariaDB, the Identity service (keystone), the Orchestration service (heat), and Telemetry
should be already adopted.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to deploy autoscaling services:</p>
<div class="listingblock">
<div class="content">
<pre>oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  telemetry:
    enabled: true
    template:
      autoscaling:
        enabled: true
        aodh:
          passwordSelector:
            aodhService: AodhPassword
          databaseAccount: aodh
          databaseInstance: openstack
          secret: osp-secret
          serviceUser: aodh
        heatInstance: heat
'</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>If autoscaling services are enabled, inspect Aodh pods:</p>
<div class="listingblock">
<div class="content">
<pre>AODH_POD=`oc get pods -l service=aodh -n openstack | tail -n 1 | cut -f 1 -d' '`
oc exec -t $AODH_POD -c aodh-api -- cat /etc/aodh/aodh.conf</pre>
</div>
</div>
</li>
<li>
<p>Check whether Aodh API service is registered in Identity service:</p>
<div class="listingblock">
<div class="content">
<pre>openstack endpoint list | grep aodh
| d05d120153cd4f9b8310ac396b572926 | regionOne | aodh  | alarming  | True    | internal  | http://aodh-internal.openstack.svc:8042  |
| d6daee0183494d7a9a5faee681c79046 | regionOne | aodh  | alarming  | True    | public    | http://aodh-public.openstack.svc:8042    |</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Autoscaling template adoption</div>
<ul>
<li>
<p><code>PrometheusAlarm</code> alarm type must be used instead of <code>GnocchiAggregationByResourcesAlarm</code></p>
</li>
<li>
<p>Create Aodh alarms of type prometheus</p>
<div class="listingblock">
<div class="content">
<pre>openstack alarm create --name high_cpu_alarm \
--type prometheus \
--query "(rate(ceilometer_cpu{resource_name=~'cirros'})) * 100" \
--alarm-action 'log://' \
--granularity 15 \
--evaluation-periods 3 \
--comparison-operator gt \
--threshold 7000000000</pre>
</div>
</div>
</li>
<li>
<p>Verify the state of alarm</p>
<div class="listingblock">
<div class="content">
<pre>openstack alarm list
+--------------------------------------+------------+------------------+-------------------+----------+
| alarm_id                             | type       | name             | state  | severity | enabled  |
+--------------------------------------+------------+------------------+-------------------+----------+
| 209dc2e9-f9d6-40e5-aecc-e767ce50e9c0 | prometheus | prometheus_alarm |   ok   |    low   |   True   |
+--------------------------------------+------------+------------------+-------------------+----------+</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="reviewing-the-openstack-control-plane-configuration_adopting-shared-file-systems">Reviewing the Red&#160;Hat OpenStack Platform control plane configuration</h3>
<div class="paragraph">
<p>Before starting the adoption workflow, pull the configuration from the Red&#160;Hat OpenStack Platform services and director on your file system to back up the configuration files. You can then use the files later, during the configuration of the adopted services, and for the record to compare and make sure nothing has been missed or misconfigured.</p>
</div>
<div class="paragraph">
<p>Make sure you installed and configured the os-diff tool. For more information, see
<a href="#comparing-configuration-files-between-deployments_storage-requirements">Comparing configuration files between deployments</a>.</p>
</div>
<div class="sect3">
<h4 id="pulling-configuration-from-tripleo-deployment_reviewing-configuration">Pulling the configuration from a director deployment</h4>
<div class="paragraph">
<p>You can pull configuration from your Red&#160;Hat OpenStack Platform (RHOSP) services.</p>
</div>
<div class="paragraph">
<p>All the services are describes in a yaml file:</p>
</div>
<div class="paragraph">
<p><a href="https://github.com/openstack-k8s-operators/os-diff/blob/main/config.yaml">service config parameters</a></p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Update your ssh parameters according to your environment in the os-diff.cfg. Os-diff uses those parameters to connect to your director node, query and download the configuration files:</p>
<div class="listingblock">
<div class="content">
<pre>ssh_cmd=ssh -F ssh.config standalone
container_engine=podman
connection=ssh
remote_config_path=/tmp/tripleo</pre>
</div>
</div>
<div class="paragraph">
<p>Make sure the ssh command you provide in <code>ssh_cmd</code> parameter is correct and with key authentication.</p>
</div>
</li>
<li>
<p>Enable or disable the services that you want in the <code>/etc/os-diff/config.yaml</code> file. Make sure that you have the correct rights to edit the file, for example:</p>
<div class="listingblock">
<div class="content">
<pre>chown ospng:ospng /etc/os-diff/config.yaml</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>Example with default Identity service (keystone):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># service name and file location
services:
  # Service name
  keystone:
    # Bool to enable/disable a service (not implemented yet)
    enable: true
    # Pod name, in both OCP and podman context.
    # It could be strict match or will only just grep the podman_name
    # and work with all the pods which matched with pod_name.
    # To enable/disable use strict_pod_name_match: true/false
    podman_name: keystone
    pod_name: keystone
    container_name: keystone-api
    # pod options
    # strict match for getting pod id in TripleO and podman context
    strict_pod_name_match: false
    # Path of the config files you want to analyze.
    # It could be whatever path you want:
    # /etc/&lt;service_name&gt; or /etc or /usr/share/&lt;something&gt; or even /
    # @TODO: need to implement loop over path to support multiple paths such as:
    # - /etc
    # - /usr/share
    path:
      - /etc/
      - /etc/keystone
      - /etc/keystone/keystone.conf
      - /etc/keystone/logging.conf</code></pre>
</div>
</div>
<div class="paragraph">
<p>Repeat this step for each RHOSP service that you want to disable or enable.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>If you are using non-containerized services, such as the <code>ovs-external-ids</code>, os-diff can pull configuration or command output:</p>
<div class="listingblock">
<div class="content">
<pre>services:
  ovs_external_ids:
    hosts:
      - standalone
    service_command: "ovs-vsctl list Open_vSwitch . | grep external_ids | awk -F ': ' '{ print $2; }'"
    cat_output: true
    path:
      - ovs_external_ids.json
    config_mapping:
      ovn-bridge-mappings: edpm_ovn_bridge_mappings
      ovn-bridge: edpm_ovn_bridge
      ovn-encap-type: edpm_ovn_encap_type
      ovn-monitor-all: ovn_monitor_all
      ovn-remote-probe-interval: edpm_ovn_remote_probe_interval
      ovn-ofctrl-wait-before-clear: edpm_ovn_ofctrl_wait_before_clear</pre>
</div>
</div>
<div class="paragraph">
<p>This service is not an Red&#160;Hat OpenStack Platform service executed in a container, so the description and the behavior is different. It is important to correctly configure an SSH config file or equivalent for non-standard services such as OVS. The <code>ovs_external_ids</code> does not run in a container, and the ovs data is stored
on each host of our cloud: controller_1/controller_2/&#8230;&#8203;</p>
</div>
<div class="paragraph">
<p>With the <code>hosts</code> key, os-diff loops on each host and runs the command in the <code>service_command</code> key:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>    ovs_external_ids:
        path:
            - ovs_external_ids.json
        hosts:
            - standalone</pre>
</div>
</div>
<div class="paragraph">
<p>The <code>service_command</code>  provides the required information. It could be a simple cat from a config file. If you want os-diff to get the output of the command and store the output in a file specified by the key path, set <code>cat_output</code> to true. Then you can provide a mapping between in this case the EDPM CRD, and the ovs-vsctl output with config_mapping:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>        service_command: 'ovs-vsctl list Open_vSwitch . | grep external_ids | awk -F '': '' ''{ print $2; }'''
        cat_output: true
        config_mapping:
            ovn-bridge: edpm_ovn_bridge
            ovn-bridge-mappings: edpm_ovn_bridge_mappings
            ovn-encap-type: edpm_ovn_encap_type
            ovn-monitor-all: ovn_monitor_all
            ovn-ofctrl-wait-before-clear: edpm_ovn_ofctrl_wait_before_clear
            ovn-remote-probe-interval: edpm_ovn_remote_probe_interval</pre>
</div>
</div>
<div class="paragraph">
<p>Then you can use the following command to compare the values:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>os-diff diff ovs_external_ids.json edpm.crd --crd --service ovs_external_ids</pre>
</div>
</div>
<div class="paragraph">
<p>For example, to check the <code>/etc/yum.conf</code> on every host, you must put the following statement in the <code>config.yaml</code> file. The following example uses a file called <code>yum_config</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>services:
  yum_config:
    hosts:
      - undercloud
      - controller_1
      - compute_1
      - compute_2
    service_command: "cat /etc/yum.conf"
    cat_output: true
    path:
      - yum.conf</pre>
</div>
</div>
</li>
<li>
<p>Pull the configuration:</p>
<div class="paragraph">
<p>This command will pull all the configuration files that are described in the <code>/etc/os-diff/config.yaml</code> file.
Os-diff can update this file automatically according to your running environment with the command <code>--update</code> or <code>--update-only</code>.
This option sets the podman information into the <code>config.yaml</code> for all running containers.
It can be useful later, when all the Red&#160;Hat OpenStack Platform services are turned off.</p>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>Note that when the <code>config.yaml</code> file is populated automatically you must provide the configuration paths manually for each service.</p>
</div>
<div class="listingblock">
<div class="content">
<pre># will only update the /etc/os-diff/config.yaml
os-diff pull --update-only</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre># will update the /etc/os-diff/config.yaml and pull configuration
os-diff pull --update</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre># will update the /etc/os-diff/config.yaml and pull configuration
os-diff pull</pre>
</div>
</div>
<div class="paragraph">
<p>The configuration will be pulled and stored by default:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>/tmp/tripleo/</pre>
</div>
</div>
<div class="ulist">
<div class="title">Verification</div>
<ul>
<li>
<p>You should have into your local path a directory per services such as:</p>
<div class="listingblock">
<div class="content">
<pre>   tmp/
     tripleo/
       glance/
       keystone/</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2">
<h3 id="rolling-back-control-plane-adoption_reviewing-configuration">Rolling back the control plane adoption</h3>
<div class="paragraph">
<p>If you encountered a problem during the adoption of the Red&#160;Hat OpenStack Platform (RHOSP) control plane services that prevents you from completing the adoption procedure, you can roll back the control plane adoption.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
The roll back operation is only possible during the
control plane parts of the adoption procedure. If you altered the data plane
nodes in any way during the procedure, the roll back is not possible.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>During the control plane adoption, services on the source cloud&#8217;s
control plane are stopped but not removed. The databases on the source
control plane are not edited by the adoption procedure. The
destination control plane received a copy of the original
control plane databases. The roll back procedure assumes that the data
plane has not yet been touched by the adoption procedure and it is
still connected to the source control plane.</p>
</div>
<div class="paragraph">
<p>The rollback procedure consists of the following steps:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Restoring the functionality of the source control plane.</p>
</li>
<li>
<p>Removing the partially or fully deployed destination
control plane.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>To restore the source cloud to a working state, start the RHOSP
control plane services that you previously stopped during the adoption
procedure:</p>
<div class="listingblock">
<div class="content">
<pre>ServicesToStart=("tripleo_horizon.service"
                 "tripleo_keystone.service"
                 "tripleo_barbican_api.service"
                 "tripleo_barbican_worker.service"
                 "tripleo_barbican_keystone_listener.service"
                 "tripleo_cinder_api.service"
                 "tripleo_cinder_api_cron.service"
                 "tripleo_cinder_scheduler.service"
                 "tripleo_cinder_volume.service"
                 "tripleo_cinder_backup.service"
                 "tripleo_glance_api.service"
                 "tripleo_manila_api.service"
                 "tripleo_manila_api_cron.service"
                 "tripleo_manila_scheduler.service"
                 "tripleo_neutron_api.service"
                 "tripleo_placement_api.service"
                 "tripleo_nova_api_cron.service"
                 "tripleo_nova_api.service"
                 "tripleo_nova_conductor.service"
                 "tripleo_nova_metadata.service"
                 "tripleo_nova_scheduler.service"
                 "tripleo_nova_vnc_proxy.service"
                 "tripleo_aodh_api.service"
                 "tripleo_aodh_api_cron.service"
                 "tripleo_aodh_evaluator.service"
                 "tripleo_aodh_listener.service"
                 "tripleo_aodh_notifier.service"
                 "tripleo_ceilometer_agent_central.service"
                 "tripleo_ceilometer_agent_compute.service"
                 "tripleo_ceilometer_agent_ipmi.service"
                 "tripleo_ceilometer_agent_notification.service"
                 "tripleo_ovn_cluster_north_db_server.service"
                 "tripleo_ovn_cluster_south_db_server.service"
                 "tripleo_ovn_cluster_northd.service")

PacemakerResourcesToStart=("galera-bundle"
                           "haproxy-bundle"
                           "rabbitmq-bundle"
                           "openstack-cinder-volume"
                           "openstack-cinder-backup"
                           "openstack-manila-share")

echo "Starting systemd OpenStack services"
for service in ${ServicesToStart[*]}; do
    for i in {1..3}; do
        SSH_CMD=CONTROLLER${i}_SSH
        if [ ! -z "${!SSH_CMD}" ]; then
            if ${!SSH_CMD} sudo systemctl is-enabled $service &amp;&gt; /dev/null; then
                echo "Starting the $service in controller $i"
                ${!SSH_CMD} sudo systemctl start $service
            fi
        fi
    done
done

echo "Checking systemd OpenStack services"
for service in ${ServicesToStart[*]}; do
    for i in {1..3}; do
        SSH_CMD=CONTROLLER${i}_SSH
        if [ ! -z "${!SSH_CMD}" ]; then
            if ${!SSH_CMD} sudo systemctl is-enabled $service &amp;&gt; /dev/null; then
                if ! ${!SSH_CMD} systemctl show $service | grep ActiveState=active &gt;/dev/null; then
                    echo "ERROR: Service $service is not running on controller $i"
                else
                    echo "OK: Service $service is running in controller $i"
                fi
            fi
        fi
    done
done

echo "Starting pacemaker OpenStack services"
for i in {1..3}; do
    SSH_CMD=CONTROLLER${i}_SSH
    if [ ! -z "${!SSH_CMD}" ]; then
        echo "Using controller $i to run pacemaker commands"
        for resource in ${PacemakerResourcesToStart[*]}; do
            if ${!SSH_CMD} sudo pcs resource config $resource &amp;&gt;/dev/null; then
                echo "Starting $resource"
                ${!SSH_CMD} sudo pcs resource enable $resource
            else
                echo "Service $resource not present"
            fi
        done
        break
    fi
done

echo "Checking pacemaker OpenStack services"
for i in {1..3}; do
    SSH_CMD=CONTROLLER${i}_SSH
    if [ ! -z "${!SSH_CMD}" ]; then
        echo "Using controller $i to run pacemaker commands"
        for resource in ${PacemakerResourcesToStop[*]}; do
            if ${!SSH_CMD} sudo pcs resource config $resource &amp;&gt;/dev/null; then
                if ${!SSH_CMD} sudo pcs resource status $resource | grep Started &gt;/dev/null; then
                    echo "OK: Service $resource is started"
                else
                    echo "ERROR: Service $resource is stopped"
                fi
            fi
        done
        break
    fi
done</pre>
</div>
</div>
</li>
<li>
<p>If the Ceph NFS service is running on the deployment as a Shared File Systems service (manila)
backend, you must restore the pacemaker ordering and colocation constraints
involving the "openstack-manila-share" service:</p>
<div class="listingblock">
<div class="content">
<pre>sudo pcs constraint order start ceph-nfs then openstack-manila-share kind=Optional id=order-ceph-nfs-openstack-manila-share-Optional
sudo pcs constraint colocation add openstack-manila-share with ceph-nfs score=INFINITY id=colocation-openstack-manila-share-ceph-nfs-INFINITY</pre>
</div>
</div>
</li>
<li>
<p>Verify that the source cloud is operational again, e.g. by
running <code>openstack</code> CLI commands or using the Dashboard service (horizon).</p>
</li>
<li>
<p>Remove the partially or fully deployed control plane so that another adoption attempt can be made later:</p>
<div class="listingblock">
<div class="content">
<pre>oc delete --ignore-not-found=true --wait=false openstackcontrolplane/openstack
oc patch openstackcontrolplane openstack --type=merge --patch '
metadata:
  finalizers: []
' || true

while oc get pod | grep rabbitmq-server-0; do
    sleep 2
done
while oc get pod | grep openstack-galera-0; do
    sleep 2
done

oc delete --ignore-not-found=true --wait=false pod mariadb-copy-data
oc delete --ignore-not-found=true --wait=false pvc mariadb-data
oc delete --ignore-not-found=true --wait=false pod ovn-copy-data
oc delete --ignore-not-found=true secret osp-secret</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Since restoring the source control plane services, their internal
state may have changed. Before retrying the adoption procedure, it is
important to verify that the control plane resources have
been removed and there are no leftovers which could affect the
following adoption procedure attempt. Notably, the previously created
copies of the database contents must not be used in another adoption
attempt, and new copies must be made according to the adoption
procedure documentation.
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="adopting-data-plane_reviewing-configuration">Adopting the data plane</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Adopting the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) data plane involves the following steps:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Stopping any remaining services on the Red&#160;Hat OpenStack Platform (RHOSP) control plane.</p>
</li>
<li>
<p>Deploying the required custom resources.</p>
</li>
<li>
<p>If applicable, performing a fast-forward upgrade on Compute services from RHOSP 17.1 to RHOSO 18.0.</p>
</li>
</ol>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
After the RHOSO control plane is managing the newly deployed data plane, you must not re-enable services on the RHOSP 17.1 control plane and data plane. Re-enabling services causes workloads to be managed by two control planes or two data planes, resulting in data corruption, loss of control of existing workloads, inability to start new workloads, or other issues.
</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="stopping-infrastructure-management-and-compute-services_data-plane">Stopping infrastructure management and Compute services</h3>
<div class="paragraph">
<p>The source cloud&#8217;s control plane can be decomissioned,
which is taking down only cloud controllers, database and messaging nodes.
Nodes that must remain functional are those running the <code>Compute</code>, <code>Storage</code>,
or <code>Networker</code> roles (in terms of composable roles covered by
director Heat Templates).</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Define the following shell variables. The values that are used are examples and refer to a single node standalone director deployment. Replace these example values with values that are correct for your environment:</p>
<div class="listingblock">
<div class="content">
<pre>EDPM_PRIVATEKEY_PATH="<strong>&lt;path to SSH key&gt;</strong>"
declare -A computes
computes=(
  ["standalone.localdomain"]="192.168.122.100"
  # ...
)</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>["standalone.localdomain"]="192.168.122.100"</code> with the name of the Compute node and its IP address.</p>
</li>
<li>
<p>These ssh variables with the ssh commands are used instead of ansible to create instructions that are independent of where they are running. But ansible commands could be used to achieve the same result if you are in the right host, for example to stop a service:</p>
<div class="listingblock">
<div class="content">
<pre>. stackrc
ansible -i $(which tripleo-ansible-inventory) Compute -m shell -a "sudo systemctl stop tripleo_virtqemud.service" -b</pre>
</div>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Procedure</div>
<ul>
<li>
<p>Run the following script to remove the conflicting repositories and packages (in case of a devsetup that uses Standalone director) from all Compute hosts. That is required to install libvirt packages, when these hosts become adopted as data plane nodes, where modular libvirt daemons are no longer running in podman containers:</p>
<div class="listingblock">
<div class="content">
<pre>PacemakerResourcesToStop=(
                "galera-bundle"
                "haproxy-bundle"
                "rabbitmq-bundle")

echo "Stopping pacemaker services"
for i in {1..3}; do
    SSH_CMD=CONTROLLER${i}_SSH
    if [ ! -z "${!SSH_CMD}" ]; then
        echo "Using controller $i to run pacemaker commands"
        for resource in ${PacemakerResourcesToStop[*]}; do
            if ${!SSH_CMD} sudo pcs resource config $resource; then
                ${!SSH_CMD} sudo pcs resource disable $resource
            fi
        done
        break
    fi
done</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="adopting-compute-services-to-the-data-plane_data-plane">Adopting Compute services to the RHOSO data plane</h3>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Remaining source cloud <a href="#stopping-infrastructure-management-and-compute-services_data-plane">Stopping infrastructure management and Compute services</a> on Compute hosts.</p>
</li>
<li>
<p>Ceph backend for Nova/Libvirt is configured <a href="#configuring-a-ceph-backend_migrating-databases">Configuring a Ceph backend</a>.</p>
</li>
<li>
<p>Make sure the IPAM is configured</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre>oc apply -f - &lt;&lt;EOF
apiVersion: network.openstack.org/v1beta1
kind: NetConfig
metadata:
  name: netconfig
spec:
  networks:
  - name: ctlplane
    dnsDomain: ctlplane.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 192.168.122.120
        start: 192.168.122.100
      - end: 192.168.122.200
        start: 192.168.122.150
      cidr: 192.168.122.0/24
      gateway: 192.168.122.1
  - name: internalapi
    dnsDomain: internalapi.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.17.0.250
        start: 172.17.0.100
      cidr: 172.17.0.0/24
      vlan: 20
  - name: External
    dnsDomain: external.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 10.0.0.250
        start: 10.0.0.100
      cidr: 10.0.0.0/24
      gateway: 10.0.0.1
  - name: storage
    dnsDomain: storage.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.18.0.250
        start: 172.18.0.100
      cidr: 172.18.0.0/24
      vlan: 21
  - name: storagemgmt
    dnsDomain: storagemgmt.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.20.0.250
        start: 172.20.0.100
      cidr: 172.20.0.0/24
      vlan: 23
  - name: tenant
    dnsDomain: tenant.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.19.0.250
        start: 172.19.0.100
      cidr: 172.19.0.0/24
      vlan: 22
EOF</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>When <code>neutron-sriov-nic-agent</code> is running on the existing Compute nodes, check the physical device mappings and ensure that they match the values that are defined in the <code>OpenStackDataPlaneNodeSet</code> custom resource (CR). For more information, see <a href="#pulling-configuration-from-tripleo-deployment_reviewing-configuration">Pulling the configuration from a director deployment</a>.</p>
</li>
<li>
<p>Define the shell variables necessary to run the script that runs the fast-forward upgrade. Omit setting <code>CEPH_FSID</code>, if the local storage backend is going to be configured by Nova for Libvirt. The storage backend cannot be changed during adoption, and must match the one used on the source cloud:</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre>PODIFIED_DB_ROOT_PASSWORD=$(oc get -o json secret/osp-secret | jq -r .data.DbRootPassword | base64 -d)
CEPH_FSID=$(oc get secret ceph-conf-files -o json | jq -r '.data."ceph.conf"' | base64 -d | grep fsid | sed -e 's/fsid = //'

alias openstack="oc exec -t openstackclient -- openstack"
declare -A computes
export computes=(
  ["standalone.localdomain"]="192.168.122.100"
  # ...
)</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>["standalone.localdomain"]="192.168.122.100"</code> with the name of the Compute node and its IP address.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create a ssh authentication secret for the data plane nodes:</p>
<div class="listingblock">
<div class="content">
<pre>oc apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Secret
metadata:
    name: dataplane-adoption-secret
    namespace: openstack
data:
    ssh-privatekey: |
$(cat <strong>&lt;path to SSH key&gt;</strong> | base64 | sed 's/^/        /')
EOF</pre>
</div>
</div>
</li>
<li>
<p>Generate an ssh key-pair <code>nova-migration-ssh-key</code> secret:</p>
<div class="listingblock">
<div class="content">
<pre>cd "$(mktemp -d)"
ssh-keygen -f ./id -t ecdsa-sha2-nistp521 -N ''
oc get secret nova-migration-ssh-key || oc create secret generic nova-migration-ssh-key \
  -n openstack \
  --from-file=ssh-privatekey=id \
  --from-file=ssh-publickey=id.pub \
  --type kubernetes.io/ssh-auth
rm -f id*
cd -</pre>
</div>
</div>
</li>
<li>
<p>Create a <code>nova-compute-extra-config</code> service (with local storage backend for <code>libvirt</code>):</p>
</li>
<li>
<p>If TLS Everywhere is enabled, append the following to the OpenStackDataPlaneService spec:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">  tlsCerts:
    contents:
      - dnsnames
      - ips
    networks:
      - ctlplane
    issuer: osp-rootca-issuer-internal
  caCerts: combined-ca-bundle
  edpmServiceType: nova</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">oc apply -f - &lt;&lt;EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: nova-extra-config
  namespace: openstack
data:
  19-nova-compute-cell1-workarounds.conf: |
    [workarounds]
    disable_compute_service_check_for_ffu=true
EOF</code></pre>
</div>
</div>
<div class="paragraph">
<p>The secret <code>nova-cell&lt;X&gt;-compute-config</code> is auto-generated for each
<code>cell&lt;X&gt;</code>. You must specify <code>nova-cell&lt;X&gt;-compute-config</code> and <code>nova-migration-ssh-key</code> for each custom <code>OpenStackDataPlaneService</code> related to the Compute service.</p>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>That service removes pre-FFU workarounds and configures Compute
services for local storage backend.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Or, create a <code>nova-compute-extra-config</code> service (with Ceph backend for <code>libvirt</code>):</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">oc apply -f - &lt;&lt;EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: nova-extra-config
  namespace: openstack
data:
  19-nova-compute-cell1-workarounds.conf: |
    [workarounds]
    disable_compute_service_check_for_ffu=true
  03-ceph-nova.conf: |
    [libvirt]
    images_type=rbd
    images_rbd_pool=vms
    images_rbd_ceph_conf=/etc/ceph/ceph.conf
    images_rbd_glance_store_name=default_backend
    images_rbd_glance_copy_poll_interval=15
    images_rbd_glance_copy_timeout=600
    rbd_user=openstack
    rbd_secret_uuid=$CEPH_FSID
EOF</code></pre>
</div>
</div>
<div class="paragraph">
<p>That service removes pre-FFU workarounds and configures Compute
services for Ceph storage backend.
Provided above resources should contain a cell-specific configurations.
For multi-cell, config maps and Red&#160;Hat OpenStack Platform data plane services should be named like <code>nova-custom-ceph-cellX</code> and <code>nova-compute-extraconfig-cellX</code>.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create a secret for the subscription manager and a secret for the Red Hat registry:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">oc apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Secret
metadata:
  name: subscription-manager
data:
  username: &lt;base64 encoded subscription-manager username&gt;
  password: &lt;base64 encoded subscription-manager password&gt;
---
apiVersion: v1
kind: Secret
metadata:
  name: redhat-registry
data:
  username: &lt;base64 encoded registry username&gt;
  password: &lt;base64 encoded registry password&gt;
EOF</code></pre>
</div>
</div>
</li>
<li>
<p>Deploy the <code>OpenStackDataPlaneNodeSet</code> CR:</p>
</li>
<li>
<p>If TLS Everywhere is enabled, change spec:tlsEnabled to true</p>
</li>
<li>
<p>If using a custom DNS Domain, modify the spec:nodes:[NODE NAME]:hostName to use fqdn for the node</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">oc apply -f - &lt;&lt;EOF
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneNodeSet
metadata:
  name: openstack
spec:
  tlsEnabled: false
  networkAttachments:
      - ctlplane
  preProvisioned: true
  services:
    - bootstrap
    - download-cache
    - configure-network
    - validate-network
    - install-os
    - configure-os
    - ssh-known-hosts
    - run-os
    - reboot-os
    - install-certs
    - libvirt
    - nova
    - ovn
    - neutron-metadata
    - telemetry
  env:
    - name: ANSIBLE_CALLBACKS_ENABLED
      value: "profile_tasks"
    - name: ANSIBLE_FORCE_COLOR
      value: "True"
  nodes:
    standalone:
      hostName: standalone
      ansible:
        ansibleHost: ${computes[standalone.localdomain]}
      networks:
      - defaultRoute: true
        fixedIP: ${computes[standalone.localdomain]}
        name: ctlplane
        subnetName: subnet1
      - name: internalapi
        subnetName: subnet1
      - name: storage
        subnetName: subnet1
      - name: tenant
        subnetName: subnet1
  nodeTemplate:
    ansibleSSHPrivateKeySecret: dataplane-adoption-secret
    ansible:
      ansibleUser: root
      ansibleVarsFrom:
      - prefix: subscription_manager_
        secretRef:
          name: subscription-manager
      - prefix: registry_
        secretRef:
          name: redhat-registry
      ansibleVars:
        edpm_bootstrap_release_version_package: []
        # edpm_network_config
        # Default nic config template for a EDPM node
        # These vars are edpm_network_config role vars
        edpm_network_config_template: |
           ---
           {% set mtu_list = [ctlplane_mtu] %}
           {% for network in nodeset_networks %}
           {{ mtu_list.append(lookup('vars', networks_lower[network] ~ '_mtu')) }}
           {%- endfor %}
           {% set min_viable_mtu = mtu_list | max %}
           network_config:
           - type: ovs_bridge
             name: {{ neutron_physical_bridge_name }}
             mtu: {{ min_viable_mtu }}
             use_dhcp: false
             dns_servers: {{ ctlplane_dns_nameservers }}
             domain: {{ dns_search_domains }}
             addresses:
             - ip_netmask: {{ ctlplane_ip }}/{{ ctlplane_cidr }}
             routes: {{ ctlplane_host_routes }}
             members:
             - type: interface
               name: nic1
               mtu: {{ min_viable_mtu }}
               # force the MAC address of the bridge to this interface
               primary: true
           {% for network in nodeset_networks %}
             - type: vlan
               mtu: {{ lookup('vars', networks_lower[network] ~ '_mtu') }}
               vlan_id: {{ lookup('vars', networks_lower[network] ~ '_vlan_id') }}
               addresses:
               - ip_netmask:
                   {{ lookup('vars', networks_lower[network] ~ '_ip') }}/{{ lookup('vars', networks_lower[network] ~ '_cidr') }}
               routes: {{ lookup('vars', networks_lower[network] ~ '_host_routes') }}
           {% endfor %}

        edpm_network_config_hide_sensitive_logs: false
        #
        # These vars are for the network config templates themselves and are
        # considered EDPM network defaults.
        neutron_physical_bridge_name: br-ctlplane
        neutron_public_interface_name: eth0

        # edpm_nodes_validation
        edpm_nodes_validation_validate_controllers_icmp: false
        edpm_nodes_validation_validate_gateway_icmp: false

        # edpm ovn-controller configuration
        edpm_ovn_bridge_mappings: &lt;bridge_mappings&gt;
        edpm_ovn_bridge: br-int
        edpm_ovn_encap_type: geneve
        ovn_monitor_all: true
        edpm_ovn_remote_probe_interval: 60000
        edpm_ovn_ofctrl_wait_before_clear: 8000

        timesync_ntp_servers:
        - hostname: clock.redhat.com
        - hostname: clock2.redhat.com

        edpm_bootstrap_command: |
          subscription-manager register --username {{ subscription_manager_username }} --password {{ subscription_manager_password }}
          subscription-manager release --set=9.2
          subscription-manager repos --disable=*
          subscription-manager repos --enable=rhel-9-for-x86_64-baseos-eus-rpms --enable=rhel-9-for-x86_64-appstream-eus-rpms --enable=rhel-9-for-x86_64-highavailability-eus-rpms --enable=openstack-17.1-for-rhel-9-x86_64-rpms --enable=fast-datapath-for-rhel-9-x86_64-rpms --enable=openstack-dev-preview-for-rhel-9-x86_64-rpms
          # FIXME: perform dnf upgrade for other packages in EDPM ansible
          # here we only ensuring that decontainerized libvirt can start
          dnf -y upgrade openstack-selinux
          rm -f /run/virtlogd.pid
          podman login -u {{ registry_username }} -p {{ registry_password }} registry.redhat.io

        gather_facts: false
        # edpm firewall, change the allowed CIDR if needed
        edpm_sshd_configure_firewall: true
        edpm_sshd_allowed_ranges: ['192.168.122.0/24']

        # Do not attempt OVS major upgrades here
        edpm_ovs_packages:
        - openvswitch3.1
EOF</code></pre>
</div>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Prepare adopted EDPM workloads to use Ceph backend for Block Storage service (cinder), if configured so</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">oc patch osdpns/openstack --type=merge --patch "
spec:
  services:
    - bootstrap
    - download-cache
    - configure-network
    - validate-network
    - install-os
    - configure-os
    - ssh-known-hosts
    - run-os
    - reboot-os
    - ceph-client
    - install-certs
    - ovn
    - neutron-metadata
    - libvirt
    - nova
    - telemetry
  nodeTemplate:
    extraMounts:
    - extraVolType: Ceph
      volumes:
      - name: ceph
        secret:
          secretName: ceph-conf-files
      mounts:
      - name: ceph
        mountPath: "/etc/ceph"
        readOnly: true
"</code></pre>
</div>
</div>
<div class="paragraph">
<p>Note that you should retain the original <code>OpenStackDataPlaneNodeSet</code> services
composition, except the inserted <code>ceph-client</code> service.</p>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>+ Ensure that the <code>ovn-controller</code> settings that are configured in the <code>OpenStackDataPlaneNodeSet</code> CR are the same as were set in the Compute nodes before adoption. This configuration is stored in the <code>external_ids</code> column in the <code>Open_vSwitch</code> table in the Open vSwitch database:</p>
</div>
<div class="paragraph">
<p>+</p>
</div>
<div class="listingblock">
<div class="content">
<pre>ovs-vsctl list Open .
...
external_ids        : {hostname=standalone.localdomain, ovn-bridge=br-int, ovn-bridge-mappings=&lt;bridge_mappings&gt;, ovn-chassis-mac-mappings="datacentre:1e:0a:bb:e6:7c:ad", ovn-encap-ip="172.19.0.100", ovn-encap-tos="0", ovn-encap-type=geneve, ovn-match-northd-version=False, ovn-monitor-all=True, ovn-ofctrl-wait-before-clear="8000", ovn-openflow-probe-interval="60", ovn-remote="tcp:ovsdbserver-sb.openstack.svc:6642", ovn-remote-probe-interval="60000", rundir="/var/run/openvswitch", system-id="2eec68e6-aa21-4c95-a868-31aeafc11736"}
...</pre>
</div>
</div>
<div class="paragraph">
<p>+
* Replace <code>&lt;bridge_mappings&gt;</code> with the value of the bridge mappings in your configuration, for example, <code>"datacentre:br-ctlplane"</code>.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Optional: Enable <code>neutron-sriov-nic-agent</code> in the <code>OpenStackDataPlaneNodeSet</code> CR:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">oc patch openstackdataplanenodeset openstack --type='json' --patch='[
  {
    "op": "add",
    "path": "/spec/services/-",
    "value": "neutron-sriov"
  }, {
    "op": "add",
    "path": "/spec/nodeTemplate/ansible/ansibleVars/edpm_neutron_sriov_agent_SRIOV_NIC_physical_device_mappings",
    "value": "dummy_sriov_net:dummy-dev"
  }, {
    "op": "add",
    "path": "/spec/nodeTemplate/ansible/ansibleVars/edpm_neutron_sriov_agent_SRIOV_NIC_resource_provider_bandwidths",
    "value": "dummy-dev:40000000:40000000"
  }, {
    "op": "add",
    "path": "/spec/nodeTemplate/ansible/ansibleVars/edpm_neutron_sriov_agent_SRIOV_NIC_resource_provider_hypervisors",
    "value": "dummy-dev:standalone.localdomain"
  }
]'</code></pre>
</div>
</div>
</li>
<li>
<p>Optional: Enable <code>neutron-dhcp</code> in the <code>OpenStackDataPlaneNodeSet</code> CR:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">oc patch openstackdataplanenodeset openstack --type='json' --patch='[
  {
    "op": "add",
    "path": "/spec/services/-",
    "value": "neutron-dhcp"
  }]'</code></pre>
</div>
</div>
</li>
<li>
<p>Run pre-adoption validation:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Create the validation service:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">oc apply -f - &lt;&lt;EOF
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneService
metadata:
  name: pre-adoption-validation
spec:
  playbook: osp.edpm.pre_adoption_validation
EOF</code></pre>
</div>
</div>
</li>
<li>
<p>Create a <code>OpenStackDataPlaneDeployment</code> CR that runs the validation only:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">oc apply -f - &lt;&lt;EOF
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneDeployment
metadata:
  name: openstack-pre-adoption
spec:
  nodeSets:
  - openstack
  servicesOverride:
  - pre-adoption-validation
EOF</code></pre>
</div>
</div>
<div class="paragraph">
<p>Wait for the validation to finish.</p>
</div>
</li>
<li>
<p>Confirm that all the Ansible EE pods reach a <code>Completed</code> status:</p>
<div class="listingblock">
<div class="content">
<pre># watching the pods
watch oc get pod -l app=openstackansibleee</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre># following the ansible logs with:
oc logs -l app=openstackansibleee -f --max-log-requests 20</pre>
</div>
</div>
</li>
<li>
<p>Wait for the deployment to reach the <code>Ready</code> status:</p>
<div class="listingblock">
<div class="content">
<pre>oc wait --for condition=Ready openstackdataplanedeployment/openstack-pre-adoption --timeout=10m</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>If any openstack-pre-adoption validations fail, you must first determine
which ones were unsuccessful based on the ansible logs and then follow the
instructions below for each case:</p>
<div class="ulist">
<ul>
<li>
<p>if the hostname validation failed then check that the hostname of the EDPM
node is correctly listed in the <code>OpenStackDataPlaneNodeSet</code></p>
</li>
<li>
<p>if the kernel argument check failed then make sure that the
<code>OpenStackDataPlaneNodeSet</code> has the same kernel argument configuration in
<code>edpm_kernel_args</code> and <code>edpm_kernel_hugepages</code> variables than what is used in
the 17 node.</p>
</li>
<li>
<p>if the tuned profile check failed then make sure that the
<code>edpm_tuned_profile</code> variable in the <code>OpenStackDataPlaneNodeSet</code> is configured
to use the same profile as set on the (source) OSP 17 node.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Remove leftover director services</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Create cleanup data plane service</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">---
oc apply -f - &lt;&lt;EOF
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneService
metadata:
  name: tripleo-cleanup
spec:
  playbook: osp.edpm.tripleo_cleanup
EOF
---</code></pre>
</div>
</div>
</li>
<li>
<p>Create OpenStackDataPlaneDeployment to run cleanup</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">---
oc apply -f - &lt;&lt;EOF
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneDeployment
metadata:
  name: tripleo-cleanup
spec:
  nodeSets:
  - openstack
  servicesOverride:
  - tripleo-cleanup
EOF
---</code></pre>
</div>
</div>
</li>
<li>
<p>Wait for the removal to finish.</p>
</li>
</ol>
</div>
</li>
<li>
<p>Deploy the <code>OpenStackDataPlaneDeployment</code> CR:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">oc apply -f - &lt;&lt;EOF
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneDeployment
metadata:
  name: openstack
spec:
  nodeSets:
  - openstack
EOF</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>Note: if you have other node sets to deploy (e.g. for networker nodes), you may
add them above in <code>nodeSets</code> list, or create separate
<code>OpenStackDataPlaneDeployment</code> CRs later.</p>
</div>
<div class="paragraph">
<p>Note that once a <code>OpenStackDataPlaneDeployment</code> CR is deployed, it&#8217;s impossible
to add new node sets to it later.</p>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>Confirm that all the Ansible EE pods reach a <code>Completed</code> status:</p>
<div class="listingblock">
<div class="content">
<pre># watching the pods
watch oc get pod -l app=openstackansibleee</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre># following the ansible logs with:
oc logs -l app=openstackansibleee -f --max-log-requests 20</pre>
</div>
</div>
</li>
<li>
<p>Wait for the data plane node set to reach the <code>Ready</code> status:</p>
<div class="listingblock">
<div class="content">
<pre>oc wait --for condition=Ready osdpns/openstack --timeout=30m</pre>
</div>
</div>
</li>
<li>
<p>Verify that Networking service (neutron) agents are alive:</p>
<div class="listingblock">
<div class="content">
<pre>oc exec openstackclient -- openstack network agent list
+--------------------------------------+------------------------------+------------------------+-------------------+-------+-------+----------------------------+
| ID                                   | Agent Type                   | Host                   | Availability Zone | Alive | State | Binary                     |
+--------------------------------------+------------------------------+------------------------+-------------------+-------+-------+----------------------------+
| 174fc099-5cc9-4348-b8fc-59ed44fcfb0e | DHCP agent                   | standalone.localdomain | nova              | :-)   | UP    | neutron-dhcp-agent         |
| 10482583-2130-5b0d-958f-3430da21b929 | OVN Metadata agent           | standalone.localdomain |                   | :-)   | UP    | neutron-ovn-metadata-agent |
| a4f1b584-16f1-4937-b2b0-28102a3f6eaa | OVN Controller agent         | standalone.localdomain |                   | :-)   | UP    | ovn-controller             |
+--------------------------------------+------------------------------+------------------------+-------------------+-------+-------+----------------------------+</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="performing-a-fast-forward-upgrade-on-compute-services_data-plane">Performing a fast-forward upgrade on Compute services</h3>
<div class="paragraph">
<p>Compute services rolling upgrade cannot be done during adoption,
there is in a lock-step with Compute control plane services, because those are managed independently by data plane ansible and Kubernetes Operators.
The Compute service operator and OpenStack Operator ensure upgrading
is done independently of each other, by configuring
<code>[upgrade_levels]compute=auto</code> for Compute services. Compute control plane
services apply the change right after custom resource (CR) is patched. Compute data plane services will catch up the same config change with ansible deployment later on.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Wait for cell1 Compute data plane services version updated (it may take some time):</p>
<div class="listingblock">
<div class="content">
<pre>oc exec openstack-cell1-galera-0 -c galera -- mysql -rs -uroot -p$PODIFIED_DB_ROOT_PASSWORD \
    -e "select a.version from nova_cell1.services a join nova_cell1.services b where a.version!=b.version and a.binary='nova-compute';"</pre>
</div>
</div>
<div class="paragraph">
<p>The above query should return an empty result as a completion criterion.</p>
</div>
</li>
<li>
<p>Remove pre-fast-forward upgrade workarounds for Compute control plane services:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">oc patch openstackcontrolplane openstack -n openstack --type=merge --patch '
spec:
  nova:
    template:
      cellTemplates:
        cell0:
          conductorServiceTemplate:
            customServiceConfig: |
              [workarounds]
              disable_compute_service_check_for_ffu=false
        cell1:
          metadataServiceTemplate:
            customServiceConfig: |
              [workarounds]
              disable_compute_service_check_for_ffu=false
          conductorServiceTemplate:
            customServiceConfig: |
              [workarounds]
              disable_compute_service_check_for_ffu=false
      apiServiceTemplate:
        customServiceConfig: |
          [workarounds]
          disable_compute_service_check_for_ffu=false
      metadataServiceTemplate:
        customServiceConfig: |
          [workarounds]
          disable_compute_service_check_for_ffu=false
      schedulerServiceTemplate:
        customServiceConfig: |
          [workarounds]
          disable_compute_service_check_for_ffu=false
'</code></pre>
</div>
</div>
</li>
<li>
<p>Wait for Compute control plane services' CRs to be ready:</p>
<div class="listingblock">
<div class="content">
<pre>oc wait --for condition=Ready --timeout=300s Nova/nova</pre>
</div>
</div>
</li>
<li>
<p>Remove pre-fast-forward upgrade workarounds for Compute data plane services:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">oc apply -f - &lt;&lt;EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: nova-extra-config
  namespace: openstack
data:
  20-nova-compute-cell1-workarounds.conf: |
    [workarounds]
    disable_compute_service_check_for_ffu=false
---
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneDeployment
metadata:
  name: openstack-nova-compute-ffu
  namespace: openstack
spec:
  nodeSets:
    - openstack
  servicesOverride:
    - nova
EOF</code></pre>
</div>
</div>
</li>
<li>
<p>Wait for Compute data plane service to be ready:</p>
<div class="listingblock">
<div class="content">
<pre>oc wait --for condition=Ready openstackdataplanedeployment/openstack-nova-compute-ffu --timeout=5m</pre>
</div>
</div>
</li>
<li>
<p>Run Compute database online migrations to complete the fast-forward upgrade:</p>
<div class="listingblock">
<div class="content">
<pre>oc exec -it nova-cell0-conductor-0 -- nova-manage db online_data_migrations
oc exec -it nova-cell1-conductor-0 -- nova-manage db online_data_migrations</pre>
</div>
</div>
</li>
<li>
<p>Discover Compute hosts in the cell:</p>
<div class="listingblock">
<div class="content">
<pre>oc rsh nova-cell0-conductor-0 nova-manage cell_v2 discover_hosts --verbose</pre>
</div>
</div>
</li>
<li>
<p>Verify if Compute services can stop the existing test VM instance:</p>
<div class="listingblock">
<div class="content">
<pre>${BASH_ALIASES[openstack]} server list | grep -qF '| test | ACTIVE |' &amp;&amp; ${BASH_ALIASES[openstack]} server stop test || echo PASS
${BASH_ALIASES[openstack]} server list | grep -qF '| test | SHUTOFF |' || echo FAIL
${BASH_ALIASES[openstack]} server --os-compute-api-version 2.48 show --diagnostics test 2&gt;&amp;1 || echo PASS</pre>
</div>
</div>
</li>
<li>
<p>Verify if Compute services can start the existing test VM instance:</p>
<div class="listingblock">
<div class="content">
<pre>${BASH_ALIASES[openstack]} server list | grep -qF '| test | SHUTOFF |' &amp;&amp; ${BASH_ALIASES[openstack]} server start test || echo PASS
${BASH_ALIASES[openstack]} server list | grep -F '| test | ACTIVE |' &amp;&amp; \
  ${BASH_ALIASES[openstack]} server --os-compute-api-version 2.48 show --diagnostics test --fit-width -f json | jq -r '.state' | grep running || echo FAIL</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
After the data plane adoption, the hosts continue to run Red Hat Enterprise Linux (RHEL)
9.2. To take advantage of RHEL 9.4, perform a minor update procedure after finishing the adoption procedure.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="adopting-networker-services-to-the-data-plane_data-plane">Adopting Networker services to the RHOSO data plane</h3>
<div class="paragraph">
<p>If you have <code>Networker</code> nodes in your existing RHOSO deployment, you
should adopt them to the RHOSO data plane too. The procedure is largely
the same as for <code>Compute</code> nodes.</p>
</div>
<div class="paragraph">
<p>The main difference between <code>Networker</code> and <code>Compute</code> nodes is that <code>Networker</code>
nodes do not run <code>nova-compute</code> and <code>libvirt</code> services.</p>
</div>
<div class="paragraph">
<p>You won&#8217;t need to run <code>neutron-sriov</code> service on <code>Networker</code> nodes either. But
depending on your topology, you might need to run <code>neutron-metadata</code> service
there. Specifically, when you&#8217;d like to serve metadata to SR-IOV ports hosted
on <code>Compute</code> nodes.</p>
</div>
<div class="paragraph">
<p>Assuming you&#8217;d like to continue running OVN gateway services on <code>Networker</code>
nodes, you will keep <code>ovn</code> service in the list to deploy. (You may later decide
to deprovision OVN gateway services from <code>Networker</code> nodes and run them on OCP
nodes instead. This is currently not recommended and is out of scope for the
adoption procedure.)</p>
</div>
<div class="paragraph">
<p>You may also want to run <code>neutron-dhcp</code> service on <code>Networker</code> nodes instead of
<code>Compute</code> nodes. (But note that using <code>neutron-dhcp</code> with OVN is not generally
needed.)</p>
</div>
<div class="paragraph">
<p>Whatever the final list of services you&#8217;d like to run on <code>Networker</code> nodes, you
will create a separate <code>OpenStackDataPlaneNodeSet</code> CR just for these nodes.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Define the following shell variable. The value that is used is an example
and refers to a single node standalone director deployment.
Adjust the value to your environment:</p>
<div class="listingblock">
<div class="content">
<pre>declare -A networkers
networkers=(
  ["standalone.localdomain"]="192.168.122.100"
  # ...
)</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>["standalone.localdomain"]="192.168.122.100"</code> with the name of the <code>Networker</code> node and its IP address.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Deploy the other <code>OpenStackDataPlaneNodeSet</code> CR. For simplicity, you can
reuse most of the <code>nodeTemplate</code> section from the <code>openstack</code> CR designated for
<code>Compute</code> nodes, though strictly speaking, some of the variables may be
unnecessary because of a more limited set of services running on <code>Networker</code>
nodes. Make sure <code>edpm_enable_chassis_gw: true</code> is set to run <code>ovn-controller</code>
in gateway mode.</p>
</li>
<li>
<p>If TLS Everywhere is enabled, change <code>spec:tlsEnabled</code> to <code>true</code>.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">oc apply -f - &lt;&lt;EOF
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneNodeSet
metadata:
  name: openstack-networker
spec:
  tlsEnabled: false
  networkAttachments:
      - ctlplane
  preProvisioned: true
  services:
    - bootstrap
    - download-cache
    - configure-network
    - validate-network
    - install-os
    - configure-os
    - ssh-known-hosts
    - run-os
    - install-certs
    - ovn
  env:
    - name: ANSIBLE_CALLBACKS_ENABLED
      value: "profile_tasks"
    - name: ANSIBLE_FORCE_COLOR
      value: "True"
  nodes:
    standalone:
      hostName: standalone
      ansible:
        ansibleHost: ${networkers[standalone.localdomain]}
      networks:
      - defaultRoute: true
        fixedIP: ${networkers[standalone.localdomain]}
        name: ctlplane
        subnetName: subnet1
      - name: internalapi
        subnetName: subnet1
      - name: storage
        subnetName: subnet1
      - name: tenant
        subnetName: subnet1
  nodeTemplate:
    ansibleSSHPrivateKeySecret: dataplane-adoption-secret
    ansible:
      ansibleUser: root
      ansibleVarsFrom:
      - prefix: subscription_manager_
        secretRef:
          name: subscription-manager
      - prefix: registry_
        secretRef:
          name: redhat-registry
      ansibleVars:
        edpm_bootstrap_release_version_package: []
        # edpm_network_config
        # Default nic config template for a EDPM node
        # These vars are edpm_network_config role vars
        edpm_network_config_template: |
           ---
           {% set mtu_list = [ctlplane_mtu] %}
           {% for network in nodeset_networks %}
           {{ mtu_list.append(lookup('vars', networks_lower[network] ~ '_mtu')) }}
           {%- endfor %}
           {% set min_viable_mtu = mtu_list | max %}
           network_config:
           - type: ovs_bridge
             name: {{ neutron_physical_bridge_name }}
             mtu: {{ min_viable_mtu }}
             use_dhcp: false
             dns_servers: {{ ctlplane_dns_nameservers }}
             domain: {{ dns_search_domains }}
             addresses:
             - ip_netmask: {{ ctlplane_ip }}/{{ ctlplane_cidr }}
             routes: {{ ctlplane_host_routes }}
             members:
             - type: interface
               name: nic1
               mtu: {{ min_viable_mtu }}
               # force the MAC address of the bridge to this interface
               primary: true
           {% for network in nodeset_networks %}
             - type: vlan
               mtu: {{ lookup('vars', networks_lower[network] ~ '_mtu') }}
               vlan_id: {{ lookup('vars', networks_lower[network] ~ '_vlan_id') }}
               addresses:
               - ip_netmask:
                   {{ lookup('vars', networks_lower[network] ~ '_ip') }}/{{ lookup('vars', networks_lower[network] ~ '_cidr') }}
               routes: {{ lookup('vars', networks_lower[network] ~ '_host_routes') }}
           {% endfor %}

        edpm_network_config_hide_sensitive_logs: false
        #
        # These vars are for the network config templates themselves and are
        # considered EDPM network defaults.
        neutron_physical_bridge_name: br-ctlplane
        neutron_public_interface_name: eth0

        # edpm_nodes_validation
        edpm_nodes_validation_validate_controllers_icmp: false
        edpm_nodes_validation_validate_gateway_icmp: false

        # edpm ovn-controller configuration
        edpm_ovn_bridge_mappings: &lt;bridge_mappings&gt;
        edpm_ovn_bridge: br-int
        edpm_ovn_encap_type: geneve
        ovn_monitor_all: true
        edpm_ovn_remote_probe_interval: 60000
        edpm_ovn_ofctrl_wait_before_clear: 8000

        # serve as a OVN gateway
        edpm_enable_chassis_gw: true

        timesync_ntp_servers:
        - hostname: clock.redhat.com
        - hostname: clock2.redhat.com

        edpm_bootstrap_command: |
          subscription-manager register --username {{ subscription_manager_username }} --password {{ subscription_manager_password }}
          subscription-manager release --set=9.2
          subscription-manager repos --disable=*
          subscription-manager repos --enable=rhel-9-for-x86_64-baseos-eus-rpms --enable=rhel-9-for-x86_64-appstream-eus-rpms --enable=rhel-9-for-x86_64-highavailability-eus-rpms --enable=openstack-17.1-for-rhel-9-x86_64-rpms --enable=fast-datapath-for-rhel-9-x86_64-rpms --enable=openstack-dev-preview-for-rhel-9-x86_64-rpms
          podman login -u {{ registry_username }} -p {{ registry_password }} registry.redhat.io

        gather_facts: false
        enable_debug: false
        # edpm firewall, change the allowed CIDR if needed
        edpm_sshd_configure_firewall: true
        edpm_sshd_allowed_ranges: ['192.168.122.0/24']
        # SELinux module
        edpm_selinux_mode: enforcing

        # Do not attempt OVS major upgrades here
        edpm_ovs_packages:
        - openvswitch3.1
EOF</code></pre>
</div>
</div>
</li>
<li>
<p>Ensure that the <code>ovn-controller</code> settings that are configured in the <code>OpenStackDataPlaneNodeSet</code> CR are the same as were set in the <code>Networker</code> nodes before adoption. This configuration is stored in the <code>external_ids`</code> column in the <code>Open_vSwitch</code> table in the Open vSwitch database:</p>
<div class="listingblock">
<div class="content">
<pre>ovs-vsctl list Open .
...
external_ids        : {hostname=standalone.localdomain, ovn-bridge=br-int, ovn-bridge-mappings=&lt;bridge_mappings&gt;, ovn-chassis-mac-mappings="datacentre:1e:0a:bb:e6:7c:ad", ovn-cms-options=enable-chassis-as-gw, ovn-encap-ip="172.19.0.100", ovn-encap-tos="0", ovn-encap-type=geneve, ovn-match-northd-version=False, ovn-monitor-all=True, ovn-ofctrl-wait-before-clear="8000", ovn-openflow-probe-interval="60", ovn-remote="tcp:ovsdbserver-sb.openstack.svc:6642", ovn-remote-probe-interval="60000", rundir="/var/run/openvswitch", system-id="2eec68e6-aa21-4c95-a868-31aeafc11736"}
...</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;bridge_mappings&gt;</code> with the value of the bridge mappings in your configuration, for example, <code>"datacentre:br-ctlplane"</code>.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Optional: Enable <code>neutron-metadata</code> in the <code>OpenStackDataPlaneNodeSet</code> CR:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">oc patch openstackdataplanenodeset openstack-networker --type='json' --patch='[
  {
    "op": "add",
    "path": "/spec/services/-",
    "value": "neutron-metadata"
  }]'</code></pre>
</div>
</div>
</li>
<li>
<p>Optional: Enable <code>neutron-dhcp</code> in the <code>OpenStackDataPlaneNodeSet</code> CR:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">oc patch openstackdataplanenodeset openstack-networker --type='json' --patch='[
  {
    "op": "add",
    "path": "/spec/services/-",
    "value": "neutron-dhcp"
  }]'</code></pre>
</div>
</div>
</li>
<li>
<p>Run pre-adoption validation for <code>Networker</code> nodes. This step reuses the
service that was previously created for <code>Compute</code> nodes:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Create a <code>OpenStackDataPlaneDeployment</code> CR that runs the validation only:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">oc apply -f - &lt;&lt;EOF
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneDeployment
metadata:
  name: openstack-pre-adoption-networker
spec:
  nodeSets:
  - openstack-networker
  servicesOverride:
  - pre-adoption-validation
EOF</code></pre>
</div>
</div>
<div class="paragraph">
<p>Wait for the validation to finish.</p>
</div>
</li>
<li>
<p>Confirm that all the Ansible EE pods reach a <code>Completed</code> status:</p>
<div class="listingblock">
<div class="content">
<pre># watching the pods
watch oc get pod -l app=openstackansibleee</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre># following the ansible logs with:
oc logs -l app=openstackansibleee -f --max-log-requests 20</pre>
</div>
</div>
</li>
<li>
<p>Wait for the deployment to reach the <code>Ready</code> status:</p>
<div class="listingblock">
<div class="content">
<pre>oc wait --for condition=Ready openstackdataplanedeployment/openstack-pre-adoption-networker --timeout=10m</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Deploy the <code>OpenStackDataPlaneDeployment</code> CR for <code>Networker</code> nodes:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">oc apply -f - &lt;&lt;EOF
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneDeployment
metadata:
  name: openstack-networker
spec:
  nodeSets:
  - openstack-networker
EOF</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>Note: alternatively, you could include the <code>Networker</code> specific node set in the
<code>nodeSets</code> list when creating the main, <code>openstack</code>,
<code>OpenStackDataPlaneDeployment</code> CR.</p>
</div>
<div class="paragraph">
<p>Note that once a <code>OpenStackDataPlaneDeployment</code> CR is deployed, it&#8217;s impossible
to add new node sets to it later.</p>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>Confirm that all the Ansible EE pods reach a <code>Completed</code> status:</p>
<div class="listingblock">
<div class="content">
<pre># watching the pods
watch oc get pod -l app=openstackansibleee</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre># following the ansible logs with:
oc logs -l app=openstackansibleee -f --max-log-requests 20</pre>
</div>
</div>
</li>
<li>
<p>Wait for the data plane node set to reach the <code>Ready</code> status:</p>
<div class="listingblock">
<div class="content">
<pre>oc wait --for condition=Ready osdpns/openstack-networker --timeout=30m</pre>
</div>
</div>
</li>
<li>
<p>Verify that Networking service agents are alive (the list of agents may
vary depending on the services you&#8217;ve enabled):</p>
<div class="listingblock">
<div class="content">
<pre>oc exec openstackclient -- openstack network agent list
+--------------------------------------+------------------------------+------------------------+-------------------+-------+-------+----------------------------+
| ID                                   | Agent Type                   | Host                   | Availability Zone | Alive | State | Binary                     |
+--------------------------------------+------------------------------+------------------------+-------------------+-------+-------+----------------------------+
| 174fc099-5cc9-4348-b8fc-59ed44fcfb0e | DHCP agent                   | standalone.localdomain | nova              | :-)   | UP    | neutron-dhcp-agent         |
| 10482583-2130-5b0d-958f-3430da21b929 | OVN Metadata agent           | standalone.localdomain |                   | :-)   | UP    | neutron-ovn-metadata-agent |
| a4f1b584-16f1-4937-b2b0-28102a3f6eaa | OVN Controller Gateway agent | standalone.localdomain |                   | :-)   | UP    | ovn-controller             |
+--------------------------------------+------------------------------+------------------------+-------------------+-------+-------+----------------------------+</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="migrating-the-object-storage-service_data-plane">Migrating the Object Storage service (swift) to Red&#160;Hat OpenStack Services on OpenShift (RHOSO) nodes</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This section only applies if you are using Red&#160;Hat OpenStack Platform Object Storage service (swift) as Object Storage
service. If you are using the Object Storage <strong>API</strong> of Ceph Object Gateway (RGW), you can skip this section.</p>
</div>
<div class="paragraph">
<p>Data migration to the new deployment might be a long running process that runs mostly in the background. The Object Storage service replicators will take care of moving data from old to new nodes, but depending on the amount of used storage this might take a very long time. You can still use the old nodes as long as they are running and continue with adopting other services in the meantime, reducing the amount of downtime. Note that performance might be decreased to the amount of replication traffic in the network.</p>
</div>
<div class="paragraph">
<p>Migration of the data happens replica by replica. Assuming you start with 3 replicas, only 1 one them is being moved at any time, ensuring the remaining 2 replicas are still available and the Object Storage service is usable during the migration.</p>
</div>
<div class="sect2">
<h3 id="migrating-object-storage-data-to-rhoso-nodes_migrate-object-storage-service">Migrating the Object Storage service (swift) data from RHOSP to Red&#160;Hat OpenStack Services on OpenShift (RHOSO) nodes</h3>
<div class="paragraph">
<p>To ensure availability during the Object Storage service (swift) migration, you perform the following steps:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Add new nodes to the Object Storage service rings</p>
</li>
<li>
<p>Set weights of existing nodes to 0</p>
</li>
<li>
<p>Rebalance rings, moving one replica</p>
</li>
<li>
<p>Copy rings to old nodes and restart services</p>
</li>
<li>
<p>Check replication status and repeat previous two steps until old nodes are drained</p>
</li>
<li>
<p>Remove the old nodes from the rings</p>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Previous Object Storage service adoption steps are completed.</p>
</li>
<li>
<p>No new environmental variables need to be defined, though you use the
<code>CONTROLLER1_SSH</code> alias that was defined in a previous step.</p>
</li>
<li>
<p>For DNS servers, all existing nodes must be able to resolve host names of the Red Hat OpenShift Container Platform pods, for example by using the
external IP of the DNSMasq service as name server in <code>/etc/resolv.conf</code>:</p>
<div class="listingblock">
<div class="content">
<pre>oc get service dnsmasq-dns -o jsonpath="{.status.loadBalancer.ingress[0].ip}" | CONTROLLER1_SSH tee /etc/resolv.conf</pre>
</div>
</div>
</li>
<li>
<p>To track the current status of the replication a tool called <code>swift-dispersion</code> is used. It consists of two parts, a population tool to be run before changing the Object Storage service rings and a report tool to run afterwards to gather the current status. Run the <code>swift-dispersion-populate</code> command:</p>
<div class="listingblock">
<div class="content">
<pre>oc debug --keep-labels=true job/swift-ring-rebalance -- /bin/sh -c 'swift-ring-tool get &amp;&amp; swift-dispersion-populate'</pre>
</div>
</div>
<div class="paragraph">
<p>The command might need a few minutes to complete. It creates 0-byte objects distributed across the Object Storage service deployment, and its counter-part <code>swift-dispersion-report</code> can be used afterwards to show the current replication status.</p>
</div>
<div class="paragraph">
<p>The output of the <code>swift-dispersion-report</code> command should look like the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>oc debug --keep-labels=true job/swift-ring-rebalance -- /bin/sh -c 'swift-ring-tool get &amp;&amp; swift-dispersion-report'</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>Queried 1024 containers for dispersion reporting, 5s, 0 retries
100.00% of container copies found (3072 of 3072)
Sample represents 100.00% of the container partition space
Queried 1024 objects for dispersion reporting, 4s, 0 retries
There were 1024 partitions missing 0 copies.
100.00% of object copies found (3072 of 3072)
Sample represents 100.00% of the object partition space</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Add new nodes by scaling up the SwiftStorage resource from 0 to 3. In
that case 3 storage instances using PVCs are created, running on the
Red Hat OpenShift Container Platform cluster.</p>
<div class="listingblock">
<div class="content">
<pre>oc patch openstackcontrolplane openstack --type=merge -p='{"spec":{"swift":{"template":{"swiftStorage":{"replicas": 3}}}}}'</pre>
</div>
</div>
</li>
<li>
<p>Wait until all three pods are running:</p>
<div class="listingblock">
<div class="content">
<pre>oc wait pods --for condition=Ready -l component=swift-storage</pre>
</div>
</div>
</li>
<li>
<p>Drain the existing nodes. Get the storage management IP
addresses of the nodes to drain from the current rings:</p>
<div class="listingblock">
<div class="content">
<pre>oc debug --keep-labels=true job/swift-ring-rebalance -- /bin/sh -c 'swift-ring-tool get &amp;&amp; swift-ring-builder object.builder' | tail -n +7 | awk '{print $4}' | sort -u</pre>
</div>
</div>
<div class="paragraph">
<p>The output will look similar to the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>172.20.0.100:6200
swift-storage-0.swift-storage.openstack.svc:6200
swift-storage-1.swift-storage.openstack.svc:6200
swift-storage-2.swift-storage.openstack.svc:6200</pre>
</div>
</div>
<div class="paragraph">
<p>In this case the old node 172.20.0.100 is drained. Your nodes might be
different, and depending on the deployment there are likely more nodes to be included in the following commands.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>oc debug --keep-labels=true job/swift-ring-rebalance -- /bin/sh -c '
swift-ring-tool get
swift-ring-tool drain 172.20.0.100
swift-ring-tool rebalance
swift-ring-tool push'</pre>
</div>
</div>
</li>
<li>
<p>Copy and apply the updated rings need to the original nodes. Run the
ssh commands for your existing nodes storing Object Storage service data.</p>
<div class="listingblock">
<div class="content">
<pre>oc extract --confirm cm/swift-ring-files
CONTROLLER1_SSH "tar -C /var/lib/config-data/puppet-generated/swift/etc/swift/ -xzf -" &lt; swiftrings.tar.gz
CONTROLLER1_SSH "systemctl restart tripleo_swift_*"</pre>
</div>
</div>
</li>
<li>
<p>Track the replication progress by using the <code>swift-dispersion-report</code> tool:</p>
<div class="listingblock">
<div class="content">
<pre>oc debug --keep-labels=true job/swift-ring-rebalance -- /bin/sh -c "swift-ring-tool get &amp;&amp; swift-dispersion-report"</pre>
</div>
</div>
<div class="paragraph">
<p>The output shows less than 100% of copies found. Repeat the above command until both the container and all container and object copies are found:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>Queried 1024 containers for dispersion reporting, 6s, 0 retries
There were 5 partitions missing 1 copy.
99.84% of container copies found (3067 of 3072)
Sample represents 100.00% of the container partition space
Queried 1024 objects for dispersion reporting, 7s, 0 retries
There were 739 partitions missing 1 copy.
There were 285 partitions missing 0 copies.
75.94% of object copies found (2333 of 3072)
Sample represents 100.00% of the object partition space</pre>
</div>
</div>
</li>
<li>
<p>Move the next replica to the new nodes. To do so, rebalance and distribute the rings again:</p>
<div class="listingblock">
<div class="content">
<pre>oc debug --keep-labels=true job/swift-ring-rebalance -- /bin/sh -c '
swift-ring-tool get
swift-ring-tool rebalance
swift-ring-tool push'

oc extract --confirm cm/swift-ring-files
CONTROLLER1_SSH "tar -C /var/lib/config-data/puppet-generated/swift/etc/swift/ -xzf -" &lt; swiftrings.tar.gz
CONTROLLER1_SSH "systemctl restart tripleo_swift_*"</pre>
</div>
</div>
<div class="paragraph">
<p>Monitor the <code>swift-dispersion-report</code> output again, wait until all copies are found again and repeat this step until all your replicas are moved to the new nodes.</p>
</div>
</li>
<li>
<p>After the nodes are drained, remove the nodes from the rings:</p>
<div class="listingblock">
<div class="content">
<pre>oc debug --keep-labels=true job/swift-ring-rebalance -- /bin/sh -c '
swift-ring-tool get
swift-ring-tool remove 172.20.0.100
swift-ring-tool rebalance
swift-ring-tool push'</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Verification</div>
<ul>
<li>
<p>Even if all replicas are already on the the new nodes and the
<code>swift-dispersion-report</code> command reports 100% of the copies found, there might still be data on old nodes. This data is removed by the replicators, but it might take some more time.</p>
<div class="paragraph">
<p>You can check the disk usage of all disks in the cluster:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>oc debug --keep-labels=true job/swift-ring-rebalance -- /bin/sh -c 'swift-ring-tool get &amp;&amp; swift-recon -d'</pre>
</div>
</div>
</li>
<li>
<p>Confirm that there are no more <code>\*.db</code> or <code>*.data</code> files in the directory <code>/srv/node</code> on these nodes:</p>
<div class="listingblock">
<div class="content">
<pre>CONTROLLER1_SSH "find /srv/node/ -type f -name '*.db' -o -name '*.data' | wc -l"</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="troubleshooting-object-storage-migration_migrate-object-storage-service">Troubleshooting the Object Storage service (swift) migration</h3>
<div class="paragraph">
<p>You can troubleshoot issues with the Object Storage service (swift) migration.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The following command might be helpful to debug if the replication is not working and the <code>swift-dispersion-report</code> is not back to 100% availability.</p>
<div class="listingblock">
<div class="content">
<pre>CONTROLLER1_SSH tail /var/log/containers/swift/swift.log | grep object-server</pre>
</div>
</div>
<div class="paragraph">
<p>This should show the replicator progress, for example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>Mar 14 06:05:30 standalone object-server[652216]: &lt;f+++++++++ 4e2/9cbea55c47e243994b0b10d8957184e2/1710395823.58025.data
Mar 14 06:05:30 standalone object-server[652216]: Successful rsync of /srv/node/vdd/objects/626/4e2 to swift-storage-1.swift-storage.openstack.svc::object/d1/objects/626 (0.094)
Mar 14 06:05:30 standalone object-server[652216]: Removing partition: /srv/node/vdd/objects/626
Mar 14 06:05:31 standalone object-server[652216]: &lt;f+++++++++ 85f/cf53b5a048e5b19049e05a548cde185f/1710395796.70868.data
Mar 14 06:05:31 standalone object-server[652216]: Successful rsync of /srv/node/vdb/objects/829/85f to swift-storage-2.swift-storage.openstack.svc::object/d1/objects/829 (0.095)
Mar 14 06:05:31 standalone object-server[652216]: Removing partition: /srv/node/vdb/objects/829</pre>
</div>
</div>
</li>
<li>
<p>You can also check the ring consistency and replicator status:</p>
<div class="listingblock">
<div class="content">
<pre>oc debug --keep-labels=true job/swift-ring-rebalance -- /bin/sh -c 'swift-ring-tool get &amp;&amp; swift-recon -r --md5'</pre>
</div>
</div>
<div class="paragraph">
<p>Note that the output might show a md5 mismatch until approx. 2 minutes after pushing new rings. Eventually it looks similar to the following example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>[...]
Oldest completion was 2024-03-14 16:53:27 (3 minutes ago) by 172.20.0.100:6000.
Most recent completion was 2024-03-14 16:56:38 (12 seconds ago) by swift-storage-0.swift-storage.openstack.svc:6200.
===============================================================================
[2024-03-14 16:56:50] Checking ring md5sums
4/4 hosts matched, 0 error[s] while checking hosts.
[...]</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="ceph-migration_migrate-object-storage-service">Migrating the Red Hat Ceph Storage Cluster</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In the context of data plane adoption, where the Red&#160;Hat OpenStack Platform
(RHOSP) services are redeployed in Red Hat OpenShift Container Platform, you migrate a
director-deployed Red Hat Ceph Storage cluster by using a process
called externalizing the Red Hat Ceph Storage cluster.</p>
</div>
<div class="paragraph">
<p>There are two deployment topologies that include an internal Red Hat Ceph Storage
cluster:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>RHOSP includes dedicated Red Hat Ceph Storage nodes to host object
storage daemons (OSDs)</p>
</li>
<li>
<p>Hyperconverged Infrastructure (HCI), where Compute and Storage services are
colocated on hyperconverged nodes</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In either scenario, there are some Red Hat Ceph Storage processes that are deployed on
RHOSP Controller nodes: Red Hat Ceph Storage monitors, Ceph Object Gateway (RGW),
Rados Block Device (RBD), Ceph Metadata Server (MDS), Ceph Dashboard, and NFS
Ganesha. To migrate your Red Hat Ceph Storage cluster, you must decommission the
Controller nodes and move the Red Hat Ceph Storage daemons to a set of target nodes that are
already part of the Red Hat Ceph Storage cluster.</p>
</div>
<div class="sect2">
<h3 id="ceph-daemon-cardinality_migrate-object-storage-service">Red Hat Ceph Storage daemon cardinality</h3>
<div class="paragraph">
<p>Red Hat Ceph Storage 6 and later applies strict constraints in the way daemons can be
colocated within the same node.
For more information, see <a href="https://access.redhat.com/articles/1548993">Red Hat Ceph Storage: Supported configurations</a>.
The resulting topology depends on the available hardware, as well as the amount
of Red Hat Ceph Storage services present in the Controller nodes that are going to be
retired.
As a general rule, the number of services that can be migrated depends on the
number of available nodes in the cluster. The following diagrams cover the
distribution of the Red Hat Ceph Storage daemons on the Red Hat Ceph Storage nodes where at least three
nodes are required in a scenario that includes only RGW and RBD, without the
Red Hat Ceph Storage Dashboard:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>|    |                     |             |
|----|---------------------|-------------|
| osd | mon/mgr/crash      | rgw/ingress |
| osd | mon/mgr/crash      | rgw/ingress |
| osd | mon/mgr/crash      | rgw/ingress |</pre>
</div>
</div>
<div class="paragraph">
<p>With the Dashboard service, and without Shared File Systems service (manila), at
least 4 nodes are required. The Red Hat Ceph Storage dashboard has no failover:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>|     |                     |             |
|-----|---------------------|-------------|
| osd | mon/mgr/crash | rgw/ingress       |
| osd | mon/mgr/crash | rgw/ingress       |
| osd | mon/mgr/crash | dashboard/grafana |
| osd | rgw/ingress   | (free)            |</pre>
</div>
</div>
<div class="paragraph">
<p>With the Red Hat Ceph Storage dashboard and the Shared File Systems service, 5 nodes
minimum are required, and the Red Hat Ceph Storage dashboard has no failover:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>|     |                     |                         |
|-----|---------------------|-------------------------|
| osd | mon/mgr/crash       | rgw/ingress             |
| osd | mon/mgr/crash       | rgw/ingress             |
| osd | mon/mgr/crash       | mds/ganesha/ingress     |
| osd | rgw/ingress         | mds/ganesha/ingress     |
| osd | mds/ganesha/ingress | dashboard/grafana       |</pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="migrating-ceph-monitoring_migrate-object-storage-service">Migrating the monitoring stack component to new nodes within an existing Red Hat Ceph Storage cluster</h3>
<div class="paragraph">
<p>The Ceph Dashboard module adds web-based monitoring and administration to the
Ceph Manager.
With director-deployed Red Hat Ceph Storage, this component is enabled as
part of the overcloud deploy and it is composed of the following:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Ceph Manager module</p>
</li>
<li>
<p>Grafana</p>
</li>
<li>
<p>Prometheus</p>
</li>
<li>
<p>Alertmanager</p>
</li>
<li>
<p>Node exporter</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The Ceph Dashboard containers are included through
<code>tripleo-container-image-prepare</code> parameters and the high availability relies
on <code>Haproxy</code> and <code>Pacemaker</code> deployed on the RHOSP front. For an
external Red Hat Ceph Storage cluster, high availability is not supported. The goal of
this procedure is to migrate and relocate the Ceph Monitoring components to
free Controller nodes.</p>
</div>
<div class="paragraph">
<p>For this procedure, we assume that we are beginning with a RHOSP
based on 17.1 and a Red Hat Ceph Storage 7 deployment managed by
director. We assume that:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Red Hat Ceph Storage has been upgraded to 7 and is managed by
cephadm/orchestrator</p>
</li>
<li>
<p>Both the Red Hat Ceph Storage public and cluster networks are propagated,
through director, to the target nodes</p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="completing-prerequisites-for-migrating-ceph-monitoring-stack_migrating-ceph-monitoring">Completing prerequisites for a Red Hat Ceph Storage cluster with monitoring stack components</h4>
<div class="paragraph">
<p>You must complete the following prerequisites before you migrate a Red Hat Ceph Storage cluster with monitoring stack components.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Gather the current status of the monitoring stack. Verify that
the hosts have no <code>monitoring</code> label (or <code>grafana</code>, <code>prometheus</code>, <code>alertmanager</code>
in case of a per daemons placement evaluation) associated:</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The entire relocation process is driven by cephadm and relies on labels to be
assigned to the target nodes, where the daemons are scheduled.
Review the <a href="https://access.redhat.com/articles/1548993">cardinality matrix</a> before assigning labels and choose carefully the nodes where the monitoring
stack components should be scheduled on.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre>[tripleo-admin@controller-0 ~]$ sudo cephadm shell -- ceph orch host ls

HOST                    	ADDR       	LABELS                 	STATUS
cephstorage-0.redhat.local  192.168.24.11  osd mds
cephstorage-1.redhat.local  192.168.24.12  osd mds
cephstorage-2.redhat.local  192.168.24.47  osd mds
controller-0.redhat.local   192.168.24.35  _admin mon mgr
controller-1.redhat.local   192.168.24.53  mon _admin mgr
controller-2.redhat.local   192.168.24.10  mon _admin mgr
6 hosts in cluster</pre>
</div>
</div>
<div class="paragraph">
<p>Confirm that the cluster is healthy and both <code>ceph orch ls</code> and
<code>ceph orch ps</code> return the expected number of deployed daemons.</p>
</div>
</li>
<li>
<p>Review and update the container image registry. If the Red Hat Ceph Storage externalization procedure is executed after the Red&#160;Hat OpenStack Platform control
plane has been migrated, its important to consider updating the container
images referenced in the Red Hat Ceph Storage cluster config. The current container images point to the undercloud registry, and it might be no longer available. As the undercloud wont be available in the future, replace the undercloud provided
images with an alternative registry.</p>
<div class="listingblock">
<div class="content">
<pre>$ ceph config dump
...
...
mgr   advanced  mgr/cephadm/container_image_alertmanager    undercloud-0.ctlplane.redhat.local:8787/rh-osbs/openshift-ose-prometheus-alertmanager:v4.10
mgr   advanced  mgr/cephadm/container_image_base            undercloud-0.ctlplane.redhat.local:8787/rh-osbs/rhceph
mgr   advanced  mgr/cephadm/container_image_grafana         undercloud-0.ctlplane.redhat.local:8787/rh-osbs/grafana:latest
mgr   advanced  mgr/cephadm/container_image_node_exporter   undercloud-0.ctlplane.redhat.local:8787/rh-osbs/openshift-ose-prometheus-node-exporter:v4.10
mgr   advanced  mgr/cephadm/container_image_prometheus      undercloud-0.ctlplane.redhat.local:8787/rh-osbs/openshift-ose-prometheus:v4.10</pre>
</div>
</div>
</li>
<li>
<p>Remove the undercloud container images:</p>
<div class="listingblock">
<div class="content">
<pre># remove the base image
cephadm shell -- ceph config rm mgr mgr/cephadm/container_image_base
# remove the undercloud images associated to the monitoring
# stack components
for i in prometheus grafana alertmanager node_exporter; do
    cephadm shell -- ceph config rm mgr mgr/cephadm/container_image_$i
done</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
In the example above, in addition to the monitoring stack related
container images, we update the config entry related to the
container_image_base. This has an impact on all the Red Hat Ceph Storage daemons that rely on
the undercloud images.
New daemons will be deployed using the new/default Red Hat Ceph Storage image.
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="migrating-monitoring-stack-to-target-nodes_migrating-ceph-monitoring">Migrating the monitoring stack to the target nodes</h4>
<div class="paragraph">
<p>The migration procedure relies on nodes re-labeling: this kind of action,
combined with an update in the existing spec, results in the daemons'
relocation on the target nodes.</p>
</div>
<div class="paragraph">
<p>Before start this process, a few considerations are required:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Theres no need to migrate node exporters: these daemons are deployed across
the nodes that are part of the Red Hat Ceph Storage cluster (placement is *), and  were going to lose metrics as long as the Controller nodes are not part of the Red Hat Ceph Storage cluster anymore</p>
</li>
<li>
<p>Each monitoring stack component is bound to specific ports that director is
supposed to open beforehand; make sure to double check the firewall rules are
in place and the ports are opened for a given monitoring stack service</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Depending on the target nodes and the number of deployed/active daemons, it is
possible to either relocate the existing containers to the target nodes, or
select a subset of nodes that are supposed to host the monitoring stack
daemons. As we mentioned in the previous section, HA is not supported, hence
reducing the placement with <code>count: 1</code> is a reasonable solution and allows to
successfully migrate the existing daemons in an HCI (or HW limited) scenario
without impacting other services.
However, it is still possible to put in place a dedicated HA solution and
realize a component that is consistent with the director model to reach HA.
Building and deployment such HA model is out of scope for this procedure.</p>
</div>
<div class="sect4">
<h5 id="migrating-existing-daemons-to-target-nodes_migrating-monitoring-stack">Scenario 1: Migrating the existing daemons to the target nodes</h5>
<div class="paragraph">
<p>Assuming we have 3 Red Hat Ceph Storage nodes or ComputeHCI, this scenario extends the
monitoring labels to all the Red Hat Ceph Storage (or ComputeHCI) nodes that are part
of the cluster. This means that we keep the count: 3 placements for the target
nodes.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Add the monitoring label to all the Red Hat Ceph Storage (or ComputeHCI) nodes in the cluster:</p>
<div class="listingblock">
<div class="content">
<pre>for item in $(sudo cephadm shell --  ceph orch host ls --format json | jq -r '.[].hostname'); do
    sudo cephadm shell -- ceph orch host label add  $item monitoring;
done</pre>
</div>
</div>
</li>
<li>
<p>Verify that all the (three) hosts have the monitoring label:</p>
<div class="listingblock">
<div class="content">
<pre>[tripleo-admin@controller-0 ~]$ sudo cephadm shell -- ceph orch host ls

HOST                        ADDR           LABELS
cephstorage-0.redhat.local  192.168.24.11  osd monitoring
cephstorage-1.redhat.local  192.168.24.12  osd monitoring
cephstorage-2.redhat.local  192.168.24.47  osd monitoring
controller-0.redhat.local   192.168.24.35  _admin mon mgr monitoring
controller-1.redhat.local   192.168.24.53  mon _admin mgr monitoring
controller-2.redhat.local   192.168.24.10  mon _admin mgr monitoring</pre>
</div>
</div>
</li>
<li>
<p>Remove the labels from the Controller nodes:</p>
<div class="listingblock">
<div class="content">
<pre>$ for i in 0 1 2; do ceph orch host label rm "controller-$i.redhat.local" monitoring; done

Removed label monitoring from host controller-0.redhat.local
Removed label monitoring from host controller-1.redhat.local
Removed label monitoring from host controller-2.redhat.local</pre>
</div>
</div>
</li>
<li>
<p>Dump the current monitoring stack spec:</p>
<div class="listingblock">
<div class="content">
<pre>function export_spec {
    local component="$1"
    local target_dir="$2"
    sudo cephadm shell -- ceph orch ls --export "$component" &gt; "$target_dir/$component"
}

SPEC_DIR=${SPEC_DIR:-"$PWD/ceph_specs"}
for m in grafana prometheus alertmanager; do
    export_spec "$m" "$SPEC_DIR"
done</pre>
</div>
</div>
</li>
<li>
<p>For each daemon, edit the current spec and replace the placement/hosts section
with the placement/label section, for example:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">service_type: grafana
service_name: grafana
placement:
  label: monitoring
networks:
- 172.17.3.0/24
spec:
  port: 3100</code></pre>
</div>
</div>
<div class="paragraph">
<p>The same procedure applies to Prometheus and Alertmanager specs.</p>
</div>
</li>
<li>
<p>Apply the new monitoring spec to relocate the monitoring stack daemons:</p>
<div class="listingblock">
<div class="content">
<pre>SPEC_DIR=${SPEC_DIR:-"$PWD/ceph_specs"}
function migrate_daemon {
    local component="$1"
    local target_dir="$2"
    sudo cephadm shell -m "$target_dir" -- ceph orch apply -i /mnt/ceph_specs/$component
}
for m in grafana prometheus alertmanager; do
    migrate_daemon  "$m" "$SPEC_DIR"
done</pre>
</div>
</div>
</li>
<li>
<p>Verify that the daemons are deployed on the expected nodes:</p>
<div class="listingblock">
<div class="content">
<pre>[ceph: root@controller-0 /]# ceph orch ps | grep -iE "(prome|alert|grafa)"
alertmanager.cephstorage-2  cephstorage-2.redhat.local  172.17.3.144:9093,9094
grafana.cephstorage-0       cephstorage-0.redhat.local  172.17.3.83:3100
prometheus.cephstorage-1    cephstorage-1.redhat.local  172.17.3.53:9092</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
After you migrate the monitoring stack, you lose High Availability: the monitoring stack daemons have no VIP and HAproxy anymore; Node exporters are still running on all the nodes: instead of using labels we keep the current approach as we want to not reduce the monitoring space covered.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>You must review the Red Hat Ceph Storageconfiguration to ensure that it is aligned with the relocation you just made. In particular, focus on the following configuration entries:</p>
<div class="listingblock">
<div class="content">
<pre>[ceph: root@controller-0 /]# ceph config dump
...
mgr  advanced  mgr/dashboard/ALERTMANAGER_API_HOST  http://172.17.3.83:9093
mgr  advanced  mgr/dashboard/GRAFANA_API_URL        https://172.17.3.144:3100
mgr  advanced  mgr/dashboard/PROMETHEUS_API_HOST    http://172.17.3.83:9092
mgr  advanced  mgr/dashboard/controller-0.ycokob/server_addr  172.17.3.33
mgr  advanced  mgr/dashboard/controller-1.lmzpuc/server_addr  172.17.3.147
mgr  advanced  mgr/dashboard/controller-2.xpdgfl/server_addr  172.17.3.138</pre>
</div>
</div>
</li>
<li>
<p>Verify that <code>grafana</code>, <code>alertmanager</code> and <code>prometheus</code> <code>API_HOST/URL</code> point to
the IP addresses (on the storage network) of the node where each daemon has been
relocated. This should be automatically addressed by cephadm and it shouldnt
require any manual action.</p>
<div class="listingblock">
<div class="content">
<pre>[ceph: root@controller-0 /]# ceph orch ps | grep -iE "(prome|alert|grafa)"
alertmanager.cephstorage-0  cephstorage-0.redhat.local  172.17.3.83:9093,9094
alertmanager.cephstorage-1  cephstorage-1.redhat.local  172.17.3.53:9093,9094
alertmanager.cephstorage-2  cephstorage-2.redhat.local  172.17.3.144:9093,9094
grafana.cephstorage-0       cephstorage-0.redhat.local  172.17.3.83:3100
grafana.cephstorage-1       cephstorage-1.redhat.local  172.17.3.53:3100
grafana.cephstorage-2       cephstorage-2.redhat.local  172.17.3.144:3100
prometheus.cephstorage-0    cephstorage-0.redhat.local  172.17.3.83:9092
prometheus.cephstorage-1    cephstorage-1.redhat.local  172.17.3.53:9092
prometheus.cephstorage-2    cephstorage-2.redhat.local  172.17.3.144:9092</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>[ceph: root@controller-0 /]# ceph config dump
...
...
mgr  advanced  mgr/dashboard/ALERTMANAGER_API_HOST   http://172.17.3.83:9093
mgr  advanced  mgr/dashboard/PROMETHEUS_API_HOST     http://172.17.3.83:9092
mgr  advanced  mgr/dashboard/GRAFANA_API_URL         https://172.17.3.144:3100</pre>
</div>
</div>
</li>
<li>
<p>The Ceph Dashboard (mgr module plugin) has not been impacted at all by this
relocation. The service is provided by the Ceph Manager daemon, hence we might
experience an impact when the active mgr is migrated or is force-failed.
However, having three replicas definition allows to redirect requests to a
different instance (its still an A/P model), hence the impact should be
limited.</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>When the RBD migration is over, the following Red Hat Ceph Storage config keys must
be regenerated to point to the right mgr container:</p>
<div class="listingblock">
<div class="content">
<pre>mgr    advanced  mgr/dashboard/controller-0.ycokob/server_addr  172.17.3.33
mgr    advanced  mgr/dashboard/controller-1.lmzpuc/server_addr  172.17.3.147
mgr    advanced  mgr/dashboard/controller-2.xpdgfl/server_addr  172.17.3.138</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell
$ ceph orch ps | awk '/mgr./ {print $1}'</pre>
</div>
</div>
</li>
<li>
<p>For each retrieved mgr, update the entry in the Red Hat Ceph Storage configuration:</p>
<div class="listingblock">
<div class="content">
<pre>$ ceph config set mgr mgr/dashboard/&lt;&gt;/server_addr/&lt;ip addr&gt;</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="migrating-ceph-mds_migrating-monitoring-stack">Migrating Red Hat Ceph Storage MDS to new nodes within the existing cluster</h3>
<div class="paragraph">
<p>In the context of data plane adoption, where the Red&#160;Hat OpenStack Platform (RHOSP) services are
redeployed in Red Hat OpenShift Container Platform, a director-deployed Red Hat Ceph Storage cluster will undergo a migration in a process we are calling externalizing the Red Hat Ceph Storage cluster.
There are two deployment topologies, broadly, that include an internal Red Hat Ceph Storage cluster today: one is where RHOSP includes dedicated Red Hat Ceph Storage nodes to host object storage daemons (OSDs), and the other is Hyperconverged Infrastructure (HCI) where Compute nodes double up as Red Hat Ceph Storage nodes. In either scenario, there are some Red Hat Ceph Storage processes that are deployed on RHOSP Controller nodes: Red Hat Ceph Storage monitors, Ceph Object Gateway (RGW), Rados Block Device (RBD), Ceph Metadata Server (MDS), Ceph Dashboard, and NFS Ganesha.
This document describes how to migrate the MDS daemon in case Shared File Systems service (manila) (deployed with either a cephfs-native or ceph-nfs backend) is part of the overcloud deployment. The MDS migration is performed by cephadm, and as done for the other daemons, the general idea is to move the daemons placement from a "hosts" based approach to a "label" based one.
This ensures that the human operator can easily visualize the status of the cluster and where daemons are placed using the <code>ceph orch host</code> command, and have a general view of how the daemons are co-located within a given host, according to the <a href="https://access.redhat.com/articles/1548993">cardinality matrix</a>.</p>
</div>
<div class="paragraph">
<p>For this procedure, we assume that we are beginning with a RHOSP based on 17.1 and a Red Hat Ceph Storage 7 deployment managed by director.
We assume that:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Red Hat Ceph Storage is upgraded to Red Hat Ceph Storage 7 and is managed by cephadm/orchestrator.</p>
</li>
<li>
<p>Both the Red Hat Ceph Storage public and cluster networks are propagated, throughdirector, to the target nodes.</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Verify that the Red Hat Ceph Storage cluster is healthy and check the MDS status:</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre>[ceph: root@controller-0 /]# ceph fs ls
name: cephfs, metadata pool: manila_metadata, data pools: [manila_data ]

[ceph: root@controller-0 /]# ceph mds stat
cephfs:1 {0=mds.controller-2.oebubl=up:active} 2 up:standby

[ceph: root@controller-0 /]# ceph fs status cephfs

cephfs - 0 clients
======
RANK  STATE         	MDS           	ACTIVITY 	DNS	INOS   DIRS   CAPS
 0	active  mds.controller-2.oebubl  Reqs:	0 /s   696	196	173  	0
  	POOL     	TYPE 	USED  AVAIL
manila_metadata  metadata   152M   141G
  manila_data  	data	3072M   141G
  	STANDBY MDS
mds.controller-0.anwiwd
mds.controller-1.cwzhog
MDS version: ceph version 17.2.6-100.el9cp (ea4e3ef8df2cf26540aae06479df031dcfc80343) quincy (stable)</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Retrieve more detailed information on the Ceph File System (CephFS) MDS status:</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre>[ceph: root@controller-0 /]# ceph fs dump

e8
enable_multiple, ever_enabled_multiple: 1,1
default compat: compat={},rocompat={},incompat={1=base v0.20,2=client writeable ranges,3=default file layouts on dirs,4=dir inode in separate object,5=mds uses versioned encoding,6=dirfrag is stored in omap,8=no anchor table,9=file layout v2,10=snaprealm v2}
legacy client fscid: 1

Filesystem 'cephfs' (1)
fs_name cephfs
epoch   5
flags   12 joinable allow_snaps allow_multimds_snaps
created 2024-01-18T19:04:01.633820+0000
modified    	2024-01-18T19:04:05.393046+0000
tableserver 	0
root	0
session_timeout 60
session_autoclose   	300
max_file_size   1099511627776
required_client_features    	{}
last_failure	0
last_failure_osd_epoch  0
compat  compat={},rocompat={},incompat={1=base v0.20,2=client writeable ranges,3=default file layouts on dirs,4=dir inode in separate object,5=mds uses versioned encoding,6=dirfrag is stored in omap,7=mds uses inline data,8=no anchor table,9=file layout v2,10=snaprealm v2}
max_mds 1
in  	0
up  	{0=24553}
failed
damaged
stopped
data_pools  	[7]
metadata_pool   9
inline_data 	disabled
balancer
standby_count_wanted	1
[mds.mds.controller-2.oebubl{0:24553} state up:active seq 2 addr [v2:172.17.3.114:6800/680266012,v1:172.17.3.114:6801/680266012] compat {c=[1],r=[1],i=[7ff]}]


Standby daemons:

[mds.mds.controller-0.anwiwd{-1:14715} state up:standby seq 1 addr [v2:172.17.3.20:6802/3969145800,v1:172.17.3.20:6803/3969145800] compat {c=[1],r=[1],i=[7ff]}]
[mds.mds.controller-1.cwzhog{-1:24566} state up:standby seq 1 addr [v2:172.17.3.43:6800/2227381308,v1:172.17.3.43:6801/2227381308] compat {c=[1],r=[1],i=[7ff]}]
dumped fsmap epoch 8</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Check the OSD blocklist and clean up the client list:</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre>[ceph: root@controller-0 /]# ceph osd blocklist ls
..
..
for item in $(ceph osd blocklist ls | awk '{print $0}'); do
     ceph osd blocklist rm $item;
done</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
When a file system client is unresponsive or misbehaving, it may happen that
the access to the file system is forcibly terminated. This process is called
eviction. Evicting a CephFS client prevents it from communicating further with MDS daemons and OSD daemons.
Ordinarily, a blocklisted client may not reconnect to the servers: it must be unmounted and then remounted. However, in some situations it may be useful to permit a client that was evicted to attempt to reconnect. Because CephFS uses the RADOS OSD blocklist to control client eviction, CephFS clients can be permitted to reconnect by removing them from the blocklist.
</td>
</tr>
</table>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Get the hosts that are currently part of the Red Hat Ceph Storage cluster:</p>
<div class="listingblock">
<div class="content">
<pre>[ceph: root@controller-0 /]# ceph orch host ls
HOST                        ADDR           LABELS          STATUS
cephstorage-0.redhat.local  192.168.24.25  osd mds
cephstorage-1.redhat.local  192.168.24.50  osd mds
cephstorage-2.redhat.local  192.168.24.47  osd mds
controller-0.redhat.local   192.168.24.24  _admin mgr mon
controller-1.redhat.local   192.168.24.42  mgr _admin mon
controller-2.redhat.local   192.168.24.37  mgr _admin mon
6 hosts in cluster

[ceph: root@controller-0 /]# ceph orch ls --export mds
service_type: mds
service_id: mds
service_name: mds.mds
placement:
  hosts:
  - controller-0.redhat.local
  - controller-1.redhat.local
  - controller-2.redhat.local</pre>
</div>
</div>
</li>
<li>
<p>Extend the MDS labels to the target nodes:</p>
<div class="listingblock">
<div class="content">
<pre>for item in $(sudo cephadm shell --  ceph orch host ls --format json | jq -r '.[].hostname'); do
    sudo cephadm shell -- ceph orch host label add  $item mds;
done</pre>
</div>
</div>
</li>
<li>
<p>Verify all the hosts have the MDS label:</p>
<div class="listingblock">
<div class="content">
<pre>[tripleo-admin@controller-0 ~]$ sudo cephadm shell -- ceph orch host ls

HOST                    	ADDR       	   LABELS
cephstorage-0.redhat.local  192.168.24.11  osd mds
cephstorage-1.redhat.local  192.168.24.12  osd mds
cephstorage-2.redhat.local  192.168.24.47  osd mds
controller-0.redhat.local   192.168.24.35  _admin mon mgr mds
controller-1.redhat.local   192.168.24.53  mon _admin mgr mds
controller-2.redhat.local   192.168.24.10  mon _admin mgr mds</pre>
</div>
</div>
</li>
<li>
<p>Dump the current MDS spec:</p>
<div class="listingblock">
<div class="content">
<pre>[ceph: root@controller-0 /]# ceph orch ls --export mds &gt; mds.yaml</pre>
</div>
</div>
</li>
<li>
<p>Edit the retrieved spec and replace the <code>placement.hosts</code> section with
<code>placement.label</code>:</p>
<div class="listingblock">
<div class="content">
<pre>service_type: mds
service_id: mds
service_name: mds.mds
placement:
  label: mds</pre>
</div>
</div>
</li>
<li>
<p>Use the <code>ceph orchestrator</code> to apply the new MDS spec: it results in an
increased number of mds daemons:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -m mds.yaml -- ceph orch apply -i /mnt/mds.yaml
Scheduling new mds deployment </pre>
</div>
</div>
</li>
<li>
<p>Check the new standby daemons temporarily added to the cephfs fs:</p>
<div class="listingblock">
<div class="content">
<pre>$ ceph fs dump

Active

standby_count_wanted    1
[mds.mds.controller-0.awzplm{0:463158} state up:active seq 307 join_fscid=1 addr [v2:172.17.3.20:6802/51565420,v1:172.17.3.20:6803/51565420] compat {c=[1],r=[1],i=[7ff]}]


Standby daemons:

[mds.mds.cephstorage-1.jkvomp{-1:463800} state up:standby seq 1 join_fscid=1 addr [v2:172.17.3.135:6820/2075903648,v1:172.17.3.135:6821/2075903648] compat {c=[1],r=[1],i=[7ff]}]
[mds.mds.controller-2.gfrqvc{-1:475945} state up:standby seq 1 addr [v2:172.17.3.114:6800/2452517189,v1:172.17.3.114:6801/2452517189] compat {c=[1],r=[1],i=[7ff]}]
[mds.mds.cephstorage-0.fqcshx{-1:476503} state up:standby seq 1 join_fscid=1 addr [v2:172.17.3.92:6820/4120523799,v1:172.17.3.92:6821/4120523799] compat {c=[1],r=[1],i=[7ff]}]
[mds.mds.cephstorage-2.gnfhfe{-1:499067} state up:standby seq 1 addr [v2:172.17.3.79:6820/2448613348,v1:172.17.3.79:6821/2448613348] compat {c=[1],r=[1],i=[7ff]}]
[mds.mds.controller-1.tyiziq{-1:499136} state up:standby seq 1 addr [v2:172.17.3.43:6800/3615018301,v1:172.17.3.43:6801/3615018301] compat {c=[1],r=[1],i=[7ff]}]</pre>
</div>
</div>
</li>
<li>
<p>To migrate MDS to the right nodes, set the MDS affinity that manages the MDS failover:</p>
<div class="listingblock">
<div class="content">
<pre>ceph config set mds.mds.cephstorage-0.fqcshx mds_join_fs cephfs</pre>
</div>
</div>
</li>
<li>
<p>Remove the labels from Controller nodes and force the MDS failover to the
target node:</p>
<div class="listingblock">
<div class="content">
<pre>$ for i in 0 1 2; do ceph orch host label rm "controller-$i.redhat.local" mds; done

Removed label mds from host controller-0.redhat.local
Removed label mds from host controller-1.redhat.local
Removed label mds from host controller-2.redhat.local</pre>
</div>
</div>
<div class="paragraph">
<p>The switch happens behind the scenes, and the new active MDS is the one that
you set through the <code>mds_join_fs</code> command.</p>
</div>
</li>
<li>
<p>Check the result of the failover and the new deployed daemons:</p>
<div class="listingblock">
<div class="content">
<pre>$ ceph fs dump


standby_count_wanted    1
[mds.mds.cephstorage-0.fqcshx{0:476503} state up:active seq 168 join_fscid=1 addr [v2:172.17.3.92:6820/4120523799,v1:172.17.3.92:6821/4120523799] compat {c=[1],r=[1],i=[7ff]}]


Standby daemons:

[mds.mds.cephstorage-2.gnfhfe{-1:499067} state up:standby seq 1 addr [v2:172.17.3.79:6820/2448613348,v1:172.17.3.79:6821/2448613348] compat {c=[1],r=[1],i=[7ff]}]
[mds.mds.cephstorage-1.jkvomp{-1:499760} state up:standby seq 1 join_fscid=1 addr [v2:172.17.3.135:6820/452139733,v1:172.17.3.135:6821/452139733] compat {c=[1],r=[1],i=[7ff]}]


$ ceph orch ls

NAME                     PORTS   RUNNING  REFRESHED  AGE  PLACEMENT
crash                                6/6  10m ago    10d  *
mds.mds                          3/3  10m ago    32m  label:mds


$ ceph orch ps | grep mds


mds.mds.cephstorage-0.fqcshx  cephstorage-0.redhat.local                     running (79m)     3m ago  79m    27.2M        -  17.2.6-100.el9cp  1af7b794f353  2a2dc5ba6d57
mds.mds.cephstorage-1.jkvomp  cephstorage-1.redhat.local                     running (79m)     3m ago  79m    21.5M        -  17.2.6-100.el9cp  1af7b794f353  7198b87104c8
mds.mds.cephstorage-2.gnfhfe  cephstorage-2.redhat.local                     running (79m)     3m ago  79m    24.2M        -  17.2.6-100.el9cp  1af7b794f353  f3cb859e2a15</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="migrating-ceph-rgw_migrating-monitoring-stack">Migrating Red Hat Ceph Storage RGW to external RHEL nodes</h3>
<div class="paragraph">
<p>For hyperconverged infrastructure (HCI) or dedicated Storage nodes that are running Red Hat Ceph Storage version 6 or later, you must migrate the RGW daemons that are included in the Red&#160;Hat OpenStack Platform Controller nodes into the existing external Red Hat Enterprise Linux (RHEL) nodes. The existing external RHEL nodes typically include the Compute nodes for an HCI environment or Red Hat Ceph Storage nodes.</p>
</div>
<div class="paragraph">
<p>To migrate Ceph Object Gateway (RGW), your environment must meet the following requirements:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Red Hat Ceph Storage is running version 6 or later and is managed by cephadm/orchestrator.</p>
</li>
<li>
<p>An undercloud is still available, and the nodes and networks are managed by director.</p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="completing-prerequisites-for-migrating-ceph-rgw_migrating-ceph-rgw">Completing prerequisites for migrating Red Hat Ceph Storage RGW</h4>
<div class="paragraph">
<p>You must complete the following prerequisites before you begin the Red Hat Ceph Storage RGW migration.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Check the current status of the Red Hat Ceph Storage nodes:</p>
<div class="listingblock">
<div class="content">
<pre>(undercloud) [stack@undercloud-0 ~]$ metalsmith list


    +------------------------+    +----------------+
    | IP Addresses           |    |  Hostname      |
    +------------------------+    +----------------+
    | ctlplane=192.168.24.25 |    | cephstorage-0  |
    | ctlplane=192.168.24.10 |    | cephstorage-1  |
    | ctlplane=192.168.24.32 |    | cephstorage-2  |
    | ctlplane=192.168.24.28 |    | compute-0      |
    | ctlplane=192.168.24.26 |    | compute-1      |
    | ctlplane=192.168.24.43 |    | controller-0   |
    | ctlplane=192.168.24.7  |    | controller-1   |
    | ctlplane=192.168.24.41 |    | controller-2   |
    +------------------------+    +----------------+</pre>
</div>
</div>
</li>
<li>
<p>Log in to <code>controller-0</code> and check the <code>pacemaker</code> status to help you
identify the information that you need before you start the RGW migration.</p>
<div class="listingblock">
<div class="content">
<pre>Full List of Resources:
  * ip-192.168.24.46	(ocf:heartbeat:IPaddr2):     	Started controller-0
  * ip-10.0.0.103   	(ocf:heartbeat:IPaddr2):     	Started controller-1
  * ip-172.17.1.129 	(ocf:heartbeat:IPaddr2):     	Started controller-2
  * ip-172.17.3.68  	(ocf:heartbeat:IPaddr2):     	Started controller-0
  * ip-172.17.4.37  	(ocf:heartbeat:IPaddr2):     	Started controller-1
  * Container bundle set: haproxy-bundle

[undercloud-0.ctlplane.redhat.local:8787/rh-osbs/rhosp17-openstack-haproxy:pcmklatest]:
    * haproxy-bundle-podman-0   (ocf:heartbeat:podman):  Started controller-2
    * haproxy-bundle-podman-1   (ocf:heartbeat:podman):  Started controller-0
    * haproxy-bundle-podman-2   (ocf:heartbeat:podman):  Started controller-1</pre>
</div>
</div>
</li>
<li>
<p>Use the <code>ip</code> command to identify the ranges of the storage networks.</p>
<div class="listingblock">
<div class="content">
<pre>[heat-admin@controller-0 ~]$ ip -o -4 a

1: lo	inet 127.0.0.1/8 scope host lo\   	valid_lft forever preferred_lft forever
2: enp1s0	inet 192.168.24.45/24 brd 192.168.24.255 scope global enp1s0\   	valid_lft forever preferred_lft forever
2: enp1s0	inet 192.168.24.46/32 brd 192.168.24.255 scope global enp1s0\   	valid_lft forever preferred_lft forever
7: br-ex	inet 10.0.0.122/24 brd 10.0.0.255 scope global br-ex\   	valid_lft forever preferred_lft forever
8: vlan70	inet 172.17.5.22/24 brd 172.17.5.255 scope global vlan70\   	valid_lft forever preferred_lft forever
8: vlan70	inet 172.17.5.94/32 brd 172.17.5.255 scope global vlan70\   	valid_lft forever preferred_lft forever
9: vlan50	inet 172.17.2.140/24 brd 172.17.2.255 scope global vlan50\   	valid_lft forever preferred_lft forever
10: vlan30	inet 172.17.3.73/24 brd 172.17.3.255 scope global vlan30\   	valid_lft forever preferred_lft forever
10: vlan30	inet 172.17.3.68/32 brd 172.17.3.255 scope global vlan30\   	valid_lft forever preferred_lft forever
11: vlan20	inet 172.17.1.88/24 brd 172.17.1.255 scope global vlan20\   	valid_lft forever preferred_lft forever
12: vlan40	inet 172.17.4.24/24 brd 172.17.4.255 scope global vlan40\   	valid_lft forever preferred_lft forever</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>vlan30 represents the Storage Network, where the new RGW instances should be
started on the Red Hat Ceph Storage nodes.</p>
</li>
<li>
<p>br-ex represents the External Network, which is where in the current
environment, haproxy has the frontend Virtual IP (VIP) assigned.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Identify the network that you previously had in haproxy and propagate it through
director to the Red Hat Ceph Storage nodes. This network is used to reserve a new VIP
that is owned by Red Hat Ceph Storage and used as the entry point for the RGW service.</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Log into <code>controller-0</code> and check the current HAProxy configuration until you
find <code>ceph_rgw</code> section:</p>
<div class="listingblock">
<div class="content">
<pre>$ less /var/lib/config-data/puppet-generated/haproxy/etc/haproxy/haproxy.cfg

...
...
listen ceph_rgw
  bind 10.0.0.103:8080 transparent
  bind 172.17.3.68:8080 transparent
  mode http
  balance leastconn
  http-request set-header X-Forwarded-Proto https if { ssl_fc }
  http-request set-header X-Forwarded-Proto http if !{ ssl_fc }
  http-request set-header X-Forwarded-Port %[dst_port]
  option httpchk GET /swift/healthcheck
  option httplog
  option forwardfor
  server controller-0.storage.redhat.local 172.17.3.73:8080 check fall 5 inter 2000 rise 2
  server controller-1.storage.redhat.local 172.17.3.146:8080 check fall 5 inter 2000 rise 2
  server controller-2.storage.redhat.local 172.17.3.156:8080 check fall 5 inter 2000 rise 2</pre>
</div>
</div>
</li>
<li>
<p>Confirm that the network is used as an HAProxy frontend:</p>
<div class="listingblock">
<div class="content">
<pre>[controller-0]$ ip -o -4 a

...
7: br-ex	inet 10.0.0.106/24 brd 10.0.0.255 scope global br-ex\   	valid_lft forever preferred_lft forever
...</pre>
</div>
</div>
<div class="paragraph">
<p>This example shows that <code>controller-0</code> is exposing the services by using the external network, which is not present in
the Red Hat Ceph Storage nodes, and you need to propagate it through director.</p>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Propagate the HAProxy frontend network to Red Hat Ceph Storage nodes.</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Change the NIC template used to define the <code>ceph-storage</code> network interfaces and add the new config section:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">---
network_config:
- type: interface
  name: nic1
  use_dhcp: false
  dns_servers: {{ ctlplane_dns_nameservers }}
  addresses:
  - ip_netmask: {{ ctlplane_ip }}/{{ ctlplane_cidr }}
  routes: {{ ctlplane_host_routes }}
- type: vlan
  vlan_id: {{ storage_mgmt_vlan_id }}
  device: nic1
  addresses:
  - ip_netmask: {{ storage_mgmt_ip }}/{{ storage_mgmt_cidr }}
  routes: {{ storage_mgmt_host_routes }}
- type: interface
  name: nic2
  use_dhcp: false
  defroute: false
- type: vlan
  vlan_id: {{ storage_vlan_id }}
  device: nic2
  addresses:
  - ip_netmask: {{ storage_ip }}/{{ storage_cidr }}
  routes: {{ storage_host_routes }}
- type: ovs_bridge
  name: {{ neutron_physical_bridge_name }}
  dns_servers: {{ ctlplane_dns_nameservers }}
  domain: {{ dns_search_domains }}
  use_dhcp: false
  addresses:
  - ip_netmask: {{ external_ip }}/{{ external_cidr }}
  routes: {{ external_host_routes }}
  members:
  - type: interface
    name: nic3
    primary: true</code></pre>
</div>
</div>
</li>
<li>
<p>In addition, add the External Network to the <code>baremetal.yaml</code> file used by
metalsmith:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">- name: CephStorage
  count: 3
  hostname_format: cephstorage-%index%
  instances:
  - hostname: cephstorage-0
  name: ceph-0
  - hostname: cephstorage-1
  name: ceph-1
  - hostname: cephstorage-2
  name: ceph-2
  defaults:
  profile: ceph-storage
  network_config:
      template: /home/stack/composable_roles/network/nic-configs/ceph-storage.j2
  networks:
  - network: ctlplane
      vif: true
  - network: storage
  - network: storage_mgmt
  - network: external</code></pre>
</div>
</div>
</li>
<li>
<p>Run the <code>overcloud node provision</code> command passing the <code>--network-config</code> option:</p>
<div class="listingblock">
<div class="content">
<pre>(undercloud) [stack@undercloud-0]$

openstack overcloud node provision
   -o overcloud-baremetal-deployed-0.yaml
   --stack overcloud
   --network-config -y
  $PWD/network/baremetal_deployment.yaml</pre>
</div>
</div>
</li>
<li>
<p>Check the new network on the Red Hat Ceph Storage nodes:</p>
<div class="listingblock">
<div class="content">
<pre>[root@cephstorage-0 ~]# ip -o -4 a

1: lo	inet 127.0.0.1/8 scope host lo\   	valid_lft forever preferred_lft forever
2: enp1s0	inet 192.168.24.54/24 brd 192.168.24.255 scope global enp1s0\   	valid_lft forever preferred_lft forever
11: vlan40	inet 172.17.4.43/24 brd 172.17.4.255 scope global vlan40\   	valid_lft forever preferred_lft forever
12: vlan30	inet 172.17.3.23/24 brd 172.17.3.255 scope global vlan30\   	valid_lft forever preferred_lft forever
14: br-ex	inet 10.0.0.133/24 brd 10.0.0.255 scope global br-ex\   	valid_lft forever preferred_lft forever</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="migrating-the-rgw-backends_migrating-ceph-rgw">Migrating the Red Hat Ceph Storage RGW backends</h4>
<div class="paragraph">
<p>To match the cardinality diagram, you use cephadm labels to refer to a group of nodes where a given daemon type should be deployed. For more information about the cardinality diagram, see <a href="#ceph-daemon-cardinality_migrating-ceph-rgw">Red Hat Ceph Storage daemon cardinality</a>.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Add the RGW label to the Red Hat Ceph Storage nodes:</p>
<div class="listingblock">
<div class="content">
<pre>for i in 0 1 2; {
    ceph orch host label add cephstorage-$i rgw;
}</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>[ceph: root@controller-0 /]#

for i in 0 1 2; {
    ceph orch host label add cephstorage-$i rgw;
}

Added label rgw to host cephstorage-0
Added label rgw to host cephstorage-1
Added label rgw to host cephstorage-2

[ceph: root@controller-0 /]# ceph orch host ls

HOST       	ADDR       	LABELS      	STATUS
cephstorage-0  192.168.24.54  osd rgw
cephstorage-1  192.168.24.44  osd rgw
cephstorage-2  192.168.24.30  osd rgw
controller-0   192.168.24.45  _admin mon mgr
controller-1   192.168.24.11  _admin mon mgr
controller-2   192.168.24.38  _admin mon mgr

6 hosts in cluster</pre>
</div>
</div>
</li>
<li>
<p>During the overcloud deployment, RGW is applied at step 2
(external_deployment_steps), and a cephadm compatible spec is generated in
<code>/home/ceph-admin/specs/rgw</code> from director. Find the RGW spec:</p>
<div class="listingblock">
<div class="content">
<pre>[root@controller-0 heat-admin]# cat rgw

networks:
- 172.17.3.0/24
placement:
  hosts:
  - controller-0
  - controller-1
  - controller-2
service_id: rgw
service_name: rgw.rgw
service_type: rgw
spec:
  rgw_frontend_port: 8080
  rgw_realm: default
  rgw_zone: default</pre>
</div>
</div>
</li>
<li>
<p>In the <code>placement</code> section, replace the following values:</p>
<div class="ulist">
<ul>
<li>
<p>Replace the controller nodes with the <code>label: rgw</code> label.</p>
</li>
<li>
<p>Change the ` rgw_frontend_port` value to <code>8090</code> to avoid conflicts with the Ceph ingress daemon.</p>
<div class="listingblock">
<div class="content">
<pre>---
networks:
- 172.17.3.0/24
placement:
  label: rgw
service_id: rgw
service_name: rgw.rgw
service_type: rgw
spec:
  rgw_frontend_port: 8090
  rgw_realm: default
  rgw_zone: default</pre>
</div>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Apply the new RGW spec by using the orchestrator CLI:</p>
<div class="listingblock">
<div class="content">
<pre>$ cephadm shell -m /home/ceph-admin/specs/rgw
$ cephadm shell -- ceph orch apply -i /mnt/rgw</pre>
</div>
</div>
<div class="paragraph">
<p>This command triggers the redeploy:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>...
osd.9                     	cephstorage-2
rgw.rgw.cephstorage-0.wsjlgx  cephstorage-0  172.17.3.23:8090   starting
rgw.rgw.cephstorage-1.qynkan  cephstorage-1  172.17.3.26:8090   starting
rgw.rgw.cephstorage-2.krycit  cephstorage-2  172.17.3.81:8090   starting
rgw.rgw.controller-1.eyvrzw   controller-1   172.17.3.146:8080  running (5h)
rgw.rgw.controller-2.navbxa   controller-2   172.17.3.66:8080   running (5h)

...
osd.9                     	cephstorage-2
rgw.rgw.cephstorage-0.wsjlgx  cephstorage-0  172.17.3.23:8090  running (19s)
rgw.rgw.cephstorage-1.qynkan  cephstorage-1  172.17.3.26:8090  running (16s)
rgw.rgw.cephstorage-2.krycit  cephstorage-2  172.17.3.81:8090  running (13s)</pre>
</div>
</div>
</li>
<li>
<p>Ensure that the new RGW backends are reachable on
the new ports, because you are going to enable an IngressDaemon on port 8080
later in the process. For this reason, log in to each RGW node (the Red Hat Ceph Storage
nodes) and add the iptables rule to allow connections to both 8080 and 8090
ports in the Red Hat Ceph Storage nodes.</p>
<div class="listingblock">
<div class="content">
<pre>iptables -I INPUT -p tcp -m tcp --dport 8080 -m conntrack --ctstate NEW -m comment --comment "ceph rgw ingress" -j ACCEPT

iptables -I INPUT -p tcp -m tcp --dport 8090 -m conntrack --ctstate NEW -m comment --comment "ceph rgw backends" -j ACCEPT

for port in 8080 8090; {
    for i in 25 10 32; {
       ssh heat-admin@192.168.24.$i sudo iptables -I INPUT \
       -p tcp -m tcp --dport $port -m conntrack --ctstate NEW \
       -j ACCEPT;
   }
}</pre>
</div>
</div>
</li>
<li>
<p>From a Controller node (e.g. controller-0) try to reach (curl) the RGW backends:</p>
<div class="listingblock">
<div class="content">
<pre>for i in 26 23 81; do {
    echo "---"
    curl 172.17.3.$i:8090;
    echo "---"
    echo
done</pre>
</div>
</div>
<div class="paragraph">
<p>You should observe the following output:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>---
Query 172.17.3.23
&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;ListAllMyBucketsResult xmlns="http://s3.amazonaws.com/doc/2006-03-01/"&gt;&lt;Owner&gt;&lt;ID&gt;anonymous&lt;/ID&gt;&lt;DisplayName&gt;&lt;/DisplayName&gt;&lt;/Owner&gt;&lt;Buckets&gt;&lt;/Buckets&gt;&lt;/ListAllMyBucketsResult&gt;
---

---
Query 172.17.3.26
&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;ListAllMyBucketsResult xmlns="http://s3.amazonaws.com/doc/2006-03-01/"&gt;&lt;Owner&gt;&lt;ID&gt;anonymous&lt;/ID&gt;&lt;DisplayName&gt;&lt;/DisplayName&gt;&lt;/Owner&gt;&lt;Buckets&gt;&lt;/Buckets&gt;&lt;/ListAllMyBucketsResult&gt;
---

---
Query 172.17.3.81
&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;ListAllMyBucketsResult xmlns="http://s3.amazonaws.com/doc/2006-03-01/"&gt;&lt;Owner&gt;&lt;ID&gt;anonymous&lt;/ID&gt;&lt;DisplayName&gt;&lt;/DisplayName&gt;&lt;/Owner&gt;&lt;Buckets&gt;&lt;/Buckets&gt;&lt;/ListAllMyBucketsResult&gt;
---</pre>
</div>
</div>
</li>
<li>
<p>If RGW backends are migrated in the Red Hat Ceph Storage nodes, there is no "<code>internalAPI</code>" network(this is not true in the case of HCI). Reconfigure the RGW keystone endpoint, pointing to the external network that has been propagated. For more information about propagating the external network, see <a href="#completing-prerequisites-for-migrating-ceph-rgw_migrating-ceph-rgw">Completing prerequisites for migrating Red Hat Ceph Storage RGW</a>.</p>
<div class="listingblock">
<div class="content">
<pre>[ceph: root@controller-0 /]# ceph config dump | grep keystone
global   basic rgw_keystone_url  http://172.16.1.111:5000

[ceph: root@controller-0 /]# ceph config set global rgw_keystone_url http://10.0.0.103:5000</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="deploying-a-ceph-ingress-daemon_migrating-ceph-rgw">Deploying a Red Hat Ceph Storage ingress daemon</h4>
<div class="paragraph">
<p>To match the cardinality diagram, you use cephadm labels to refer to a group of nodes where a given daemon type should be deployed. For more information about the cardinality diagram, see <a href="#ceph-daemon-cardinality_migrating-ceph-rgw">Red Hat Ceph Storage daemon cardinality</a>.
<code>HAProxy</code> is managed by director through <code>Pacemaker</code>: the three running instances at this point will point to the old RGW backends, resulting in a broken configuration.
Since you are going to deploy the Ceph ingress daemon, the first thing to do is remove the existing <code>ceph_rgw</code> config, clean up the config created by director and restart the service to make sure other services are not affected by this change.
After you complete this procedure, you can reach the RGW backend from the ingress daemon and use RGW through the Object Storage service command line interface (CLI).</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Log in to each Controller node and remove the following configuration from the <code>/var/lib/config-data/puppet-generated/haproxy/etc/haproxy/haproxy.cfg</code> file:</p>
<div class="listingblock">
<div class="content">
<pre>listen ceph_rgw
  bind 10.0.0.103:8080 transparent
  mode http
  balance leastconn
  http-request set-header X-Forwarded-Proto https if { ssl_fc }
  http-request set-header X-Forwarded-Proto http if !{ ssl_fc }
  http-request set-header X-Forwarded-Port %[dst_port]
  option httpchk GET /swift/healthcheck
  option httplog
  option forwardfor
   server controller-0.storage.redhat.local 172.17.3.73:8080 check fall 5 inter 2000 rise 2
  server controller-1.storage.redhat.local 172.17.3.146:8080 check fall 5 inter 2000 rise 2
  server controller-2.storage.redhat.local 172.17.3.156:8080 check fall 5 inter 2000 rise 2</pre>
</div>
</div>
</li>
<li>
<p>Restart <code>haproxy-bundle</code> and ensure it is started:</p>
<div class="listingblock">
<div class="content">
<pre>[root@controller-0 ~]# sudo pcs resource restart haproxy-bundle
haproxy-bundle successfully restarted


[root@controller-0 ~]# sudo pcs status | grep haproxy

  * Container bundle set: haproxy-bundle [undercloud-0.ctlplane.redhat.local:8787/rh-osbs/rhosp17-openstack-haproxy:pcmklatest]:
    * haproxy-bundle-podman-0   (ocf:heartbeat:podman):  Started controller-0
    * haproxy-bundle-podman-1   (ocf:heartbeat:podman):  Started controller-1
    * haproxy-bundle-podman-2   (ocf:heartbeat:podman):  Started controller-2</pre>
</div>
</div>
</li>
<li>
<p>Confirm that no process is bound to 8080:</p>
<div class="listingblock">
<div class="content">
<pre>[root@controller-0 ~]# ss -antop | grep 8080
[root@controller-0 ~]#</pre>
</div>
</div>
<div class="paragraph">
<p>The Object Storage service (swift) CLI fails at this point:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>(overcloud) [root@cephstorage-0 ~]# swift list

HTTPConnectionPool(host='10.0.0.103', port=8080): Max retries exceeded with url: /swift/v1/AUTH_852f24425bb54fa896476af48cbe35d3?format=json (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7fc41beb0430&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))</pre>
</div>
</div>
</li>
<li>
<p>Set the required images for both HAProxy and Keepalived:</p>
<div class="listingblock">
<div class="content">
<pre>[ceph: root@controller-0 /]# ceph config set mgr mgr/cephadm/container_image_haproxy registry.redhat.io/rhceph/rhceph-haproxy-rhel9:latest
[ceph: root@controller-0 /]# ceph config set mgr mgr/cephadm/container_image_keepalived registry.redhat.io/rhceph/keepalived-rhel9:latest</pre>
</div>
</div>
</li>
<li>
<p>Create a file called <code>rgw_ingress</code> in the <code>/home/ceph-admin/specs/</code> directory in <code>controller-0</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo vim /home/ceph-admin/specs/rgw_ingress</pre>
</div>
</div>
</li>
<li>
<p>Paste the following content in to the <code>rgw_ingress</code> file:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">---
service_type: ingress
service_id: rgw.rgw
placement:
  label: rgw
spec:
  backend_service: rgw.rgw
  virtual_ip: 10.0.0.89/24
  frontend_port: 8080
  monitor_port: 8898
  virtual_interface_networks:
    - &lt;external_network&gt;</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;external_network&gt;</code> with your external network, for example, <code>10.0.0.0/24</code>. For more information, see <a href="#completing-prerequisites-for-migrating-ceph-rgw_migrating-ceph-rgw">Completing prerequisites for migrating Red Hat Ceph Storage RGW</a>.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Apply the <code>rgw_ingress</code> spec by using the Ceph orchestrator CLI:</p>
<div class="listingblock">
<div class="content">
<pre>$ cephadm shell -m /home/ceph-admin/specs/rgw_ingress
$ cephadm shell -- ceph orch apply -i /mnt/rgw_ingress</pre>
</div>
</div>
</li>
<li>
<p>Wait until the ingress is deployed and query the resulting endpoint:</p>
<div class="listingblock">
<div class="content">
<pre>[ceph: root@controller-0 /]# ceph orch ls

NAME                 	PORTS            	RUNNING  REFRESHED  AGE  PLACEMENT
crash                                         	6/6  6m ago 	3d   *
ingress.rgw.rgw      	10.0.0.89:8080,8898  	6/6  37s ago	60s  label:rgw
mds.mds                   3/3  6m ago 	3d   controller-0;controller-1;controller-2
mgr                       3/3  6m ago 	3d   controller-0;controller-1;controller-2
mon                       3/3  6m ago 	3d   controller-0;controller-1;controller-2
osd.default_drive_group   15  37s ago	3d   cephstorage-0;cephstorage-1;cephstorage-2
rgw.rgw   ?:8090          3/3  37s ago	4m   label:rgw</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>[ceph: root@controller-0 /]# curl  10.0.0.89:8080

---
&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;ListAllMyBucketsResult xmlns="http://s3.amazonaws.com/doc/2006-03-01/"&gt;&lt;Owner&gt;&lt;ID&gt;anonymous&lt;/ID&gt;&lt;DisplayName&gt;&lt;/DisplayName&gt;&lt;/Owner&gt;&lt;Buckets&gt;&lt;/Buckets&gt;&lt;/ListAllMyBucketsResult&gt;[ceph: root@controller-0 /]#
</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="updating-the-object-storage-endpoints_migrating-ceph-rgw">Updating the object-store endpoints</h4>
<div class="paragraph">
<p>The object-storage endpoints still point to the original virtual IP address (VIP) that is owned by pacemaker. You must update the object-store endpoints because other services stll use the original VIP, and you reserved a new VIP on the same network.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>List the current endpoints:</p>
<div class="listingblock">
<div class="content">
<pre>(overcloud) [stack@undercloud-0 ~]$ openstack endpoint list | grep object

| 1326241fb6b6494282a86768311f48d1 | regionOne | swift    	| object-store   | True	| internal  | http://172.17.3.68:8080/swift/v1/AUTH_%(project_id)s |
| 8a34817a9d3443e2af55e108d63bb02b | regionOne | swift    	| object-store   | True	| public	| http://10.0.0.103:8080/swift/v1/AUTH_%(project_id)s  |
| fa72f8b8b24e448a8d4d1caaeaa7ac58 | regionOne | swift    	| object-store   | True	| admin 	| http://172.17.3.68:8080/swift/v1/AUTH_%(project_id)s |</pre>
</div>
</div>
</li>
<li>
<p>Update the endpoints that are pointing to the Ingress VIP:</p>
<div class="listingblock">
<div class="content">
<pre>(overcloud) [stack@undercloud-0 ~]$ openstack endpoint set --url "http://10.0.0.89:8080/swift/v1/AUTH_%(project_id)s" 95596a2d92c74c15b83325a11a4f07a3

(overcloud) [stack@undercloud-0 ~]$ openstack endpoint list | grep object-store
| 6c7244cc8928448d88ebfad864fdd5ca | regionOne | swift    	| object-store   | True	| internal  | http://172.17.3.79:8080/swift/v1/AUTH_%(project_id)s |
| 95596a2d92c74c15b83325a11a4f07a3 | regionOne | swift    	| object-store   | True	| public	| http://10.0.0.89:8080/swift/v1/AUTH_%(project_id)s   |
| e6d0599c5bf24a0fb1ddf6ecac00de2d | regionOne | swift    	| object-store   | True	| admin 	| http://172.17.3.79:8080/swift/v1/AUTH_%(project_id)s |</pre>
</div>
</div>
<div class="paragraph">
<p>Repeat this step for both internal and admin endpoints.</p>
</div>
</li>
<li>
<p>Test the migrated service:</p>
<div class="listingblock">
<div class="content">
<pre>(overcloud) [stack@undercloud-0 ~]$ swift list --debug

DEBUG:swiftclient:Versionless auth_url - using http://10.0.0.115:5000/v3 as endpoint
DEBUG:keystoneclient.auth.identity.v3.base:Making authentication request to http://10.0.0.115:5000/v3/auth/tokens
DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): 10.0.0.115:5000
DEBUG:urllib3.connectionpool:http://10.0.0.115:5000 "POST /v3/auth/tokens HTTP/1.1" 201 7795
DEBUG:keystoneclient.auth.identity.v3.base:{"token": {"methods": ["password"], "user": {"domain": {"id": "default", "name": "Default"}, "id": "6f87c7ffdddf463bbc633980cfd02bb3", "name": "admin", "password_expires_at": null},


...
...
...

DEBUG:swiftclient:REQ: curl -i http://10.0.0.89:8080/swift/v1/AUTH_852f24425bb54fa896476af48cbe35d3?format=json -X GET -H "X-Auth-Token: gAAAAABj7KHdjZ95syP4c8v5a2zfXckPwxFQZYg0pgWR42JnUs83CcKhYGY6PFNF5Cg5g2WuiYwMIXHm8xftyWf08zwTycJLLMeEwoxLkcByXPZr7kT92ApT-36wTfpi-zbYXd1tI5R00xtAzDjO3RH1kmeLXDgIQEVp0jMRAxoVH4zb-DVHUos" -H "Accept-Encoding: gzip"
DEBUG:swiftclient:RESP STATUS: 200 OK
DEBUG:swiftclient:RESP HEADERS: {'content-length': '2', 'x-timestamp': '1676452317.72866', 'x-account-container-count': '0', 'x-account-object-count': '0', 'x-account-bytes-used': '0', 'x-account-bytes-used-actual': '0', 'x-account-storage-policy-default-placement-container-count': '0', 'x-account-storage-policy-default-placement-object-count': '0', 'x-account-storage-policy-default-placement-bytes-used': '0', 'x-account-storage-policy-default-placement-bytes-used-actual': '0', 'x-trans-id': 'tx00000765c4b04f1130018-0063eca1dd-1dcba-default', 'x-openstack-request-id': 'tx00000765c4b04f1130018-0063eca1dd-1dcba-default', 'accept-ranges': 'bytes', 'content-type': 'application/json; charset=utf-8', 'date': 'Wed, 15 Feb 2023 09:11:57 GMT'}
DEBUG:swiftclient:RESP BODY: b'[]'</pre>
</div>
</div>
</li>
<li>
<p>Run tempest tests against object-storage:</p>
<div class="listingblock">
<div class="content">
<pre>(overcloud) [stack@undercloud-0 tempest-dir]$  tempest run --regex tempest.api.object_storage
...
...
...
======
Totals
======
Ran: 141 tests in 606.5579 sec.
 - Passed: 128
 - Skipped: 13
 - Expected Fail: 0
 - Unexpected Success: 0
 - Failed: 0
Sum of execute time for each test: 657.5183 sec.

==============
Worker Balance
==============
 - Worker 0 (1 tests) =&gt; 0:10:03.400561
 - Worker 1 (2 tests) =&gt; 0:00:24.531916
 - Worker 2 (4 tests) =&gt; 0:00:10.249889
 - Worker 3 (30 tests) =&gt; 0:00:32.730095
 - Worker 4 (51 tests) =&gt; 0:00:26.246044
 - Worker 5 (6 tests) =&gt; 0:00:20.114803
 - Worker 6 (20 tests) =&gt; 0:00:16.290323
 - Worker 7 (27 tests) =&gt; 0:00:17.103827</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="migrating-ceph-rbd_migrating-ceph-rgw">Migrating Red Hat Ceph Storage RBD to external RHEL nodes</h3>
<div class="paragraph">
<p>For Hyperconverged Infrastructure (HCI) or dedicated Storage nodes that are
running Red Hat Ceph Storage version 6 or later, you must migrate the daemons that are
included in the Red&#160;Hat OpenStack Platform control plane into the existing external Red
Hat Enterprise Linux (RHEL) nodes. The external RHEL nodes typically include
the Compute nodes for an HCI environment or dedicated storage nodes.</p>
</div>
<div class="paragraph">
<p>To migrate Red Hat Ceph Storage Rados Block Device (RBD), your environment must
meet the following requirements:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Red Hat Ceph Storage is running version 6 or later and is managed by cephadm.</p>
</li>
<li>
<p>NFS Ganesha is migrated from a director-based
deployment to cephadm. For more information, see
<a href="#creating-a-ceph-nfs-cluster_migrating-databases">Creating a NFS Ganesha
cluster</a>.</p>
</li>
<li>
<p>Both the Red Hat Ceph Storage public and cluster networks are propagated, with
director, to the target nodes.</p>
</li>
<li>
<p>Ceph MDS, Ceph Monitoring stack, Ceph MDS, Ceph RGW and other services are
migrated to the target nodes.</p>
</li>
<li>
<p>The daemons distribution follows the cardinality constraints that are
described in <a href="https://access.redhat.com/articles/1548993">Red Hat Ceph
Storage: Supported configurations</a>.</p>
</li>
<li>
<p>The Red Hat Ceph Storage cluster is healthy, and the <code>ceph -s</code> command returns <code>HEALTH_OK</code>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>During the procedure to migrate the Ceph Mon daemons, the following actions
occur:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The mon IP addresses are moved to the target Red Hat Ceph Storage nodes.</p>
</li>
<li>
<p>The existing Controller nodes are drained and decommisioned.</p>
</li>
<li>
<p>Additional monitors are deployed to the target nodes, and they are promoted
as <code>_admin</code> nodes that can be used to manage the Red Hat Ceph Storage cluster and
perform day 2 operations.</p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="_migrating_ceph_manager_daemons_to_red_hat_ceph_storage_nodes">Migrating Ceph Manager daemons to Red Hat Ceph Storage nodes</h4>
<div class="paragraph">
<p>The following section describes how to move Ceph Manager daemons from the
Red&#160;Hat OpenStack Platform Controller nodes to a set of target nodes. Target nodes might
be pre-existing Red Hat Ceph Storage nodes, or RHOSP Compute nodes if Red Hat Ceph Storage is
deployed by director with an HCI topology.
This procedure assumes that Cephadm and the Red Hat Ceph Storage Orchestrator are the tools
that drive the Ceph Manager migration. As is done with the other Ceph daemons
(MDS, Monitoring, and RGW), the procedure uses the Ceph spec to modify the
placement and reschedule the daemons. Ceph Manager is run in an active/passive
fashion, and it also provides many modules, including the Ceph orchestrator.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Configure the target nodes (CephStorage or ComputeHCI) to have both <code>storage</code>
and <code>storage_mgmt</code> networks to ensure that you can use both Red Hat Ceph Storage public and
cluster networks from the same node. This step requires you to interact with
director. From Red&#160;Hat OpenStack Platform 17.1 and later
you do not have to run a stack update.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Ssh into the target node and enable the firewall rules that are required to
reach a Manager service:</p>
<div class="listingblock">
<div class="content">
<pre>dports="6800:7300"
ssh heat-admin@&lt;target_node&gt; sudo iptables -I INPUT \
    -p tcp --match multiport --dports $dports -j ACCEPT;</pre>
</div>
</div>
<div class="paragraph">
<p>Repeat this step for each <code>&lt;target_node&gt;</code>.</p>
</div>
</li>
<li>
<p>Check that the rules are properly applied and persist them:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo iptables-save
$ sudo systemctl restart iptables</pre>
</div>
</div>
</li>
<li>
<p>Prepare the target node to host the new Ceph Manager daemon, and add the <code>mgr</code>
label to the target node:</p>
<div class="listingblock">
<div class="content">
<pre>ceph orch host label add &lt;target_node&gt; mgr; done</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;target_node&gt;</code> with the hostname of the hosts listed in the Red Hat Ceph Storage
through the <code>ceph orch host ls</code> command</p>
<div class="paragraph">
<p>Repeat the actions described above for each `&lt;target_node&gt; that will host a
Ceph Manager daemon.</p>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Get the Ceph Manager spec:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">sudo cephadm shell -- ceph orch ls --export mgr &gt; mgr.yaml</code></pre>
</div>
</div>
</li>
<li>
<p>Edit the retrieved spec and add the <code>label: mgr</code> section to the <code>placement</code>
section:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">service_type: mgr
service_id: mgr
placement:
  label: mgr</code></pre>
</div>
</div>
</li>
<li>
<p>Save the spec in the <code>/tmp/mgr.yaml</code> file.</p>
</li>
<li>
<p>Apply the spec with cephadm by using the orchestrator:</p>
<div class="listingblock">
<div class="content">
<pre>sudo cephadm shell -m /tmp/mgr.yaml -- ceph orch apply -i /mnt/mgr.yaml</pre>
</div>
</div>
<div class="paragraph">
<p>As a result of this procedure, you see a Ceph Manager daemon count that matches
the number of hosts where the <code>mgr</code> label is added.</p>
</div>
</li>
<li>
<p>Verify that the new Ceph Manager are created in the target nodes:</p>
<div class="listingblock">
<div class="content">
<pre>ceph orch ps | grep -i mgr
ceph -s</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The procedure does not shrink the Ceph Manager daemons. The count is grown by
the number of target nodes, and migrating Ceph Monitor daemons to Red Hat Ceph Storage nodes
decommissions the stand-by Ceph Manager instances. For more information, see
<a href="#migrating-mon-from-controller-nodes_migrating-ceph-rbd">Migrating Ceph Monitor
daemons to Red Hat Ceph Storage nodes</a>.
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="migrating-mon-from-controller-nodes_migrating-ceph-rbd">Migrating Ceph Monitor daemons to Red Hat Ceph Storage nodes</h4>
<div class="paragraph">
<p>The following section describes how to move Ceph Monitor daemons from the
Red&#160;Hat OpenStack Platform Controller nodes to a set of target nodes. Target nodes might
be pre-existing Red Hat Ceph Storage nodes, or RHOSP Compute nodes if Red Hat Ceph Storage is
deployed by director with an HCI topology.
This procedure assumes that some of the steps are run on the source node that
we want to decommission, while other steps are run on the target node that is
supposed to host the redeployed daemon.</p>
</div>
<div class="paragraph">
<div class="title">Prerequisites</div>
<p>Configure the target nodes (CephStorage or ComputeHCI) to have both <code>storage</code>
and <code>storage_mgmt</code> networks to ensure that you can use both Red Hat Ceph Storage public and
cluster networks from the same node. This step requires you to interact with
director. From Red&#160;Hat OpenStack Platform 17.1 and later
you do not have to run a stack update. However, there are commands that you
must perform to run <code>os-net-config</code> on the bare metal node and configure
additional networks.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>If target nodes are <code>CephStorage</code>, ensure that the network is defined in the
<code>metalsmith.yaml</code> for the CephStorageNodes:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">  - name: CephStorage
    count: 2
    instances:
      - hostname: oc0-ceph-0
        name: oc0-ceph-0
      - hostname: oc0-ceph-1
        name: oc0-ceph-1
    defaults:
      networks:
        - network: ctlplane
          vif: true
        - network: storage_cloud_0
            subnet: storage_cloud_0_subnet
        - network: storage_mgmt_cloud_0
            subnet: storage_mgmt_cloud_0_subnet
      network_config:
        template: templates/single_nic_vlans/single_nic_vlans_storage.j2</code></pre>
</div>
</div>
</li>
<li>
<p>Run the provisioning command:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack overcloud node provision \
  -o overcloud-baremetal-deployed-0.yaml --stack overcloud-0 \
  --network-config -y --concurrency 2 /home/stack/metalsmith-0.yam</pre>
</div>
</div>
</li>
<li>
<p>Verify that the storage network is configured on the target nodes:</p>
<div class="listingblock">
<div class="content">
<pre>(undercloud) [stack@undercloud ~]$ ssh heat-admin@192.168.24.14 ip -o -4 a
1: lo    inet 127.0.0.1/8 scope host lo\       valid_lft forever preferred_lft forever
5: br-storage    inet 192.168.24.14/24 brd 192.168.24.255 scope global br-storage\       valid_lft forever preferred_lft forever
6: vlan1    inet 192.168.24.14/24 brd 192.168.24.255 scope global vlan1\       valid_lft forever preferred_lft forever
7: vlan11    inet 172.16.11.172/24 brd 172.16.11.255 scope global vlan11\       valid_lft forever preferred_lft forever
8: vlan12    inet 172.16.12.46/24 brd 172.16.12.255 scope global vlan12\       valid_lft forever preferred_lft forever</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Ssh into the target node and enable the firewall rules that are required to
reach a Mon service:</p>
<div class="listingblock">
<div class="content">
<pre>$ for port in 3300 6789; {
    ssh heat-admin@&lt;target_node&gt; sudo iptables -I INPUT \
    -p tcp -m tcp --dport $port -m conntrack --ctstate NEW \
    -j ACCEPT;
}</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;target_node&gt;</code> with the hostname of the node that is supposed to
host the new mon.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Check that the rules are properly applied and persist them:</p>
<div class="listingblock">
<div class="content">
<pre>sudo iptables-save
sudo systemctl restart iptables</pre>
</div>
</div>
</li>
<li>
<p>To migrate the existing Mons to the target Red Hat Ceph Storage nodes, create the following
Red Hat Ceph Storage spec from the first mon (or the first Controller node) and modify the
placement based on the appropriate label.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">service_type: mon
service_id: mon
placement:
  label: mon</code></pre>
</div>
</div>
</li>
<li>
<p>Save the spec in the <code>/tmp/mon.yaml</code> file.</p>
</li>
<li>
<p>Apply the spec with cephadm by using the orchestrator:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -m /tmp/mon.yaml
$ ceph orch apply -i /mnt/mon.yaml</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Applying the <code>mon.yaml</code> spec allows the existing strategy to use <code>labels</code>
instead of <code>hosts</code>. As a result, any node with the <code>mon</code> label can host a Ceph
mon daemon.
Execute this step once to avoid multiple iterations when multiple Ceph Mons are
migrated.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Check the status of the Red Hat Ceph Storage and the Ceph orchestrator daemons list.
Make sure that the three mons are in quorum and listed by the <code>ceph orch</code>
command:</p>
<div class="listingblock">
<div class="content">
<pre># ceph -s
  cluster:
    id:     f6ec3ebe-26f7-56c8-985d-eb974e8e08e3
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum oc0-controller-0,oc0-controller-1,oc0-controller-2 (age 19m)
    mgr: oc0-controller-0.xzgtvo(active, since 32m), standbys: oc0-controller-1.mtxohd, oc0-controller-2.ahrgsk
    osd: 8 osds: 8 up (since 12m), 8 in (since 18m); 1 remapped pgs

  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   43 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>[ceph: root@oc0-controller-0 /]# ceph orch host ls
HOST              ADDR           LABELS          STATUS
oc0-ceph-0        192.168.24.14  osd
oc0-ceph-1        192.168.24.7   osd
oc0-controller-0  192.168.24.15  _admin mgr mon
oc0-controller-1  192.168.24.23  _admin mgr mon
oc0-controller-2  192.168.24.13  _admin mgr mon</pre>
</div>
</div>
</li>
<li>
<p>On the source node, back up the <code>/etc/ceph/</code> directory. The backup allows you
to execute cephadm and get a shell to the Red Hat Ceph Storage cluster from the source node:</p>
<div class="listingblock">
<div class="content">
<pre>$ mkdir -p $HOME/ceph_client_backup
$ sudo cp -R /etc/ceph $HOME/ceph_client_backup</pre>
</div>
</div>
</li>
<li>
<p>Before draining the source node and relocating the IP address of the storage
network to the target node, fail the ceph-mgr if it is active on the
source node:</p>
<div class="listingblock">
<div class="content">
<pre>$ ceph mgr fail &lt;mgr instance&gt;</pre>
</div>
</div>
</li>
<li>
<p>Drain the source node and start the mon migration. From the cephadm shell,
remove the labels on the source node:</p>
<div class="listingblock">
<div class="content">
<pre>for label in mon mgr _admin; do
    ceph orch host rm label &lt;source_node&gt; $label;
done</pre>
</div>
</div>
</li>
<li>
<p>Remove the running mon daemon from the source node:</p>
<div class="listingblock">
<div class="content">
<pre>$ cephadm shell -- ceph orch daemon rm mon.&lt;source_node&gt; --force"</pre>
</div>
</div>
</li>
<li>
<p>Run the drain command:</p>
<div class="listingblock">
<div class="content">
<pre>$ cephadm shell -- ceph drain &lt;source_node&gt;</pre>
</div>
</div>
</li>
<li>
<p>Remove the <code>&lt;source_node&gt;</code> host from the Red Hat Ceph Storage cluster:</p>
<div class="listingblock">
<div class="content">
<pre>$ cephadm shell -- ceph orch host rm &lt;source_node&gt; --force"</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;source_node&gt;</code> with the hostname of the source node.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The source node is not part of the cluster anymore, and should not appear in
the Red Hat Ceph Storage host list when <code>cephadm shell -- ceph orch host ls</code> is run.
However, a <code>sudo podman ps</code> in the <code>&lt;source_node&gt;</code> might list both mon and mgr
still up and running.
</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
<div class="listingblock">
<div class="content">
<pre>[root@oc0-controller-1 ~]# sudo podman ps
CONTAINER ID  IMAGE                                                                                        COMMAND               CREATED         STATUS             PORTS       NAMES
5c1ad36472bc  registry.redhat.io/ceph/rhceph@sha256:320c364dcc8fc8120e2a42f54eb39ecdba12401a2546763b7bef15b02ce93bc4  -n mon.oc0-contro...  35 minutes ago  Up 35 minutes ago              ceph-f6ec3ebe-26f7-56c8-985d-eb974e8e08e3-mon-oc0-controller-1
3b14cc7bf4dd  registry.redhat.io/ceph/rhceph@sha256:320c364dcc8fc8120e2a42f54eb39ecdba12401a2546763b7bef15b02ce93bc4  -n mgr.oc0-contro...  35 minutes ago  Up 35 minutes ago              ceph-f6ec3ebe-26f7-56c8-985d-eb974e8e08e3-mgr-oc0-controller-1-mtxohd</pre>
</div>
</div>
<div class="paragraph">
<p>To cleanup the source node before moving to the next phase, cleanup the existing
containers and remove the cephadm related data from the node.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Prepare the target node to host the new mon and add the <code>mon</code> label to the
target node:</p>
<div class="listingblock">
<div class="content">
<pre>for label in mon mgr _admin; do
    ceph orch host label add &lt;target_node&gt; $label; done
done</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace &lt;target_node&gt; with the hostname of the host listed in the Red Hat Ceph Storage
through the <code>ceph orch host ls</code> command.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>At this point the cluster is running with only two mons, but a third mon appears
and will be deployed on the target node.
However, The third mon might be deployed on a different ip address available in
the node, and you need to redeploy it when the ip migration is concluded.
Even though the mon is deployed on the wrong ip address, it&#8217;s useful keep the
quorum to three and it ensures we do not risk to lose the cluster because two
mons go in split brain.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Confirm that the cluster has three mons and they are in quorum:</p>
<div class="listingblock">
<div class="content">
<pre>$ cephadm shell -- ceph -s
$ cephadm shell -- ceph orch ps | grep -i mon</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>It is now possible to migrate the original mon IP address to the target node and
redeploy the existing mon on it.
The following IP address migration procedure assumes that the target nodes have
been originally deployed by director and the network configuration
is managed by <code>os-net-config</code>.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Get the mon ip address from the existing <code>/etc/ceph/ceph.conf</code> (check the <code>mon_host</code>
line), for example:</p>
<div class="listingblock">
<div class="content">
<pre>mon_host = [v2:172.17.3.60:3300/0,v1:172.17.3.60:6789/0] [v2:172.17.3.29:3300/0,v1:172.17.3.29:6789/0] [v2:172.17.3.53:3300/0,v1:172.17.3.53:6789/0]</pre>
</div>
</div>
</li>
<li>
<p>Confirm that the mon ip address is present on the source node <code>os-net-config</code>
configuration located in <code>/etc/os-net-config</code>:</p>
<div class="listingblock">
<div class="content">
<pre>[tripleo-admin@controller-0 ~]$ grep "172.17.3.60" /etc/os-net-config/config.yaml
    - ip_netmask: 172.17.3.60/24</pre>
</div>
</div>
</li>
<li>
<p>Edit <code>/etc/os-net-config/config.yaml</code> and remove the <code>ip_netmask</code> line.</p>
</li>
<li>
<p>Save the file and refresh the node network configuration:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo os-net-config -c /etc/os-net-config/config.yaml</pre>
</div>
</div>
</li>
<li>
<p>Verify, using the <code>ip</code> command, that the IP address is not present in the source
node anymore.</p>
</li>
<li>
<p>Ssh into the target node, for example <code>cephstorage-0</code>, and add the IP address
for the new mon.</p>
</li>
<li>
<p>On the target node, edit <code>/etc/os-net-config/config.yaml</code> and
add the <code>- ip_netmask: 172.17.3.60</code> line that you removed in the source node.</p>
</li>
<li>
<p>Save the file and refresh the node network configuration:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo os-net-config -c /etc/os-net-config/config.yaml</pre>
</div>
</div>
</li>
<li>
<p>Verify, using the <code>ip</code> command, that the IP address is present in the target
node.</p>
</li>
<li>
<p>Get the ceph mon spec:</p>
<div class="listingblock">
<div class="content">
<pre>ceph orch ls --export mon &gt; mon.yaml</pre>
</div>
</div>
</li>
<li>
<p>Edit the retrieved spec and add the <code>unmanaged: true</code> keyword:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">service_type: mon
service_id: mon
placement:
  label: mon
unmanaged: true</code></pre>
</div>
</div>
</li>
<li>
<p>Save the spec in the <code>/tmp/mon.yaml</code> file</p>
</li>
<li>
<p>Apply the spec with cephadm by using the orchestrator:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -m /tmp/mon.yaml
$ ceph orch apply -i /mnt/mon.yaml</pre>
</div>
</div>
<div class="paragraph">
<p>The mon daemons are marked as <code>&lt;unmanaged&gt;</code>, and it is now possible to redeploy
the existing daemon and bind it to the migrated IP address.</p>
</div>
</li>
<li>
<p>Delete the existing mon on the target node:</p>
<div class="listingblock">
<div class="content">
<pre>$ ceph orch daemon add rm mon.&lt;target_node&gt; --force</pre>
</div>
</div>
</li>
<li>
<p>Redeploy the new mon on the target using the old IP address:</p>
<div class="listingblock">
<div class="content">
<pre>$ ceph orch daemon add mon &lt;target_node&gt;:&lt;ip_address&gt;</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;target_node&gt;</code> with the hostname of the target node enrolled in the
Red Hat Ceph Storage cluster.</p>
</li>
<li>
<p>Replace <code>&lt;ip_address&gt;</code> with the ip address of the migrated address.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Get the ceph mon spec:</p>
<div class="listingblock">
<div class="content">
<pre>$ ceph orch ls --export mon &gt; mon.yaml</pre>
</div>
</div>
</li>
<li>
<p>Edit the retrieved spec and set the <code>unmanaged</code> keyword to <code>false</code>:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">service_type: mon
service_id: mon
placement:
  label: mon
unmanaged: false</code></pre>
</div>
</div>
</li>
<li>
<p>Save the spec in <code>/tmp/mon.yaml</code> file.</p>
</li>
<li>
<p>Apply the spec with cephadm by using the Ceph Orchestrator:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -m /tmp/mon.yaml
$ ceph orch apply -i /mnt/mon.yaml</pre>
</div>
</div>
<div class="paragraph">
<p>The new mon runs on the target node with the original IP address.</p>
</div>
</li>
<li>
<p>Identify the running <code>mgr</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph -s</pre>
</div>
</div>
</li>
<li>
<p>Refresh the mgr information by force-failing it:</p>
<div class="listingblock">
<div class="content">
<pre>$ ceph mgr fail</pre>
</div>
</div>
</li>
<li>
<p>Refresh the <code>OSD</code> information:</p>
<div class="listingblock">
<div class="content">
<pre>$ ceph orch reconfig osd.default_drive_group</pre>
</div>
</div>
<div class="paragraph">
<p>Verify the Red Hat Ceph Storage cluster is healthy:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>[ceph: root@oc0-controller-0 specs]# ceph -s
  cluster:
    id:     f6ec3ebe-26f7-56c8-985d-eb974e8e08e3
    health: HEALTH_OK
...
...</pre>
</div>
</div>
</li>
<li>
<p>Repeat this procedure for any additional Controller node that hosts a mon
until you have migrated all the Ceph Mon daemons to the target nodes.</p>
</li>
</ol>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="footer">
<div id="footer-text">
Last updated 2024-08-26 14:39:56 UTC
</div>
</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.3/highlight.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.3/languages/yaml.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.3/languages/bash.min.js"></script>
<script>
if (!hljs.initHighlighting.called) {
  hljs.initHighlighting.called = true
  ;[].slice.call(document.querySelectorAll('pre.highlight > code[data-lang]')).forEach(function (el) { hljs.highlightBlock(el) })
}
</script>
</body>
</html>